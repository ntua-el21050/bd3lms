{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Vw2kb8OCsy"
      },
      "source": [
        "# Table 8 Reproduction ‚Äî LITE Budget\n",
        "\n",
        "ŒëŒΩŒ±œÄŒ±œÅŒ±Œ≥œâŒ≥ŒÆ Table 8 Œ±œÄœå Arriola et al. (ICLR 2025).\n",
        "\n",
        "**Budget:** Base 5K steps / 10K samples, Finetune 3K steps / 5K samples\n",
        "\n",
        "**ŒïŒ∫œÑ. œáœÅœåŒΩŒøœÇ:** ~1 œéœÅŒ± œÉŒµ Colab T4\n",
        "\n",
        "**Œ†ŒµŒπœÅŒ¨ŒºŒ±œÑŒ±:** 5 noise schedules √ó 2 block sizes (L'=4, L'=16)\n",
        "\n",
        "**Workflow:** Base training ‚Üí Fine-tune per schedule ‚Üí Evaluate (Linear noise schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwpGl4erOCs0",
        "outputId": "d3ec7182-c73e-48b7-cbbd-af01c4fb9342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repo already exists, skipping clone\n"
          ]
        }
      ],
      "source": [
        "# Clone project repo\n",
        "import os\n",
        "if not os.path.exists('/content/bd3lms'):\n",
        "    !cd /content && git clone https://github.com/ntua-el21050/bd3lms.git\n",
        "else:\n",
        "    print(\"Repo already exists, skipping clone\")\n",
        "\n",
        "!mkdir -p /content/repro_runs_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "L9i1Mc7OOCs1"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%%capture\n",
        "!pip install -r bd3lms/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Step 1: Patch diffusion.py (Print variance)"
      ],
      "metadata": {
        "id": "Gv0iAbAEItnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import sys\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "diffusion_file = '/content/bd3lms/diffusion.py'\n",
        "with open(diffusion_file, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "if 'VARIANCE:' not in content:\n",
        "    print(\"üõ†Ô∏è Patching diffusion.py...\")\n",
        "    old_log = \"\"\"      self.log(f'valid_var_{round(eps_min, 2)} - {round(eps_max, 2)}',\n",
        "                all_vars / len(var),\n",
        "                on_epoch=True,\n",
        "                on_step=False,\n",
        "                sync_dist=True)\"\"\"\n",
        "\n",
        "    new_log = \"\"\"      _var_val = (all_vars / len(var)).item()\n",
        "      print(f'VARIANCE: valid_var_{round(eps_min, 2)} - {round(eps_max, 2)} = {_var_val:.4f}')\n",
        "      self.log(f'valid_var_{round(eps_min, 2)} - {round(eps_max, 2)}',\n",
        "                all_vars / len(var),\n",
        "                on_epoch=True,\n",
        "                on_step=False,\n",
        "                sync_dist=True)\"\"\"\n",
        "\n",
        "    content = content.replace(old_log, new_log)\n",
        "    with open(diffusion_file, 'w') as f:\n",
        "        f.write(content)\n",
        "    # Clear pycache\n",
        "    shutil.rmtree('/content/bd3lms/__pycache__', ignore_errors=True)\n",
        "    print(\"Patch applied.\")\n",
        "else:\n",
        "    print(\"diffusion.py already patched.\")"
      ],
      "metadata": {
        "id": "f6T7VosqIlHb",
        "outputId": "9410342d-37c6-40bd-b139-0c536869a15f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diffusion.py already patched.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils\n",
        "\n",
        "BASE_MAX_STEPS = 5000\n",
        "BASE_MAX_SAMPLES = 10000\n",
        "FINETUNE_MAX_STEPS = 3000\n",
        "FINETUNE_MAX_SAMPLES = 5000\n",
        "\n",
        "def run_main(overrides):\n",
        "    env = dict(os.environ)\n",
        "    env[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
        "    cmd = [sys.executable, '-u', 'main.py'] + overrides\n",
        "    print(f\"\\n$ python main.py ... {' '.join(overrides[-3:])}\")\n",
        "\n",
        "    proc = subprocess.run(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        cwd='/content/bd3lms',\n",
        "        env=env\n",
        "    )\n",
        "    if proc.returncode != 0:\n",
        "        print(f\"FAILED:\\n{proc.stdout[-2000:]}\")\n",
        "        raise RuntimeError(f\"Command failed with code {proc.returncode}\")\n",
        "    return proc.stdout\n",
        "\n",
        "def train_run(run_name, algo, block_size, from_pretrained=None, extra_overrides=None, is_base=False):\n",
        "    save_dir = f\"/content/repro_runs_final/{run_name}\"\n",
        "    ckpt_path = f\"{save_dir}/checkpoints/last.ckpt\"\n",
        "\n",
        "    if os.path.exists(ckpt_path):\n",
        "        print(f\" Checkpoint exists: {run_name}\")\n",
        "        return ckpt_path, \"\"\n",
        "\n",
        "    # Set budget\n",
        "    steps = BASE_MAX_STEPS if is_base else FINETUNE_MAX_STEPS\n",
        "    samples = BASE_MAX_SAMPLES if is_base else FINETUNE_MAX_SAMPLES\n",
        "\n",
        "    overrides = [\n",
        "        \"mode=train\",\n",
        "        \"data=lm1b-wrap\", \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\", f\"data.max_train_samples={samples}\",\n",
        "        \"model=tiny\", \"model.length=128\", \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\",\n",
        "        \"trainer.accelerator=gpu\", \"trainer.devices=1\", \"trainer.num_nodes=1\",\n",
        "        \"trainer.precision=16-mixed\", \"trainer.num_sanity_val_steps=0\",\n",
        "        \"trainer.log_every_n_steps=20\", \"trainer.val_check_interval=100\",\n",
        "        f\"trainer.max_steps={steps}\",\n",
        "        \"data.max_valid_samples=800\", \"data.max_test_samples=100\",\n",
        "        f\"checkpointing.save_dir={save_dir}\",\n",
        "        \"checkpointing.resume_from_ckpt=false\",\n",
        "        \"wandb=null\",\n",
        "        \"loader.global_batch_size=8\", \"loader.eval_global_batch_size=8\",\n",
        "        \"loader.batch_size=8\", \"loader.eval_batch_size=8\",\n",
        "        \"loader.num_workers=2\", \"trainer.accumulate_grad_batches=1\",\n",
        "        f\"block_size={block_size}\",\n",
        "    ]\n",
        "\n",
        "    if from_pretrained:\n",
        "        overrides.append(f\"training.from_pretrained={from_pretrained}\")\n",
        "\n",
        "    if extra_overrides:\n",
        "        overrides.extend(extra_overrides)\n",
        "\n",
        "    log = run_main(overrides)\n",
        "    return ckpt_path, log\n",
        "\n",
        "def eval_run(checkpoint_path, algo, block_size):\n",
        "    overrides = [\n",
        "        \"mode=ppl_eval\",\n",
        "        \"data=lm1b-wrap\", \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\", \"data.max_test_samples=1000\",\n",
        "        \"model=tiny\", \"model.length=128\", \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\", f\"eval.checkpoint_path={checkpoint_path}\",\n",
        "        \"trainer.accelerator=gpu\", \"trainer.devices=1\",\n",
        "        \"trainer.precision=16-mixed\", \"trainer.num_sanity_val_steps=0\",\n",
        "        \"wandb=null\",\n",
        "        \"noise.type=loglinear\",\n",
        "        \"algo.var_min=false\",\n",
        "        \"data.max_train_samples=1000\", \"data.max_valid_samples=100\",\n",
        "        \"loader.global_batch_size=8\", \"loader.batch_size=8\",\n",
        "        f\"block_size={block_size}\",\n",
        "    ]\n",
        "    log = run_main(overrides)\n",
        "    for line in reversed(log.splitlines()):\n",
        "        if \"val/ppl\" in line.lower():\n",
        "            m = re.search(r\"val/ppl.*?([0-9]+(?:\\.[0-9]+)?)\", line, re.IGNORECASE)\n",
        "            if m: return float(m.group(1))\n",
        "    return None\n",
        "\n",
        "def extract_valid_var(log_text, key):\n",
        "    last_val = None\n",
        "    for line in log_text.splitlines():\n",
        "        if f\"VARIANCE: {key}\" in line:\n",
        "            m = re.search(r\"=\\s*([0-9]+\\.?[0-9]*)\", line)\n",
        "            if m: last_val = float(m.group(1))\n",
        "    return last_val"
      ],
      "metadata": {
        "id": "C3sV8-lEI-iP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Run base training (if missing)"
      ],
      "metadata": {
        "id": "fa8gOihXJKmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n{'='*60}\\nSTEP A: BASE MODEL\\n{'='*60}\")\n",
        "base_run_name = \"bd3lm_base_len128_vfinal\"\n",
        "bd3lm_base_ckpt, _ = train_run(\n",
        "    run_name=base_run_name,\n",
        "    algo=\"bd3lm\",\n",
        "    block_size=128,\n",
        "    from_pretrained=None,\n",
        "    extra_overrides=[\"training.resample=false\", \"algo.var_min=false\"],\n",
        "    is_base=True\n",
        ")\n",
        "print(f\"Base Checkpoint: {bd3lm_base_ckpt}\")"
      ],
      "metadata": {
        "id": "Ox6sJQqiJK4v",
        "outputId": "368fb828-750e-4480-ef7e-3cc5342ce57f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP A: BASE MODEL\n",
            "============================================================\n",
            "\n",
            "$ python main.py ... block_size=128 training.resample=false algo.var_min=false\n",
            "Base Checkpoint: /content/repro_runs_final/bd3lm_base_len128_vfinal/checkpoints/last.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Fine-tuning and evaluation"
      ],
      "metadata": {
        "id": "aQWqGnOCJbrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paper_data = {\n",
        "    4:  {\"Clipped U[0.45,0.95]\": 29.21, \"Clipped U[0.3,0.8]\": 29.38, \"Linear U[0,1]\": 30.18, \"Logarithmic\": 30.36, \"Square root\": 31.41},\n",
        "    16: {\"Clipped U[0.45,0.95]\": 31.42, \"Clipped U[0.3,0.8]\": 31.12, \"Linear U[0,1]\": 31.72, \"Square\": 31.43, \"Cosine\": 31.41}\n",
        "}\n",
        "\n",
        "schedules = {\n",
        "    4: [\n",
        "        (\"Clipped U[0.45,0.95]\", [\"training.sampling_eps_min=0.45\", \"training.sampling_eps_max=0.95\"], \"valid_var_0.45 - 0.95\"),\n",
        "        (\"Clipped U[0.3,0.8]\",   [\"training.sampling_eps_min=0.3\", \"training.sampling_eps_max=0.8\"], \"valid_var_0.3 - 0.8\"),\n",
        "        (\"Linear U[0,1]\",        [\"training.sampling_eps_min=0.001\", \"training.sampling_eps_max=1.0\"], \"valid_var_0.0 - 1\"),\n",
        "        (\"Logarithmic\",          [\"noise.type=log\", \"training.sampling_eps_min=0.001\", \"training.sampling_eps_max=1.0\"], \"valid_var_0.0 - 1\"),\n",
        "        (\"Square root\",          [\"noise.type=square_root\", \"training.sampling_eps_min=0.001\", \"training.sampling_eps_max=1.0\"], \"valid_var_0.0 - 1\"),\n",
        "    ],\n",
        "    16: [\n",
        "        (\"Clipped U[0.45,0.95]\", [\"training.sampling_eps_min=0.45\", \"training.sampling_eps_max=0.95\"], \"valid_var_0.45 - 0.95\"),\n",
        "        (\"Clipped U[0.3,0.8]\",   [\"training.sampling_eps_min=0.3\", \"training.sampling_eps_max=0.8\"], \"valid_var_0.3 - 0.8\"),\n",
        "        (\"Linear U[0,1]\",        [\"training.sampling_eps_min=0.001\", \"training.sampling_eps_max=1.0\"], \"valid_var_0.0 - 1\"),\n",
        "        (\"Square\",               [\"noise.type=square\", \"training.sampling_eps_min=0.001\", \"training.sampling_eps_max=1.0\"], \"valid_var_0.0 - 1\"),\n",
        "        (\"Cosine\",               [\"noise.type=cosine\", \"training.sampling_eps_min=0.001\", \"training.sampling_eps_max=1.0\"], \"valid_var_0.0 - 1\"),\n",
        "    ]\n",
        "}\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for Lp in [4, 16]:\n",
        "    print(f\"\\n{'='*70}\\nTABLE 8 ‚Äî Block Size L'={Lp}\\n{'='*70}\")\n",
        "    results = []\n",
        "\n",
        "    for name, sched_ov, var_key in schedules[Lp]:\n",
        "        print(f\"\\n--- Processing: {name} ---\")\n",
        "        safe_name = name.replace(\"[\",\"\").replace(\"]\",\"\").replace(\",\",\"_\").replace(\" \",\"_\")\n",
        "        run_name = f\"bd3lm_fine_{safe_name}_Lp{Lp}\"\n",
        "\n",
        "        # A. TRAIN (With Variance Monitoring)\n",
        "        ckpt, train_log = train_run(\n",
        "            run_name,\n",
        "            algo=\"bd3lm\",\n",
        "            block_size=Lp,\n",
        "            from_pretrained=bd3lm_base_ckpt,\n",
        "            extra_overrides=[\n",
        "                \"training.resample=true\",\n",
        "                \"algo.var_min=true\",\n",
        "                \"algo.clip_search_widths=[0.5]\",\n",
        "                \"algo.fix_clipping=true\",\n",
        "            ] + sched_ov,\n",
        "            is_base=False\n",
        "        )\n",
        "\n",
        "        # B. EXTRACT VARIANCE & EVAL PPL\n",
        "        var_nelbo = extract_valid_var(train_log, key=var_key)\n",
        "        ppl = eval_run(ckpt, algo=\"bd3lm\", block_size=Lp)\n",
        "\n",
        "        print(f\"  ‚úì PPL={ppl:.2f}, Var={var_nelbo if var_nelbo else 'N/A'}\")\n",
        "        results.append({\"Schedule\": name, \"PPL\": ppl, \"Var NELBO\": var_nelbo})\n",
        "\n",
        "    all_results[Lp] = results"
      ],
      "metadata": {
        "id": "HEGMJP38Jb47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbd6bf3b-9d5c-4e17-b6cf-cd9bd8a75b7a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TABLE 8 ‚Äî Block Size L'=4\n",
            "======================================================================\n",
            "\n",
            "--- Processing: Clipped U[0.45,0.95] ---\n",
            "\n",
            "$ python main.py ... algo.fix_clipping=true training.sampling_eps_min=0.45 training.sampling_eps_max=0.95\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=4\n",
            "  ‚úì PPL=1199.43, Var=15.2313\n",
            "\n",
            "--- Processing: Clipped U[0.3,0.8] ---\n",
            "\n",
            "$ python main.py ... algo.fix_clipping=true training.sampling_eps_min=0.3 training.sampling_eps_max=0.8\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=4\n",
            "  ‚úì PPL=1101.03, Var=27.6118\n",
            "\n",
            "--- Processing: Linear U[0,1] ---\n",
            "\n",
            "$ python main.py ... algo.fix_clipping=true training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=4\n",
            "  ‚úì PPL=751.15, Var=260.1035\n",
            "\n",
            "--- Processing: Logarithmic ---\n",
            "\n",
            "$ python main.py ... noise.type=log training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=4\n",
            "  ‚úì PPL=750.05, Var=125.6219\n",
            "\n",
            "--- Processing: Square root ---\n",
            "\n",
            "$ python main.py ... noise.type=square_root training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=4\n",
            "  ‚úì PPL=718.90, Var=83.832\n",
            "\n",
            "======================================================================\n",
            "TABLE 8 ‚Äî Block Size L'=16\n",
            "======================================================================\n",
            "\n",
            "--- Processing: Clipped U[0.45,0.95] ---\n",
            "\n",
            "$ python main.py ... algo.fix_clipping=true training.sampling_eps_min=0.45 training.sampling_eps_max=0.95\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=16\n",
            "  ‚úì PPL=1056.46, Var=4.3476\n",
            "\n",
            "--- Processing: Clipped U[0.3,0.8] ---\n",
            "\n",
            "$ python main.py ... algo.fix_clipping=true training.sampling_eps_min=0.3 training.sampling_eps_max=0.8\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=16\n",
            "  ‚úì PPL=797.62, Var=6.4124\n",
            "\n",
            "--- Processing: Linear U[0,1] ---\n",
            "\n",
            "$ python main.py ... algo.fix_clipping=true training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=16\n",
            "  ‚úì PPL=661.99, Var=44.7271\n",
            "\n",
            "--- Processing: Square ---\n",
            "\n",
            "$ python main.py ... noise.type=square training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=16\n",
            "  ‚úì PPL=627.02, Var=27.5325\n",
            "\n",
            "--- Processing: Cosine ---\n",
            "\n",
            "$ python main.py ... noise.type=cosine training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=16\n",
            "  ‚úì PPL=633.59, Var=20.8008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Log Results"
      ],
      "metadata": {
        "id": "xKQJcu27JmNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for Lp, res_list in all_results.items():\n",
        "    print(f\"\\n\\n=== FINAL RESULTS TABLE 8 (L'={Lp}) ===\")\n",
        "    df = pd.DataFrame(res_list).sort_values(\"PPL\")\n",
        "    # Add Paper Reference\n",
        "    df['Paper PPL'] = df['Schedule'].map(paper_data[Lp])\n",
        "    display(df.style.format({\"PPL\": \"{:.2f}\", \"Var NELBO\": \"{:.4f}\", \"Paper PPL\": \"{:.2f}\"}).background_gradient(subset=[\"PPL\"]))"
      ],
      "metadata": {
        "id": "CVJeF1_XJmZ_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "eb67938a-4424-4783-cd95-f81d0fa442fb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "=== FINAL RESULTS TABLE 8 (L'=4) ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy.rec'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1082346485.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mLp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n\\n=== FINAL RESULTS TABLE 8 (L'={Lp}) ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PPL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Add Paper Reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Paper PPL'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Schedule'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7198\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7199\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7200\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7201\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7202\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/sorting.py\u001b[0m in \u001b[0;36mnargsort\u001b[0;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36misna\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \"\"\"\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_isna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36m_isna\u001b[0;34m(obj, inf_as_na)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_isna_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minf_as_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minf_as_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;31m# Try to use cached isna, which also short-circuits for integer dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36m_isna_array\u001b[0;34m(values, inf_as_na)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;31m# type \"ndarray[Any, dtype[bool_]]\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;31m# GH 48526\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_isna_recarray_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minf_as_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minf_as_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.rec'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}