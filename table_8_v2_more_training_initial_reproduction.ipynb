{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2WZIWHMRrWt"
      },
      "source": [
        "# Table 8 Reproduction v2: More Training\n",
        "\n",
        "**Αλλαγές από v1:**\n",
        "- `max_steps`: 1500 → **3000**\n",
        "- `max_train_samples`: 3000 → **10000**\n",
        "- `algo.var_min`: false → **true**\n",
        "\n",
        "**Στόχος:** Να δούμε αν με περισσότερο training παίρνουμε τη σωστή διάταξη:\n",
        "- L'=4: U[0.45,0.95] < U[0.3,0.8] < Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHvEjMfGRrWw",
        "outputId": "405bd7c5-6079-4172-903f-8b1850b41098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bd3lms'...\n",
            "remote: Enumerating objects: 768, done.\u001b[K\n",
            "remote: Counting objects: 100% (226/226), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 768 (delta 202), reused 176 (delta 176), pack-reused 542 (from 1)\u001b[K\n",
            "Receiving objects: 100% (768/768), 1.78 MiB | 35.67 MiB/s, done.\n",
            "Resolving deltas: 100% (496/496), done.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup - Clone repo (αν δεν υπάρχει)\n",
        "import os\n",
        "if not os.path.exists('/content/bd3lms'):\n",
        "    !cd /content && git clone https://github.com/ntua-el21050/bd3lms.git\n",
        "else:\n",
        "    print(\"Repo already exists, skipping clone\")\n",
        "\n",
        "# Create runs directory\n",
        "!mkdir -p /content/repro_runs_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYhA_AVvRrWx",
        "outputId": "dde5fea0-e19c-4d39-f9f3-ee19d45d3884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m140.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m154.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "xarray 2025.12.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "db-dtypes 1.4.4 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Cell 2: Install dependencies (αν χρειάζεται)\n",
        "!pip install -q \\\n",
        "    torchmetrics==1.6.2 \\\n",
        "    datasets==3.3.2 \\\n",
        "    einops==0.8.1 \\\n",
        "    fsspec==2024.2.0 \\\n",
        "    hydra-core==1.3.2 \\\n",
        "    lightning==2.5.0.post0 \\\n",
        "    omegaconf==2.3.0 \\\n",
        "    packaging==23.2 \\\n",
        "    pandas==2.2.1 \\\n",
        "    rich==13.7.1 \\\n",
        "    scikit-learn==1.5.1 \\\n",
        "    timm==0.9.16 \\\n",
        "    transformers==4.49.0 \\\n",
        "    matplotlib==3.10.0 \\\n",
        "    wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "825T98nqRrWy",
        "outputId": "39d47726-fbbd-4409-e0a5-26d0548bdf91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Helper functions loaded!\n",
            "\n",
            "V2 Settings:\n",
            "  - max_steps: 3000 (was 1500)\n",
            "  - max_train_samples: 10000 (was 3000)\n",
            "  - algo.var_min: true (was false)\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Helper functions\n",
        "import subprocess\n",
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def run_main(overrides, timeout=None):\n",
        "    \"\"\"Run main.py with given overrides.\"\"\"\n",
        "    env = dict(os.environ)\n",
        "    env.setdefault(\"HYDRA_FULL_ERROR\", \"1\")\n",
        "    cmd = [sys.executable, \"-u\", \"bd3lms/main.py\", *overrides]\n",
        "    print(\"\\n$\", \" \".join(cmd))\n",
        "    proc = subprocess.run(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        timeout=timeout,\n",
        "        check=False,\n",
        "        env=env,\n",
        "    )\n",
        "    print(proc.stdout[-4000:])\n",
        "    if proc.returncode != 0:\n",
        "        raise RuntimeError(f\"Command failed with return code {proc.returncode}\")\n",
        "    return proc.stdout\n",
        "\n",
        "def extract_val_ppl(log_text: str):\n",
        "    for line in reversed(log_text.splitlines()):\n",
        "        if \"val/ppl\" in line.lower():\n",
        "            m = re.search(r\"val/ppl.*?([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", line, re.IGNORECASE)\n",
        "            if m:\n",
        "                return float(m.group(1))\n",
        "    return None\n",
        "\n",
        "def _small_loader_overrides(batch_size=8, num_workers=2):\n",
        "    return [\n",
        "        f\"loader.global_batch_size={batch_size}\",\n",
        "        f\"loader.eval_global_batch_size={batch_size}\",\n",
        "        f\"loader.batch_size={batch_size}\",\n",
        "        f\"loader.eval_batch_size={batch_size}\",\n",
        "        f\"loader.num_workers={num_workers}\",\n",
        "        \"trainer.accumulate_grad_batches=1\",\n",
        "    ]\n",
        "\n",
        "def train_run_v2(\n",
        "    run_name,\n",
        "    algo,\n",
        "    block_size=128,\n",
        "    from_pretrained=None,\n",
        "    max_steps=10000,        # Αύξησε από 3000\n",
        "    max_train_samples=20000,  # Αύξησε από 10000\n",
        "    extra_overrides=None,\n",
        "):\n",
        "    \"\"\"Train a model with MORE training.\"\"\"\n",
        "    save_dir = Path(\"/content/repro_runs_v2\") / run_name\n",
        "    if save_dir.exists():\n",
        "        shutil.rmtree(save_dir)\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    overrides = [\n",
        "        \"mode=train\",\n",
        "        \"data=lm1b-wrap\",\n",
        "        \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\",\n",
        "        f\"data.max_train_samples={max_train_samples}\",  # MORE DATA!\n",
        "        \"model=tiny\",\n",
        "        \"model.length=128\",\n",
        "        \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\",\n",
        "        \"trainer.accelerator=gpu\",\n",
        "        \"trainer.devices=1\",\n",
        "        \"trainer.num_nodes=1\",\n",
        "        \"trainer.precision=16-mixed\",\n",
        "        \"trainer.num_sanity_val_steps=0\",\n",
        "        \"trainer.log_every_n_steps=20\",\n",
        "        \"trainer.val_check_interval=100\",  # Less frequent validation\n",
        "        f\"trainer.max_steps={max_steps}\",  # MORE STEPS!\n",
        "        \"data.max_valid_samples=100\",\n",
        "        \"data.max_test_samples=100\",\n",
        "        f\"checkpointing.save_dir=/content/repro_runs_v2/{run_name}\",\n",
        "        \"checkpointing.resume_from_ckpt=false\",\n",
        "        \"wandb=null\",\n",
        "    ]\n",
        "    overrides.extend(_small_loader_overrides(batch_size=8, num_workers=2))\n",
        "\n",
        "    if block_size is not None:\n",
        "        overrides.append(f\"block_size={block_size}\")\n",
        "    if from_pretrained is not None:\n",
        "        overrides.append(f\"training.from_pretrained={from_pretrained}\")\n",
        "    if extra_overrides:\n",
        "        overrides.extend(extra_overrides)\n",
        "\n",
        "    _ = run_main(overrides)\n",
        "    ckpt = save_dir / \"checkpoints\" / \"last.ckpt\"\n",
        "    if not ckpt.exists():\n",
        "        raise FileNotFoundError(f\"Expected checkpoint not found: {ckpt}\")\n",
        "    return str(ckpt)\n",
        "\n",
        "def eval_run(algo, checkpoint_path, block_size=None, extra_overrides=None):\n",
        "    \"\"\"Evaluate perplexity.\"\"\"\n",
        "    overrides = [\n",
        "        \"mode=ppl_eval\",\n",
        "        \"data=lm1b-wrap\",\n",
        "        \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\",\n",
        "        \"data.max_test_samples=1000\",\n",
        "        \"model=tiny\",\n",
        "        \"model.length=128\",\n",
        "        \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\",\n",
        "        f\"eval.checkpoint_path={checkpoint_path}\",\n",
        "        \"trainer.accelerator=gpu\",\n",
        "        \"trainer.devices=1\",\n",
        "        \"trainer.num_nodes=1\",\n",
        "        \"trainer.precision=16-mixed\",\n",
        "        \"trainer.num_sanity_val_steps=0\",\n",
        "        \"wandb=null\",\n",
        "    ]\n",
        "    overrides.extend(_small_loader_overrides(batch_size=8, num_workers=2))\n",
        "\n",
        "    if block_size is not None:\n",
        "        overrides.append(f\"block_size={block_size}\")\n",
        "    if extra_overrides:\n",
        "        overrides.extend(extra_overrides)\n",
        "\n",
        "    log_text = run_main(overrides)\n",
        "    ppl = extract_val_ppl(log_text)\n",
        "    if ppl is None:\n",
        "        raise ValueError(\"Could not parse val/ppl from output.\")\n",
        "    return ppl\n",
        "\n",
        "print(\"✓ Helper functions loaded!\")\n",
        "print(\"\\nV2 Settings:\")\n",
        "print(\"  - max_steps: 3000 (was 1500)\")\n",
        "print(\"  - max_train_samples: 10000 (was 3000)\")\n",
        "print(\"  - algo.var_min: true (was false)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mDz-_hBRrWz"
      },
      "source": [
        "## Step 1: Check for Existing Base Checkpoint\n",
        "\n",
        "Χρησιμοποιούμε το υπάρχον base checkpoint αν υπάρχει, αλλιώς κάνουμε train νέο."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2reFd58jRrWz",
        "outputId": "b2af91bd-bb8b-425f-cb30-c14e139b0c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training new base checkpoint...\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=40000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=20 trainer.val_check_interval=100 trainer.max_steps=20000 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs_v2/bd3lm_base_len128_v3 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=128 training.resample=false algo.var_min=false\n",
            "2.07it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  86%|████████▌ | 1000/1166 [01:18<00:12, 12.81it/s, v_num=0]Epoch 16, global step 19656: 'val/nll' reached 5.11897 (best 5.11897), saving model to '/content/repro_runs_v2/bd3lm_base_len128_v3/checkpoints/best.ckpt' as top 1\n",
            "\n",
            "Epoch 16:  87%|████████▋ | 1020/1166 [01:21<00:11, 12.52it/s, v_num=0]\n",
            "Epoch 16:  87%|████████▋ | 1020/1166 [01:21<00:11, 12.52it/s, v_num=0]\n",
            "Epoch 16:  89%|████████▉ | 1040/1166 [01:22<00:10, 12.58it/s, v_num=0]\n",
            "Epoch 16:  89%|████████▉ | 1040/1166 [01:22<00:10, 12.58it/s, v_num=0]\n",
            "Epoch 16:  91%|█████████ | 1060/1166 [01:23<00:08, 12.63it/s, v_num=0]\n",
            "Epoch 16:  91%|█████████ | 1060/1166 [01:23<00:08, 12.63it/s, v_num=0]\n",
            "Epoch 16:  93%|█████████▎| 1080/1166 [01:25<00:06, 12.68it/s, v_num=0]\n",
            "Epoch 16:  93%|█████████▎| 1080/1166 [01:25<00:06, 12.68it/s, v_num=0]\n",
            "Epoch 16:  94%|█████████▍| 1100/1166 [01:26<00:05, 12.74it/s, v_num=0]\n",
            "Epoch 16:  94%|█████████▍| 1100/1166 [01:26<00:05, 12.74it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 42.95it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  94%|█████████▍| 1100/1166 [01:26<00:05, 12.70it/s, v_num=0]Epoch 16, global step 19756: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16:  96%|█████████▌| 1120/1166 [01:28<00:03, 12.60it/s, v_num=0]\n",
            "Epoch 16:  96%|█████████▌| 1120/1166 [01:28<00:03, 12.60it/s, v_num=0]\n",
            "Epoch 16:  98%|█████████▊| 1140/1166 [01:30<00:02, 12.65it/s, v_num=0]\n",
            "Epoch 16:  98%|█████████▊| 1140/1166 [01:30<00:02, 12.65it/s, v_num=0]\n",
            "Epoch 16:  99%|█████████▉| 1160/1166 [01:31<00:00, 12.69it/s, v_num=0]\n",
            "Epoch 16:  99%|█████████▉| 1160/1166 [01:31<00:00, 12.69it/s, v_num=0]\n",
            "Epoch 16: 100%|██████████| 1166/1166 [01:31<00:00, 12.70it/s, v_num=0]\n",
            "Epoch 16: 100%|██████████| 1166/1166 [01:31<00:00, 12.70it/s, v_num=0]\n",
            "Epoch 16: 100%|██████████| 1166/1166 [01:31<00:00, 12.70it/s, v_num=0]\n",
            "Epoch 16:   0%|          | 0/1166 [00:00<?, ?it/s, v_num=0]           \n",
            "Epoch 17:   0%|          | 0/1166 [00:00<?, ?it/s, v_num=0]\n",
            "Epoch 17:   2%|▏         | 20/1166 [00:01<01:11, 15.92it/s, v_num=0]\n",
            "Epoch 17:   2%|▏         | 20/1166 [00:01<01:12, 15.92it/s, v_num=0]\n",
            "Epoch 17:   3%|▎         | 40/1166 [00:02<01:10, 16.05it/s, v_num=0]\n",
            "Epoch 17:   3%|▎         | 40/1166 [00:02<01:10, 16.05it/s, v_num=0]\n",
            "Epoch 17:   5%|▌         | 60/1166 [00:03<01:08, 16.10it/s, v_num=0]\n",
            "Epoch 17:   5%|▌         | 60/1166 [00:03<01:08, 16.10it/s, v_num=0]\n",
            "Epoch 17:   7%|▋         | 80/1166 [00:04<01:07, 16.08it/s, v_num=0]\n",
            "Epoch 17:   7%|▋         | 80/1166 [00:04<01:07, 16.08it/s, v_num=0]\n",
            "Epoch 17:   9%|▊         | 100/1166 [00:06<01:06, 16.09it/s, v_num=0]\n",
            "Epoch 17:   9%|▊         | 100/1166 [00:06<01:06, 16.09it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 41.85it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 17:   9%|▊         | 100/1166 [00:06<01:08, 15.49it/s, v_num=0]Epoch 17, global step 19922: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 17:  10%|█         | 120/1166 [00:08<01:16, 13.61it/s, v_num=0]\n",
            "Epoch 17:  10%|█         | 120/1166 [00:08<01:16, 13.61it/s, v_num=0]\n",
            "Epoch 17:  12%|█▏        | 140/1166 [00:10<01:13, 13.94it/s, v_num=0]\n",
            "Epoch 17:  12%|█▏        | 140/1166 [00:10<01:13, 13.94it/s, v_num=0]\n",
            "Epoch 17:  14%|█▎        | 160/1166 [00:11<01:11, 14.16it/s, v_num=0]\n",
            "Epoch 17:  14%|█▎        | 160/1166 [00:11<01:11, 14.15it/s, v_num=0]\n",
            "Epoch 17:  14%|█▎        | 160/1166 [00:14<01:30, 11.09it/s, v_num=0]`Trainer.fit` stopped: `max_steps=20000` reached.\n",
            "\n",
            "Epoch 17:  14%|█▎        | 160/1166 [00:14<01:30, 11.09it/s, v_num=0]\n",
            "\n",
            "✓ Trained new base checkpoint: /content/repro_runs_v2/bd3lm_base_len128_v3/checkpoints/last.ckpt\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Get or Train Base Checkpoint\n",
        "from pathlib import Path\n",
        "\n",
        "# Cell 4: Train Base Checkpoint (Fresh)\n",
        "print(\"Training new base checkpoint...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "bd3lm_base_ckpt = train_run_v2(\n",
        "    \"bd3lm_base_len128_v3\",  # Νέο όνομα για να μην χρησιμοποιήσει το παλιό\n",
        "    algo=\"bd3lm\",\n",
        "    block_size=128,\n",
        "    max_steps=20000,\n",
        "    max_train_samples=40000,\n",
        "    extra_overrides=[\n",
        "        \"training.resample=false\",\n",
        "        \"algo.var_min=false\",\n",
        "    ],\n",
        ")\n",
        "print(f\"✓ Trained new base checkpoint: {bd3lm_base_ckpt}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py8kk0QARrWz"
      },
      "source": [
        "## Step 2: Table 8 Experiments with MORE Training\n",
        "\n",
        "**Νέες Ρυθμίσεις:**\n",
        "- `max_steps=3000` (2x από πριν)\n",
        "- `max_train_samples=10000` (3x από πριν)\n",
        "- `algo.var_min=true` (ενεργοποιεί variance minimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSzgvibiRrWz",
        "outputId": "b0a0398c-dd1e-4afc-df4b-b2b4623be65c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TABLE 8 v2: MORE TRAINING (L' = 4)\n",
            "Settings: max_steps=3000, max_train_samples=10000, var_min=true\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "--- Linear U[0,1] ---\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=25000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=20 trainer.val_check_interval=100 trainer.max_steps=11000 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs_v2/bd3lm_v2_Linear_U0_1_Lp4 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 training.from_pretrained=/content/repro_runs_v2/bd3lm_base_len128_v3/checkpoints/last.ckpt training.resample=true algo.var_min=true training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "_num=0]\n",
            "Epoch 14:  91%|█████████ | 660/728 [00:50<00:05, 12.96it/s, v_num=0]\n",
            "Epoch 14:  93%|█████████▎| 680/728 [00:52<00:03, 13.03it/s, v_num=0]\n",
            "Epoch 14:  93%|█████████▎| 680/728 [00:52<00:03, 13.03it/s, v_num=0]\n",
            "Epoch 14:  96%|█████████▌| 700/728 [00:53<00:02, 13.09it/s, v_num=0]\n",
            "Epoch 14:  96%|█████████▌| 700/728 [00:53<00:02, 13.09it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 42.80it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 14:  96%|█████████▌| 700/728 [00:53<00:02, 13.04it/s, v_num=0]Epoch 14, global step 10714: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 14:  99%|█████████▉| 720/728 [00:55<00:00, 12.88it/s, v_num=0]\n",
            "Epoch 14:  99%|█████████▉| 720/728 [00:55<00:00, 12.88it/s, v_num=0]\n",
            "Epoch 14: 100%|██████████| 728/728 [00:56<00:00, 12.91it/s, v_num=0]\n",
            "Epoch 14: 100%|██████████| 728/728 [00:56<00:00, 12.91it/s, v_num=0]\n",
            "Epoch 14: 100%|██████████| 728/728 [00:56<00:00, 12.91it/s, v_num=0]\n",
            "Epoch 14:   0%|          | 0/728 [00:00<?, ?it/s, v_num=0]          \n",
            "Epoch 15:   0%|          | 0/728 [00:00<?, ?it/s, v_num=0]\n",
            "Epoch 15:   3%|▎         | 20/728 [00:01<00:43, 16.26it/s, v_num=0]\n",
            "Epoch 15:   3%|▎         | 20/728 [00:01<00:43, 16.26it/s, v_num=0]\n",
            "Epoch 15:   5%|▌         | 40/728 [00:02<00:42, 16.28it/s, v_num=0]\n",
            "Epoch 15:   5%|▌         | 40/728 [00:02<00:42, 16.28it/s, v_num=0]\n",
            "Epoch 15:   8%|▊         | 60/728 [00:03<00:41, 16.22it/s, v_num=0]\n",
            "Epoch 15:   8%|▊         | 60/728 [00:03<00:41, 16.22it/s, v_num=0]\n",
            "Epoch 15:  11%|█         | 80/728 [00:04<00:40, 16.13it/s, v_num=0]\n",
            "Epoch 15:  11%|█         | 80/728 [00:04<00:40, 16.13it/s, v_num=0]\n",
            "Epoch 15:  14%|█▎        | 100/728 [00:06<00:38, 16.21it/s, v_num=0]\n",
            "Epoch 15:  14%|█▎        | 100/728 [00:06<00:38, 16.21it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 42.82it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 15:  14%|█▎        | 100/728 [00:06<00:40, 15.60it/s, v_num=0]Epoch 15, global step 10842: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 15:  16%|█▋        | 120/728 [00:08<00:44, 13.74it/s, v_num=0]\n",
            "Epoch 15:  16%|█▋        | 120/728 [00:08<00:44, 13.74it/s, v_num=0]\n",
            "Epoch 15:  19%|█▉        | 140/728 [00:09<00:41, 14.02it/s, v_num=0]\n",
            "Epoch 15:  19%|█▉        | 140/728 [00:09<00:41, 14.02it/s, v_num=0]\n",
            "Epoch 15:  22%|██▏       | 160/728 [00:11<00:39, 14.27it/s, v_num=0]\n",
            "Epoch 15:  22%|██▏       | 160/728 [00:11<00:39, 14.27it/s, v_num=0]\n",
            "Epoch 15:  25%|██▍       | 180/728 [00:12<00:37, 14.48it/s, v_num=0]\n",
            "Epoch 15:  25%|██▍       | 180/728 [00:12<00:37, 14.48it/s, v_num=0]\n",
            "Epoch 15:  27%|██▋       | 200/728 [00:13<00:36, 14.66it/s, v_num=0]\n",
            "Epoch 15:  27%|██▋       | 200/728 [00:13<00:36, 14.66it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 42.63it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 15:  27%|██▋       | 200/728 [00:13<00:36, 14.40it/s, v_num=0]Epoch 15, global step 10942: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 15:  30%|███       | 220/728 [00:16<00:37, 13.61it/s, v_num=0]\n",
            "Epoch 15:  30%|███       | 220/728 [00:16<00:37, 13.61it/s, v_num=0]\n",
            "Epoch 15:  33%|███▎      | 240/728 [00:17<00:35, 13.79it/s, v_num=0]\n",
            "Epoch 15:  33%|███▎      | 240/728 [00:17<00:35, 13.79it/s, v_num=0]\n",
            "Epoch 15:  33%|███▎      | 240/728 [00:20<00:41, 11.75it/s, v_num=0]`Trainer.fit` stopped: `max_steps=11000` reached.\n",
            "\n",
            "Epoch 15:  33%|███▎      | 240/728 [00:20<00:41, 11.75it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs_v2/bd3lm_v2_Linear_U0_1_Lp4/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 algo.var_min=false\n",
            "e using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-01-11 18:06:59.267355: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-11 18:06:59.283558: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768154819.302169   78817 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768154819.307551   78817 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768154819.321628   78817 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768154819.321650   78817 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768154819.321652   78817 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768154819.321654   78817 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-11 18:06:59.326041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\n",
            "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 19.74it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 24.65it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 24.04it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │     8.005597114562988     │\n",
            "│          val/nll          │    5.5490570068359375     │\n",
            "│          val/ppl          │      256.9951171875       │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "\n",
            "✓ Linear U[0,1]: PPL = 257.00\n",
            "\n",
            "============================================================\n",
            "--- Clipped U[0.3,0.8] ---\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=25000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=20 trainer.val_check_interval=100 trainer.max_steps=11000 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs_v2/bd3lm_v2_Clipped_U0.3_0.8_Lp4 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 training.from_pretrained=/content/repro_runs_v2/bd3lm_base_len128_v3/checkpoints/last.ckpt training.resample=true algo.var_min=true training.sampling_eps_min=0.3 training.sampling_eps_max=0.8\n",
            "_num=0]\n",
            "Epoch 14:  91%|█████████ | 660/728 [00:51<00:05, 12.89it/s, v_num=0]\n",
            "Epoch 14:  93%|█████████▎| 680/728 [00:52<00:03, 12.97it/s, v_num=0]\n",
            "Epoch 14:  93%|█████████▎| 680/728 [00:52<00:03, 12.97it/s, v_num=0]\n",
            "Epoch 14:  96%|█████████▌| 700/728 [00:53<00:02, 13.05it/s, v_num=0]\n",
            "Epoch 14:  96%|█████████▌| 700/728 [00:53<00:02, 13.05it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 43.32it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 14:  96%|█████████▌| 700/728 [00:53<00:02, 12.99it/s, v_num=0]Epoch 14, global step 10714: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 14:  99%|█████████▉| 720/728 [00:56<00:00, 12.83it/s, v_num=0]\n",
            "Epoch 14:  99%|█████████▉| 720/728 [00:56<00:00, 12.83it/s, v_num=0]\n",
            "Epoch 14: 100%|██████████| 728/728 [00:56<00:00, 12.86it/s, v_num=0]\n",
            "Epoch 14: 100%|██████████| 728/728 [00:56<00:00, 12.86it/s, v_num=0]\n",
            "Epoch 14: 100%|██████████| 728/728 [00:56<00:00, 12.86it/s, v_num=0]\n",
            "Epoch 14:   0%|          | 0/728 [00:00<?, ?it/s, v_num=0]          \n",
            "Epoch 15:   0%|          | 0/728 [00:00<?, ?it/s, v_num=0]\n",
            "Epoch 15:   3%|▎         | 20/728 [00:01<00:43, 16.21it/s, v_num=0]\n",
            "Epoch 15:   3%|▎         | 20/728 [00:01<00:43, 16.21it/s, v_num=0]\n",
            "Epoch 15:   5%|▌         | 40/728 [00:02<00:41, 16.53it/s, v_num=0]\n",
            "Epoch 15:   5%|▌         | 40/728 [00:02<00:41, 16.53it/s, v_num=0]\n",
            "Epoch 15:   8%|▊         | 60/728 [00:03<00:40, 16.49it/s, v_num=0]\n",
            "Epoch 15:   8%|▊         | 60/728 [00:03<00:40, 16.49it/s, v_num=0]\n",
            "Epoch 15:  11%|█         | 80/728 [00:04<00:39, 16.41it/s, v_num=0]\n",
            "Epoch 15:  11%|█         | 80/728 [00:04<00:39, 16.41it/s, v_num=0]\n",
            "Epoch 15:  14%|█▎        | 100/728 [00:06<00:38, 16.30it/s, v_num=0]\n",
            "Epoch 15:  14%|█▎        | 100/728 [00:06<00:38, 16.30it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 43.06it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 15:  14%|█▎        | 100/728 [00:06<00:40, 15.67it/s, v_num=0]Epoch 15, global step 10842: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 15:  16%|█▋        | 120/728 [00:08<00:43, 13.92it/s, v_num=0]\n",
            "Epoch 15:  16%|█▋        | 120/728 [00:08<00:43, 13.92it/s, v_num=0]\n",
            "Epoch 15:  19%|█▉        | 140/728 [00:09<00:41, 14.15it/s, v_num=0]\n",
            "Epoch 15:  19%|█▉        | 140/728 [00:09<00:41, 14.15it/s, v_num=0]\n",
            "Epoch 15:  22%|██▏       | 160/728 [00:11<00:39, 14.37it/s, v_num=0]\n",
            "Epoch 15:  22%|██▏       | 160/728 [00:11<00:39, 14.36it/s, v_num=0]\n",
            "Epoch 15:  25%|██▍       | 180/728 [00:12<00:37, 14.58it/s, v_num=0]\n",
            "Epoch 15:  25%|██▍       | 180/728 [00:12<00:37, 14.58it/s, v_num=0]\n",
            "Epoch 15:  27%|██▋       | 200/728 [00:13<00:35, 14.76it/s, v_num=0]\n",
            "Epoch 15:  27%|██▋       | 200/728 [00:13<00:35, 14.76it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 43.63it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 15:  27%|██▋       | 200/728 [00:13<00:36, 14.50it/s, v_num=0]Epoch 15, global step 10942: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 15:  30%|███       | 220/728 [00:16<00:37, 13.72it/s, v_num=0]\n",
            "Epoch 15:  30%|███       | 220/728 [00:16<00:37, 13.72it/s, v_num=0]\n",
            "Epoch 15:  33%|███▎      | 240/728 [00:17<00:35, 13.86it/s, v_num=0]\n",
            "Epoch 15:  33%|███▎      | 240/728 [00:17<00:35, 13.86it/s, v_num=0]\n",
            "Epoch 15:  33%|███▎      | 240/728 [00:20<00:41, 11.83it/s, v_num=0]`Trainer.fit` stopped: `max_steps=11000` reached.\n",
            "\n",
            "Epoch 15:  33%|███▎      | 240/728 [00:20<00:41, 11.82it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs_v2/bd3lm_v2_Clipped_U0.3_0.8_Lp4/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 algo.var_min=false\n",
            "e using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-01-11 18:26:09.386792: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-11 18:26:09.403674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768155969.422597   84715 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768155969.428129   84715 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768155969.442471   84715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768155969.442498   84715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768155969.442501   84715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768155969.442503   84715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-11 18:26:09.446768: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\n",
            "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 19.58it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 24.35it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 23.73it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │     8.016364097595215     │\n",
            "│          val/nll          │     5.556519985198975     │\n",
            "│          val/ppl          │     258.9201965332031     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "\n",
            "✓ Clipped U[0.3,0.8]: PPL = 258.92\n",
            "\n",
            "============================================================\n",
            "--- Clipped U[0.45,0.95] ---\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=25000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=20 trainer.val_check_interval=100 trainer.max_steps=11000 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs_v2/bd3lm_v2_Clipped_U0.45_0.95_Lp4 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 training.from_pretrained=/content/repro_runs_v2/bd3lm_base_len128_v3/checkpoints/last.ckpt training.resample=true algo.var_min=true training.sampling_eps_min=0.45 training.sampling_eps_max=0.95\n",
            "_num=0]\n",
            "Epoch 14:  91%|█████████ | 660/728 [00:49<00:05, 13.26it/s, v_num=0]\n",
            "Epoch 14:  93%|█████████▎| 680/728 [00:51<00:03, 13.33it/s, v_num=0]\n",
            "Epoch 14:  93%|█████████▎| 680/728 [00:51<00:03, 13.33it/s, v_num=0]\n",
            "Epoch 14:  96%|█████████▌| 700/728 [00:52<00:02, 13.40it/s, v_num=0]\n",
            "Epoch 14:  96%|█████████▌| 700/728 [00:52<00:02, 13.40it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 42.39it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 14:  96%|█████████▌| 700/728 [00:52<00:02, 13.33it/s, v_num=0]Epoch 14, global step 10714: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 14:  99%|█████████▉| 720/728 [00:54<00:00, 13.15it/s, v_num=0]\n",
            "Epoch 14:  99%|█████████▉| 720/728 [00:54<00:00, 13.15it/s, v_num=0]\n",
            "Epoch 14: 100%|██████████| 728/728 [00:55<00:00, 13.17it/s, v_num=0]\n",
            "Epoch 14: 100%|██████████| 728/728 [00:55<00:00, 13.17it/s, v_num=0]\n",
            "Epoch 14: 100%|██████████| 728/728 [00:55<00:00, 13.17it/s, v_num=0]\n",
            "Epoch 14:   0%|          | 0/728 [00:00<?, ?it/s, v_num=0]          \n",
            "Epoch 15:   0%|          | 0/728 [00:00<?, ?it/s, v_num=0]\n",
            "Epoch 15:   3%|▎         | 20/728 [00:01<00:43, 16.30it/s, v_num=0]\n",
            "Epoch 15:   3%|▎         | 20/728 [00:01<00:43, 16.30it/s, v_num=0]\n",
            "Epoch 15:   5%|▌         | 40/728 [00:02<00:41, 16.46it/s, v_num=0]\n",
            "Epoch 15:   5%|▌         | 40/728 [00:02<00:41, 16.46it/s, v_num=0]\n",
            "Epoch 15:   8%|▊         | 60/728 [00:03<00:40, 16.56it/s, v_num=0]\n",
            "Epoch 15:   8%|▊         | 60/728 [00:03<00:40, 16.55it/s, v_num=0]\n",
            "Epoch 15:  11%|█         | 80/728 [00:04<00:39, 16.60it/s, v_num=0]\n",
            "Epoch 15:  11%|█         | 80/728 [00:04<00:39, 16.60it/s, v_num=0]\n",
            "Epoch 15:  14%|█▎        | 100/728 [00:06<00:37, 16.62it/s, v_num=0]\n",
            "Epoch 15:  14%|█▎        | 100/728 [00:06<00:37, 16.62it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 42.86it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 15:  14%|█▎        | 100/728 [00:06<00:39, 15.96it/s, v_num=0]Epoch 15, global step 10842: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 15:  16%|█▋        | 120/728 [00:08<00:43, 13.94it/s, v_num=0]\n",
            "Epoch 15:  16%|█▋        | 120/728 [00:08<00:43, 13.94it/s, v_num=0]\n",
            "Epoch 15:  19%|█▉        | 140/728 [00:09<00:41, 14.25it/s, v_num=0]\n",
            "Epoch 15:  19%|█▉        | 140/728 [00:09<00:41, 14.24it/s, v_num=0]\n",
            "Epoch 15:  22%|██▏       | 160/728 [00:11<00:39, 14.46it/s, v_num=0]\n",
            "Epoch 15:  22%|██▏       | 160/728 [00:11<00:39, 14.45it/s, v_num=0]\n",
            "Epoch 15:  25%|██▍       | 180/728 [00:12<00:37, 14.60it/s, v_num=0]\n",
            "Epoch 15:  25%|██▍       | 180/728 [00:12<00:37, 14.60it/s, v_num=0]\n",
            "Epoch 15:  27%|██▋       | 200/728 [00:13<00:35, 14.73it/s, v_num=0]\n",
            "Epoch 15:  27%|██▋       | 200/728 [00:13<00:35, 14.73it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 43.24it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 15:  27%|██▋       | 200/728 [00:13<00:36, 14.47it/s, v_num=0]Epoch 15, global step 10942: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 15:  30%|███       | 220/728 [00:16<00:37, 13.60it/s, v_num=0]\n",
            "Epoch 15:  30%|███       | 220/728 [00:16<00:37, 13.60it/s, v_num=0]\n",
            "Epoch 15:  33%|███▎      | 240/728 [00:17<00:35, 13.80it/s, v_num=0]\n",
            "Epoch 15:  33%|███▎      | 240/728 [00:17<00:35, 13.80it/s, v_num=0]\n",
            "Epoch 15:  33%|███▎      | 240/728 [00:20<00:41, 11.82it/s, v_num=0]`Trainer.fit` stopped: `max_steps=11000` reached.\n",
            "\n",
            "Epoch 15:  33%|███▎      | 240/728 [00:20<00:41, 11.81it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs_v2/bd3lm_v2_Clipped_U0.45_0.95_Lp4/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 algo.var_min=false\n",
            "e using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-01-11 18:45:07.535046: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-11 18:45:07.551707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768157107.571075   90657 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768157107.576763   90657 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768157107.591255   90657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768157107.591276   90657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768157107.591278   90657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768157107.591280   90657 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-11 18:45:07.595670: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\n",
            "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 19.93it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 24.85it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 24.25it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │     8.025533676147461     │\n",
            "│          val/nll          │     5.562876224517822     │\n",
            "│          val/ppl          │    260.57122802734375     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "\n",
            "✓ Clipped U[0.45,0.95]: PPL = 260.57\n",
            "\n",
            "============================================================\n",
            "ALL EXPERIMENTS COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Table 8 Experiments - V2 with MORE TRAINING\n",
        "\n",
        "# Define noise schedules\n",
        "noise_schedules = [\n",
        "    (\"Linear U[0,1]\", [\n",
        "        \"training.sampling_eps_min=0.001\",\n",
        "        \"training.sampling_eps_max=1.0\",\n",
        "    ]),\n",
        "    (\"Clipped U[0.3,0.8]\", [\n",
        "        \"training.sampling_eps_min=0.3\",\n",
        "        \"training.sampling_eps_max=0.8\",\n",
        "    ]),\n",
        "    (\"Clipped U[0.45,0.95]\", [\n",
        "        \"training.sampling_eps_min=0.45\",\n",
        "        \"training.sampling_eps_max=0.95\",\n",
        "    ]),\n",
        "]\n",
        "\n",
        "# Only L'=4 for now (faster testing)\n",
        "Lprime = 4\n",
        "\n",
        "results_v2 = []\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"TABLE 8 v2: MORE TRAINING (L' = {Lprime})\")\n",
        "print(\"Settings: max_steps=3000, max_train_samples=10000, var_min=true\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for schedule_name, schedule_overrides in noise_schedules:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"--- {schedule_name} ---\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Create safe run name\n",
        "    safe_name = schedule_name.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"_\").replace(\" \", \"_\")\n",
        "    run_name = f\"bd3lm_v2_{safe_name}_Lp{Lprime}\"\n",
        "\n",
        "    # Fine-tune with MORE STEPS and var_min=true\n",
        "    finetune_ckpt = train_run_v2(\n",
        "        run_name,\n",
        "        algo=\"bd3lm\",\n",
        "        block_size=Lprime,\n",
        "        from_pretrained=bd3lm_base_ckpt,\n",
        "        max_steps=11000,        # 2x MORE!\n",
        "        max_train_samples=25000,  # 3x MORE!\n",
        "        extra_overrides=[\n",
        "            \"training.resample=true\",\n",
        "            \"algo.var_min=true\",  # ENABLED!\n",
        "        ] + schedule_overrides,\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    ppl = eval_run(\n",
        "        algo=\"bd3lm\",\n",
        "        checkpoint_path=finetune_ckpt,\n",
        "        block_size=Lprime,\n",
        "        extra_overrides=[\"algo.var_min=false\"],\n",
        "    )\n",
        "\n",
        "    results_v2.append({\n",
        "        \"schedule\": schedule_name,\n",
        "        \"ppl\": ppl,\n",
        "    })\n",
        "    print(f\"\\n✓ {schedule_name}: PPL = {ppl:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ALL EXPERIMENTS COMPLETE!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7B936IoRrW0",
        "outputId": "199ca3a5-ba7a-4661-a9e6-86a1c924cf30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "RESULTS COMPARISON: V1 vs V2\n",
            "======================================================================\n",
            "\n",
            "| Schedule | Paper | V1 (1500 steps) | V2 (3000 steps) |\n",
            "|--------------------|--------|-----------------|-----------------|\n",
            "| Linear U[0,1]      | 30.18  | 1301.36         | 257.00          |\n",
            "| Clipped U[0.3,0.8] | 29.38  | 1619.97         | 258.92          |\n",
            "| Clipped U[0.45,0.95] | 29.21  | 1805.48         | 260.57          |\n",
            "\n",
            "======================================================================\n",
            "V2 RANKING (sorted by PPL, lower is better):\n",
            "======================================================================\n",
            "🥇 #1: Linear U[0,1]             PPL = 257.00\n",
            "🥈 #2: Clipped U[0.3,0.8]        PPL = 258.92\n",
            "🥉 #3: Clipped U[0.45,0.95]      PPL = 260.57\n",
            "\n",
            "======================================================================\n",
            "PAPER EXPECTED ORDER (L'=4):\n",
            "  #1: Clipped U[0.45,0.95] (Best - heavy masking)\n",
            "  #2: Clipped U[0.3,0.8]\n",
            "  #3: Linear U[0,1] (Worst)\n",
            "======================================================================\n",
            "\n",
            "⚠️ Order doesn't match paper, but PPL improved!\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Compare Results\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"RESULTS COMPARISON: V1 vs V2\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# V1 results (from previous run)\n",
        "v1_results = {\n",
        "    \"Linear U[0,1]\": 1301.36,\n",
        "    \"Clipped U[0.3,0.8]\": 1619.97,\n",
        "    \"Clipped U[0.45,0.95]\": 1805.48,\n",
        "}\n",
        "\n",
        "# Paper results\n",
        "paper_results = {\n",
        "    \"Linear U[0,1]\": 30.18,\n",
        "    \"Clipped U[0.3,0.8]\": 29.38,\n",
        "    \"Clipped U[0.45,0.95]\": 29.21,\n",
        "}\n",
        "\n",
        "print(\"\\n| Schedule | Paper | V1 (1500 steps) | V2 (3000 steps) |\")\n",
        "print(\"|\" + \"-\"*20 + \"|\" + \"-\"*8 + \"|\" + \"-\"*17 + \"|\" + \"-\"*17 + \"|\")\n",
        "\n",
        "for r in results_v2:\n",
        "    schedule = r['schedule']\n",
        "    v2_ppl = r['ppl']\n",
        "    v1_ppl = v1_results.get(schedule, 'N/A')\n",
        "    paper_ppl = paper_results.get(schedule, 'N/A')\n",
        "    print(f\"| {schedule:<18} | {paper_ppl:<6} | {v1_ppl:<15.2f} | {v2_ppl:<15.2f} |\")\n",
        "\n",
        "# Sort by PPL\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"V2 RANKING (sorted by PPL, lower is better):\")\n",
        "print(\"=\" * 70)\n",
        "for i, r in enumerate(sorted(results_v2, key=lambda x: x['ppl']), 1):\n",
        "    medal = [\"🥇\", \"🥈\", \"🥉\"][i-1] if i <= 3 else \"  \"\n",
        "    print(f\"{medal} #{i}: {r['schedule']:<25} PPL = {r['ppl']:.2f}\")\n",
        "\n",
        "# Check if order matches paper\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PAPER EXPECTED ORDER (L'=4):\")\n",
        "print(\"  #1: Clipped U[0.45,0.95] (Best - heavy masking)\")\n",
        "print(\"  #2: Clipped U[0.3,0.8]\")\n",
        "print(\"  #3: Linear U[0,1] (Worst)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "v2_sorted = sorted(results_v2, key=lambda x: x['ppl'])\n",
        "if \"0.45\" in v2_sorted[0]['schedule']:\n",
        "    print(\"\\n✅ SUCCESS! Order matches paper!\")\n",
        "elif v2_sorted[0]['ppl'] < v1_results.get(v2_sorted[0]['schedule'], float('inf')):\n",
        "    print(\"\\n⚠️ Order doesn't match paper, but PPL improved!\")\n",
        "else:\n",
        "    print(\"\\n❌ Order still doesn't match - may need even more training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsLUpArhRrW0"
      },
      "source": [
        "## Optional: Run L'=16 if V2 shows improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QPrebgW3RrW0",
        "outputId": "9145007c-0ebb-4957-997f-bf8401928a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TABLE 8 v2: L' = 16\n",
            "============================================================\n",
            "\n",
            "--- Linear U[0,1] ---\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=20000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=20 trainer.val_check_interval=100 trainer.max_steps=5000 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs_v2/bd3lm_v2_Linear_U0_1_Lp16 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=16 training.from_pretrained=/content/repro_runs_v2/bd3lm_base_len128_v3/checkpoints/last.ckpt training.resample=true algo.var_min=true training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 44.57it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 8:  34%|███▍      | 200/582 [00:15<00:29, 12.78it/s, v_num=0]Epoch 8, global step 4678: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 8:  38%|███▊      | 220/582 [00:17<00:29, 12.37it/s, v_num=0]\n",
            "Epoch 8:  38%|███▊      | 220/582 [00:17<00:29, 12.37it/s, v_num=0]\n",
            "Epoch 8:  41%|████      | 240/582 [00:19<00:27, 12.62it/s, v_num=0]\n",
            "Epoch 8:  41%|████      | 240/582 [00:19<00:27, 12.62it/s, v_num=0]\n",
            "Epoch 8:  45%|████▍     | 260/582 [00:20<00:25, 12.85it/s, v_num=0]\n",
            "Epoch 8:  45%|████▍     | 260/582 [00:20<00:25, 12.85it/s, v_num=0]\n",
            "Epoch 8:  48%|████▊     | 280/582 [00:21<00:23, 13.04it/s, v_num=0]\n",
            "Epoch 8:  48%|████▊     | 280/582 [00:21<00:23, 13.04it/s, v_num=0]\n",
            "Epoch 8:  52%|█████▏    | 300/582 [00:22<00:21, 13.21it/s, v_num=0]\n",
            "Epoch 8:  52%|█████▏    | 300/582 [00:22<00:21, 13.21it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 44.68it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 8:  52%|█████▏    | 300/582 [00:22<00:21, 13.07it/s, v_num=0]Epoch 8, global step 4778: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 8:  55%|█████▍    | 320/582 [00:25<00:20, 12.73it/s, v_num=0]\n",
            "Epoch 8:  55%|█████▍    | 320/582 [00:25<00:20, 12.73it/s, v_num=0]\n",
            "Epoch 8:  58%|█████▊    | 340/582 [00:26<00:18, 12.91it/s, v_num=0]\n",
            "Epoch 8:  58%|█████▊    | 340/582 [00:26<00:18, 12.91it/s, v_num=0]\n",
            "Epoch 8:  62%|██████▏   | 360/582 [00:27<00:16, 13.07it/s, v_num=0]\n",
            "Epoch 8:  62%|██████▏   | 360/582 [00:27<00:16, 13.07it/s, v_num=0]\n",
            "Epoch 8:  65%|██████▌   | 380/582 [00:28<00:15, 13.21it/s, v_num=0]\n",
            "Epoch 8:  65%|██████▌   | 380/582 [00:28<00:15, 13.21it/s, v_num=0]\n",
            "Epoch 8:  69%|██████▊   | 400/582 [00:29<00:13, 13.35it/s, v_num=0]\n",
            "Epoch 8:  69%|██████▊   | 400/582 [00:29<00:13, 13.35it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 43.74it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 8:  69%|██████▊   | 400/582 [00:30<00:13, 13.24it/s, v_num=0]Epoch 8, global step 4878: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 8:  72%|███████▏  | 420/582 [00:32<00:12, 12.98it/s, v_num=0]\n",
            "Epoch 8:  72%|███████▏  | 420/582 [00:32<00:12, 12.98it/s, v_num=0]\n",
            "Epoch 8:  76%|███████▌  | 440/582 [00:33<00:10, 13.09it/s, v_num=0]\n",
            "Epoch 8:  76%|███████▌  | 440/582 [00:33<00:10, 13.09it/s, v_num=0]\n",
            "Epoch 8:  79%|███████▉  | 460/582 [00:34<00:09, 13.19it/s, v_num=0]\n",
            "Epoch 8:  79%|███████▉  | 460/582 [00:34<00:09, 13.19it/s, v_num=0]\n",
            "Epoch 8:  82%|████████▏ | 480/582 [00:36<00:07, 13.28it/s, v_num=0]\n",
            "Epoch 8:  82%|████████▏ | 480/582 [00:36<00:07, 13.28it/s, v_num=0]\n",
            "Epoch 8:  86%|████████▌ | 500/582 [00:37<00:06, 13.39it/s, v_num=0]\n",
            "Epoch 8:  86%|████████▌ | 500/582 [00:37<00:06, 13.39it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 44.15it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 8:  86%|████████▌ | 500/582 [00:37<00:06, 13.30it/s, v_num=0]Epoch 8, global step 4978: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 8:  89%|████████▉ | 520/582 [00:39<00:04, 13.07it/s, v_num=0]\n",
            "Epoch 8:  89%|████████▉ | 520/582 [00:39<00:04, 13.07it/s, v_num=0]\n",
            "Epoch 8:  89%|████████▉ | 520/582 [00:41<00:04, 12.46it/s, v_num=0]`Trainer.fit` stopped: `max_steps=5000` reached.\n",
            "\n",
            "Epoch 8:  89%|████████▉ | 520/582 [00:41<00:04, 12.46it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs_v2/bd3lm_v2_Linear_U0_1_Lp16/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=16 algo.var_min=false\n",
            "e using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-01-11 18:56:17.693007: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-11 18:56:17.708949: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768157777.727392   94085 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768157777.732779   94085 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768157777.747002   94085 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768157777.747029   94085 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768157777.747031   94085 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768157777.747033   94085 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-11 18:56:17.751257: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\n",
            "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 19.65it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 24.60it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 24.01it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │     8.036428451538086     │\n",
            "│          val/nll          │     5.570427894592285     │\n",
            "│          val/ppl          │     262.5464172363281     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "✓ Linear U[0,1]: PPL = 262.55\n",
            "\n",
            "--- Clipped U[0.3,0.8] ---\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=20000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=20 trainer.val_check_interval=100 trainer.max_steps=5000 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs_v2/bd3lm_v2_Clipped_U0.3_0.8_Lp16 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=16 training.from_pretrained=/content/repro_runs_v2/bd3lm_base_len128_v3/checkpoints/last.ckpt training.resample=true algo.var_min=true training.sampling_eps_min=0.3 training.sampling_eps_max=0.8\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4279211859.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"bd3lm_v2_{safe_name}_Lp{Lprime}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         finetune_ckpt = train_run_v2(\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bd3lm\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1623456032.py\u001b[0m in \u001b[0;36mtrain_run_v2\u001b[0;34m(run_name, algo, block_size, from_pretrained, max_steps, max_train_samples, extra_overrides)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0moverrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_overrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"checkpoints\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"last.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1623456032.py\u001b[0m in \u001b[0;36mrun_main\u001b[0;34m(overrides, timeout)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-u\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bd3lms/main.py\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n$\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     proc = subprocess.run(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Cell 7 (Optional): Run L'=16\n",
        "\n",
        "RUN_L16 = True  # Set to True to run L'=16 experiments\n",
        "\n",
        "if RUN_L16:\n",
        "    Lprime = 16\n",
        "    results_v2_L16 = []\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"TABLE 8 v2: L' = {Lprime}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for schedule_name, schedule_overrides in noise_schedules:\n",
        "        print(f\"\\n--- {schedule_name} ---\")\n",
        "\n",
        "        safe_name = schedule_name.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"_\").replace(\" \", \"_\")\n",
        "        run_name = f\"bd3lm_v2_{safe_name}_Lp{Lprime}\"\n",
        "\n",
        "        finetune_ckpt = train_run_v2(\n",
        "            run_name,\n",
        "            algo=\"bd3lm\",\n",
        "            block_size=Lprime,\n",
        "            from_pretrained=bd3lm_base_ckpt,\n",
        "            max_steps=5000,\n",
        "            max_train_samples=20000,\n",
        "            extra_overrides=[\n",
        "                \"training.resample=true\",\n",
        "                \"algo.var_min=true\",\n",
        "            ] + schedule_overrides,\n",
        "        )\n",
        "\n",
        "        ppl = eval_run(\n",
        "            algo=\"bd3lm\",\n",
        "            checkpoint_path=finetune_ckpt,\n",
        "            block_size=Lprime,\n",
        "            extra_overrides=[\"algo.var_min=false\"],\n",
        "        )\n",
        "\n",
        "        results_v2_L16.append({\"schedule\": schedule_name, \"ppl\": ppl})\n",
        "        print(f\"✓ {schedule_name}: PPL = {ppl:.2f}\")\n",
        "\n",
        "    print(\"\\nL'=16 Results:\")\n",
        "    for r in sorted(results_v2_L16, key=lambda x: x['ppl']):\n",
        "        print(f\"  {r['schedule']}: {r['ppl']:.2f}\")\n",
        "else:\n",
        "    print(\"Skipping L'=16 experiments. Set RUN_L16=True to run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "094xjV9iRrW0"
      },
      "source": [
        "## Summary\n",
        "\n",
        "**Αν η διάταξη ακόμα δεν ταιριάζει:**\n",
        "\n",
        "Μπορείς να δοκιμάσεις:\n",
        "1. `max_steps=5000` (ακόμα περισσότερο training)\n",
        "2. `max_train_samples=20000`\n",
        "3. Larger batch size αν έχεις αρκετή VRAM\n",
        "\n",
        "**Για το report:**\n",
        "> Με scaled-down training, η διάταξη μπορεί να διαφέρει από το paper λόγω του ότι τα clipped schedules\n",
        "> χρειάζονται περισσότερο training για να δείξουν το πλεονέκτημά τους στη μείωση της variance."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}