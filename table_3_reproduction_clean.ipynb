{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-hTy7ZVX1Uv0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hTy7ZVX1Uv0",
        "outputId": "ac21a2fb-7222-485f-89a2-1c30461dc731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xya1p6If1Wk3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xya1p6If1Wk3",
        "outputId": "f563c716-ca64-4c0b-c139-5bbee90f305c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'bd3lms'...\n",
            "remote: Enumerating objects: 1095, done.\u001b[K\n",
            "remote: Counting objects: 100% (358/358), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 1095 (delta 284), reused 246 (delta 209), pack-reused 737 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1095/1095), 7.53 MiB | 8.74 MiB/s, done.\n",
            "Resolving deltas: 100% (693/693), done.\n",
            "total 4828\n",
            "-rw-r--r-- 1 root root  862037 Feb  8 13:22  2503.09573v3.pdf\n",
            "drwxr-xr-x 9 root root    4096 Feb  8 13:22  configs\n",
            "-rw-r--r-- 1 root root   33822 Feb  8 13:22  dataloader.py\n",
            "-rw-r--r-- 1 root root   46696 Feb  8 13:22  diffusion.py\n",
            "-rw-r--r-- 1 root root   56775 Feb  8 13:22  extension1_edm.ipynb\n",
            "-rw-r--r-- 1 root root   58511 Feb  8 13:22  extension1_fm.ipynb\n",
            "-rw-r--r-- 1 root root   58620 Feb  8 13:22  extension1_iddpm.ipynb\n",
            "-rw-r--r-- 1 root root   58457 Feb  8 13:22  extension1_sigmoid.ipynb\n",
            "-rw-r--r-- 1 root root   59098 Feb  8 13:22  extension1_sigmoid_val.ipynb\n",
            "-rw-r--r-- 1 root root   58505 Feb  8 13:22  extension1_simple.ipynb\n",
            "drwxr-xr-x 2 root root    4096 Feb  8 13:22  final_presentation\n",
            "-rw-r--r-- 1 root root  225205 Feb  8 13:22  graphical_abstract.png\n",
            "-rw-r--r-- 1 root root   11357 Feb  8 13:22  LICENSE\n",
            "-rw-r--r-- 1 root root    7873 Feb  8 13:22  main.py\n",
            "-rw-r--r-- 1 root root    8405 Feb  8 13:22  metrics.py\n",
            "-rw-r--r-- 1 root root  257463 Feb  8 13:22 'midterm_presentation_pattern_recognition (5).pdf'\n",
            "drwxr-xr-x 3 root root    4096 Feb  8 13:22  models\n",
            "-rw-r--r-- 1 root root    9371 Feb  8 13:22  noise_schedule.py\n",
            "-rw-r--r-- 1 root root    1449 Feb  8 13:22  push_to_hf.py\n",
            "-rw-r--r-- 1 root root   10070 Feb  8 13:22  README.md\n",
            "-rw-r--r-- 1 root root     363 Feb  8 13:22  requirements.txt\n",
            "drwxr-xr-x 7 root root    4096 Feb  8 13:22  scripts\n",
            "drwxr-xr-x 4 root root    4096 Feb  8 13:22  ssd-lm\n",
            "-rw-r--r-- 1 root root  327057 Feb  8 13:22  table_1_diagram_2.ipynb\n",
            "-rw-r--r-- 1 root root  525005 Feb  8 13:22  table_2_reproduction.ipynb\n",
            "-rw-r--r-- 1 root root   69314 Feb  8 13:22  table_3_reproduction.ipynb\n",
            "-rw-r--r-- 1 root root  105567 Feb  8 13:22  table4_initial_reproduction.ipynb\n",
            "-rw-r--r-- 1 root root  103948 Feb  8 13:22 'table4_more_training _initial_reproduction.ipynb'\n",
            "-rw-r--r-- 1 root root  304906 Feb  8 13:22  table_5_reproduction_initial.ipynb\n",
            "-rw-r--r-- 1 root root  121480 Feb  8 13:22  Table_6_WORKING_initial.ipynb\n",
            "-rw-r--r-- 1 root root 1262898 Feb  8 13:22  table_7_reproduction.ipynb\n",
            "-rw-r--r-- 1 root root   85975 Feb  8 13:22  table_8_reproduction_corrected_initial_reproduction.ipynb\n",
            "-rw-r--r-- 1 root root   31464 Feb  8 13:22  table_8_reproduction_final.ipynb\n",
            "-rw-r--r-- 1 root root   96726 Feb  8 13:22  table_8_v2_more_training_initial_reproduction.ipynb\n",
            "-rw-r--r-- 1 root root    7162 Feb  8 13:22  utils.py\n"
          ]
        }
      ],
      "source": [
        "!cd /content && git clone https://github.com/ntua-el21050/bd3lms.git\n",
        "!ls -l /content/bd3lms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VIiu88YO2Ami",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIiu88YO2Ami",
        "outputId": "5d2f7d7b-3775-45bd-f73a-c6fea6fdc589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 4\n",
            "drwx------ 2 root root 4096 Jan 27 23:24 rework\n"
          ]
        }
      ],
      "source": [
        "!ls -l /content/drive/MyDrive/DIFF_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4LWwArl_1Yog",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LWwArl_1Yog",
        "outputId": "fe030946-b87e-4483-fccc-c34e31a5384b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.3/857.3 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.1 which is incompatible.\n",
            "opencv-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "wheel 0.46.3 requires packaging>=24.0, but you have packaging 23.2 which is incompatible.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\n",
            "google-cloud-bigquery 3.40.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.11 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "xarray 2025.12.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\n",
            "db-dtypes 1.5.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q \\\n",
        "    torchmetrics==1.6.2 \\\n",
        "    datasets==3.3.2 \\\n",
        "    einops==0.8.1 \\\n",
        "    fsspec==2024.2.0 \\\n",
        "    hydra-core==1.3.2 \\\n",
        "    lightning==2.5.0.post0 \\\n",
        "    omegaconf==2.3.0 \\\n",
        "    packaging==23.2 \\\n",
        "    pandas==2.2.1 \\\n",
        "    rich==13.7.1 \\\n",
        "    scikit-learn==1.5.1 \\\n",
        "    timm==0.9.16 \\\n",
        "    # transformers==4.49.0 \\\n",
        "    matplotlib==3.10.0 \\\n",
        "    wandb\n",
        "    # numpy.char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06073006",
      "metadata": {
        "id": "06073006"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import re\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "import os\n",
        "\n",
        "REPO_DIR = Path.cwd()\n",
        "\n",
        "_ALLOWED_ALGOS = {\"ar\", \"mdlm\", \"sedd\", \"bd3lm\"}\n",
        "\n",
        "\n",
        "def _run_command(args: list[str]) -> str:\n",
        "    env = dict(os.environ)\n",
        "    env.setdefault(\"HYDRA_FULL_ERROR\", \"1\")\n",
        "    proc = subprocess.Popen(\n",
        "        args,\n",
        "        cwd=REPO_DIR,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1,\n",
        "        universal_newlines=True,\n",
        "    )\n",
        "\n",
        "    out_chunks: list[str] = []\n",
        "    assert proc.stdout is not None\n",
        "    for line in proc.stdout:\n",
        "        print(line, end=\"\")  # live logs in Colab/Jupyter\n",
        "        out_chunks.append(line)\n",
        "\n",
        "    rc = proc.wait()\n",
        "    if rc != 0:\n",
        "        combined = \"\".join(out_chunks)\n",
        "        raise RuntimeError(f\"Command failed with return code {rc}\\nOUTPUT:\\n{combined}\")\n",
        "\n",
        "    return \"\".join(out_chunks)\n",
        "    return None\n",
        "\n",
        "\n",
        "def _find_latest_checkpoint(best=False) -> Optional[Path]:\n",
        "    outputs_dir = REPO_DIR / \"outputs\"\n",
        "    if not outputs_dir.exists():\n",
        "        return None\n",
        "    if best:\n",
        "      candidates = list(outputs_dir.rglob(\"checkpoints/best.ckpt\"))\n",
        "    else:\n",
        "      candidates = list(outputs_dir.rglob(\"checkpoints/last.ckpt\"))\n",
        "    if not candidates:\n",
        "        return None\n",
        "    return max(candidates, key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    *,\n",
        "    algo=\"bd3lm\",\n",
        "    block_size=1,\n",
        "    trainer_max_steps=500,\n",
        "    sampling_eps_min=1e-3,\n",
        "    sampling_eps_max=1,\n",
        "    resume_from_ckpt=False,\n",
        "    resume_ckpt_path='',\n",
        "    max_train_samples=500,\n",
        "    max_valid_samples=100,\n",
        "    max_test_samples=100,\n",
        "    nll_diagram=False,\n",
        "    noise=None,\n",
        "    noise_mu=None,\n",
        "    noise_mu1=None,\n",
        "    noise_w1=None\n",
        "):\n",
        "    if algo not in _ALLOWED_ALGOS:\n",
        "        raise ValueError(f\"algo must be one of {_ALLOWED_ALGOS}\")\n",
        "\n",
        "\n",
        "    if not os.path.exists(resume_ckpt_path) and resume_ckpt_path != '':\n",
        "      print(f\"Path {resume_ckpt_path} doeds not exist\")\n",
        "      return None\n",
        "\n",
        "    else:\n",
        "\n",
        "      args = [\n",
        "          sys.executable,\n",
        "          '-u',\n",
        "          \"bd3lms/main.py\",\n",
        "          \"mode=train\",\n",
        "          \"model=tiny\",\n",
        "          f\"algo={algo}\",\n",
        "          \"data=lm1b-wrap\",\n",
        "          \"model.length=128\",\n",
        "          \"model.attn_backend=sdpa\",\n",
        "          f\"block_size={block_size}\",\n",
        "          \"trainer.devices=1\",\n",
        "          \"loader.global_batch_size=4\",\n",
        "          \"loader.batch_size=4\",\n",
        "          \"loader.eval_batch_size=4\",\n",
        "          f\"trainer.max_steps={trainer_max_steps}\",\n",
        "          f\"data.max_train_samples={max_train_samples}\",\n",
        "          f\"data.max_valid_samples={max_valid_samples}\",\n",
        "          f\"data.max_test_samples={max_test_samples}\",\n",
        "          f\"training.nll_diagram={str(nll_diagram)}\",\n",
        "          f\"training.sampling_eps_min={sampling_eps_min}\",\n",
        "          f\"training.sampling_eps_max={sampling_eps_max}\",\n",
        "          f\"checkpointing.resume_from_ckpt={str(resume_from_ckpt)}\",\n",
        "          f\"checkpointing.resume_ckpt_path={resume_ckpt_path}\",\n",
        "      ]\n",
        "\n",
        "      if noise is not None:\n",
        "          args.append(f\"noise={noise}\")\n",
        "      if noise_mu is not None:\n",
        "          args.append(f\"noise.mu={noise_mu}\")\n",
        "      if noise_mu1 is not None:\n",
        "          args.append(f\"noise.mu1={noise_mu1}\")\n",
        "      if noise_w1 is not None:\n",
        "          args.append(f\"noise.w1={noise_w1}\")\n",
        "\n",
        "      _run_command(args)\n",
        "      latest_ckpt = _find_latest_checkpoint(best=False)\n",
        "      return str(latest_ckpt) if latest_ckpt else None\n",
        "\n",
        "\n",
        "def eval_model(\n",
        "    *,\n",
        "    algo=\"bd3lm\",\n",
        "    block_size=1,\n",
        "    checkpoint_path=\"/my/path/best.ckpt\",\n",
        "    max_valid_samples=100,\n",
        "    max_test_samples=100,\n",
        "    var_min=False,\n",
        "    sampling_eps_min=0.001,\n",
        "    sampling_eps_max=1,\n",
        "    clip_search_widths=[]\n",
        "):\n",
        "    if algo not in _ALLOWED_ALGOS:\n",
        "        raise ValueError(f\"algo must be one of {_ALLOWED_ALGOS}\")\n",
        "\n",
        "    args = [\n",
        "        sys.executable,\n",
        "        \"bd3lms/main.py\",\n",
        "        \"mode=ppl_eval\",\n",
        "        \"model=tiny\",\n",
        "        f\"algo={algo}\",\n",
        "        \"data=lm1b-wrap\",\n",
        "        \"model.length=128\",\n",
        "        \"model.attn_backend=sdpa\",\n",
        "        f\"block_size={block_size}\",\n",
        "        \"trainer.devices=1\",\n",
        "        \"trainer.num_nodes=1\",\n",
        "        \"loader.global_batch_size=4\",\n",
        "        \"loader.batch_size=4\",\n",
        "        \"loader.eval_batch_size=4\",\n",
        "        f\"eval.checkpoint_path={checkpoint_path}\",\n",
        "        f\"data.max_valid_samples={max_valid_samples}\",\n",
        "        f\"data.max_test_samples={max_test_samples}\",\n",
        "        # f\"algo.var_min={str(var_min)}\",\n",
        "        # f\"training.sampling_eps_min={sampling_eps_min}\",\n",
        "        # f\"training.sampling_eps_max={sampling_eps_max}\",\n",
        "        # f\"algo.clip_search_widths={str(clip_search_widths)}\",\n",
        "    ]\n",
        "\n",
        "    output = _run_command(args)\n",
        "\n",
        "    # Best-effort extraction of perplexity from logs.\n",
        "    # Common patterns include \"val/ppl\" or \"ppl\" in metric logs.\n",
        "    match_ppl = re.search(r\"val/ppl[^\\n0-9]*([0-9]+(?:\\.[0-9]+)?)\", output)\n",
        "    match_nelbo = re.search(r\"valid_var_0.0 - 1[^\\n0-9]*([0-9]+(?:\\.[0-9]+)?)\", output)\n",
        "    if match_ppl:\n",
        "        r1 = round(float(match_ppl.group(1)), 2)\n",
        "        if match_nelbo:\n",
        "          r2 = round(float(match_nelbo.group(1)), 2)\n",
        "          return r1, r2\n",
        "        return (r1, None)\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "def print_table(rows):\n",
        "    if not rows:\n",
        "        print(\"No data to display.\")\n",
        "        return\n",
        "\n",
        "    columns = list(rows[0].keys())\n",
        "\n",
        "    str_rows = [\n",
        "        {col: str(row.get(col, \"\")) for col in columns}\n",
        "        for row in rows\n",
        "    ]\n",
        "\n",
        "    widths = {\n",
        "        col: max(len(col), max(len(row[col]) for row in str_rows))\n",
        "        for col in columns\n",
        "    }\n",
        "\n",
        "    def print_separator():\n",
        "        print(\"+\" + \"+\".join(\"-\" * (widths[col] + 2) for col in columns) + \"+\")\n",
        "\n",
        "    def print_row(row):\n",
        "        print(\n",
        "            \"| \" +\n",
        "            \" | \".join(row[col].ljust(widths[col]) for col in columns) +\n",
        "            \" |\"\n",
        "        )\n",
        "\n",
        "    # Print table\n",
        "    print_separator()\n",
        "    print_row({col: col for col in columns})\n",
        "    print_separator()\n",
        "    for row in str_rows:\n",
        "        print_row(row)\n",
        "    print_separator()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EMb4d1AedHeK",
      "metadata": {
        "id": "EMb4d1AedHeK"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = '/content/drive/MyDrive/DIFF_2/rework'\n",
        "results = []\n",
        "\n",
        "algos = ['ar', 'mdlm', 'sedd']\n",
        "\n",
        "for alg in algos:\n",
        "  model_ckpt = train_model(\n",
        "      algo=alg,\n",
        "      trainer_max_steps=500,\n",
        "      max_train_samples=500,\n",
        "      max_valid_samples=100,\n",
        "      max_test_samples=100,\n",
        "  )\n",
        "\n",
        "  print(f\"Checkpoint for algo: {alg} saved at {model_ckpt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rfjRUoYS59jM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfjRUoYS59jM",
        "outputId": "fe5ca80b-f7d9-4920-ea04-f161a8690d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── ppl_eval                                                                \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 1                                                                       \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: null                                                 \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_ar_len128_blocksize1                                  \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1.0                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/drive/MyDrive/DIFF_2/rework/checkpoints/FINAL_\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 300                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.02.01/111817                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/outputs/lm1b/2026.02.01/111817/checkpoints/la\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.01/111817/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.01/111817/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: ar                                                                \n",
            "        backbone: dit                                                           \n",
            "        parameterization: ar                                                    \n",
            "        time_conditioning: false                                                \n",
            "        causal_attention: true                                                  \n",
            "        T: 0                                                                    \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: false                                                       \n",
            "        cross_attn: false                                                       \n",
            "        var_min: false                                                          \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: ar                                                             \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-02-01 11:18:17,856][__main__][INFO] - Starting Eval.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "Loading checkpoint at 500\n",
            "[2026-02-01 11:18:20,452][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-01 11:18:20,452][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 1905.95 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 17694.50 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-01 11:20:06.482746: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769944806.661044   36417 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769944806.709299   36417 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769944807.064725   36417 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769944807.064762   36417 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769944807.064766   36417 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769944807.064768   36417 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-01 11:20:07.100232: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │    11.570764541625977     │\n",
            "│          val/nll          │     8.020242691040039     │\n",
            "│          val/ppl          │     3041.91552734375      │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Result for algo ar is 3041.91552734375\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── ppl_eval                                                                \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 1                                                                       \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: null                                                 \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_mdlm_len128_blocksize1                                \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1.0                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/drive/MyDrive/DIFF_2/rework/checkpoints/FINAL_\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 300                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.02.01/112035                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/outputs/lm1b/2026.02.01/112035/checkpoints/la\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.01/112035/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.01/112035/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: mdlm                                                              \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: false                                                       \n",
            "        var_min: false                                                          \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-02-01 11:20:35,712][__main__][INFO] - Starting Eval.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "Loading checkpoint at 470\n",
            "[2026-02-01 11:20:38,799][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-01 11:20:38,799][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 1983.38 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 12623.18 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-01 11:22:25.547972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769944945.597315   37040 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769944945.602583   37040 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769944945.676382   37040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769944945.676416   37040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769944945.676419   37040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769944945.676421   37040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-01 11:22:25.683154: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │    10.498682975769043     │\n",
            "│          val/nll          │     7.277132511138916     │\n",
            "│          val/ppl          │    1446.8333740234375     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Result for algo mdlm is 1446.8333740234375\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── ppl_eval                                                                \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 1                                                                       \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: null                                                 \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_sedd_len128_blocksize1                                \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1.0                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/drive/MyDrive/DIFF_2/rework/checkpoints/FINAL_\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 300                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.02.01/112251                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/outputs/lm1b/2026.02.01/112251/checkpoints/la\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.01/112251/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.01/112251/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: sedd                                                              \n",
            "        backbone: dit                                                           \n",
            "        parameterization: sedd                                                  \n",
            "        time_conditioning: true                                                 \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: false                                                       \n",
            "        var_min: false                                                          \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: analytic                                                       \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-02-01 11:22:51,997][__main__][INFO] - Starting Eval.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "Loading checkpoint at 470\n",
            "[2026-02-01 11:22:53,649][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-01 11:22:53,649][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 2082.51 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 19912.19 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-01 11:24:35.889004: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769945075.906819   37644 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769945075.912397   37644 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769945075.926298   37644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769945075.926323   37644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769945075.926326   37644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769945075.926329   37644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-01 11:24:35.930413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │    10.658437728881836     │\n",
            "│          val/nll          │     7.387866020202637     │\n",
            "│          val/ppl          │     1616.25341796875      │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Result for algo sedd is 1616.25341796875\n",
            "+-------+--------------+--------------------+\n",
            "| model | block_size_L | val_ppl            |\n",
            "+-------+--------------+--------------------+\n",
            "| ar    | -            | 3041.91552734375   |\n",
            "| mdlm  | -            | 1446.8333740234375 |\n",
            "| sedd  | -            | 1616.25341796875   |\n",
            "+-------+--------------+--------------------+\n"
          ]
        }
      ],
      "source": [
        "BASE_DIR = '/content/drive/MyDrive/DIFF_2/rework'\n",
        "results = []\n",
        "\n",
        "algos = ['ar', 'mdlm', 'sedd']\n",
        "\n",
        "for alg in algos:\n",
        "  result = eval_model(\n",
        "    algo=alg,\n",
        "    checkpoint_path=BASE_DIR + f'/checkpoints/FINAL_REPR_{alg}/best.ckpt',\n",
        "    max_valid_samples=100,\n",
        "    max_test_samples=100,\n",
        ")\n",
        "\n",
        "  print(f\"Result for algo {alg} is {result}\")\n",
        "  results.append({\"model\": alg, \"block_size_L\": '-', \"val_ppl\": result})\n",
        "\n",
        "print_table(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36dOR1S4yW0K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36dOR1S4yW0K",
        "outputId": "77e3ac29-ddcb-4a54-c4e4-4513c1f94561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── train                                                                   \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: 500                                                  \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1                                                     \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/outputs/lm1b/2026.01.31/123500/checkpoints/las\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 500                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.01.31/123500                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/drive/MyDrive/DIFF_2/rework/petran_new_pretra\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/123500/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/123500/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-01-31 12:35:02,596][__main__][INFO] - Starting Training.\n",
            "[2026-01-31 12:35:02,606][__main__][INFO] - Resuming training at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "[2026-01-31 12:35:02,636][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_train_bs128_wrapped.dat\n",
            "[2026-01-31 12:35:02,636][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 500 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 500 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 2147.66 examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 2130.92 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 24193.07 examples/s]\n",
            "[2026-01-31 12:35:04,901][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-01-31 12:35:04,901][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 100 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 100 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 1333.51 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 15037.12 examples/s]\n",
            "Printing train dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] while athletes in different professions dealt with doping scandals and other controversies, woods continued to do what he did best : dominate the field of professional golf and rake in endorsements. [CLS] the minutes could shed light on an internal debate, which has been evident in fed officials'recent speeches, over when to consider raising rates.\n",
            "ids: tensor([  101,  2096,  7576,  1999,  2367, 22797,  9411,  2007, 23799, 29609,\n",
            "         1998,  2060, 25962,  1010,  5249,  2506,  2000,  2079,  2054,  2002,\n",
            "         2106,  2190,  1024, 16083,  1996,  2492,  1997,  2658,  5439,  1998,\n",
            "        26008,  1999, 20380,  2015,  1012,   101,  1996,  2781,  2071,  8328,\n",
            "         2422,  2006,  2019,  4722,  5981,  1010,  2029,  2038,  2042, 10358,\n",
            "         1999,  7349,  4584,  1005,  3522, 13867,  1010,  2058,  2043,  2000,\n",
            "         5136,  6274,  6165,  1012])\n",
            "Last 64 tokens: [CLS] but metro's definition of \" urbanite \" is not accurate, says matthew gandy, director of the urban laboratory at university college london. [CLS] goodrich petroleum is an independent oil and gas exploration and production company listed on the new york stock exchange. [CLS] the support from spears is to end on november 15 [CLS]\n",
            "ids: tensor([  101,  2021,  6005,  1005,  1055,  6210,  1997,  1000,  3923,  4221,\n",
            "         1000,  2003,  2025,  8321,  1010,  2758,  5487, 25957,  5149,  1010,\n",
            "         2472,  1997,  1996,  3923,  5911,  2012,  2118,  2267,  2414,  1012,\n",
            "          101,  2204, 13149, 11540,  2003,  2019,  2981,  3514,  1998,  3806,\n",
            "         8993,  1998,  2537,  2194,  3205,  2006,  1996,  2047,  2259,  4518,\n",
            "         3863,  1012,   101,  1996,  2490,  2013, 13957,  2003,  2000,  2203,\n",
            "         2006,  2281,  2321,   101])\n",
            "Printing valid dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] in medieval times, the townspeople from norcia were so practised at butchering pigs that they gained an in - depth knowledge of anatomy and were allowed to practise as surgeons along with members of the clergy. [CLS] the satellite weighs some 660 pounds, the israeli haaretz newspaper reported, citing unnamed company officials.\n",
            "ids: tensor([  101,  1999,  5781,  2335,  1010,  1996, 27938,  2013,  4496,  7405,\n",
            "         2020,  2061, 20439,  2012, 14998,  2075, 14695,  2008,  2027,  4227,\n",
            "         2019,  1999,  1011,  5995,  3716,  1997, 13336,  1998,  2020,  3039,\n",
            "         2000, 10975, 18908,  5562,  2004, 16804,  2247,  2007,  2372,  1997,\n",
            "         1996, 11646,  1012,   101,  1996,  5871, 21094,  2070, 20982,  7038,\n",
            "         1010,  1996,  5611,  5292, 12069,  5753,  3780,  2988,  1010,  8951,\n",
            "        13294,  2194,  4584,  1012])\n",
            "Last 64 tokens: [CLS] the situation has been further complicated by growing indications that the us treasury is preparing to make funds available for recapitalsing us banks. [CLS] as north korea's close neighbor, traditional ally and main provider of economic aid, china is widely believed to hold the key to solving the north korea conundrum. [CLS]\n",
            "ids: tensor([  101,  1996,  3663,  2038,  2042,  2582,  8552,  2011,  3652, 24936,\n",
            "         2008,  1996,  2149,  9837,  2003,  8225,  2000,  2191,  5029,  2800,\n",
            "         2005, 28667,  9331, 18400,  7741,  2149,  5085,  1012,   101,  2004,\n",
            "         2167,  4420,  1005,  1055,  2485, 11429,  1010,  3151,  9698,  1998,\n",
            "         2364, 10802,  1997,  3171,  4681,  1010,  2859,  2003,  4235,  3373,\n",
            "         2000,  2907,  1996,  3145,  2000, 13729,  1996,  2167,  4420,  9530,\n",
            "         8630,  6824,  1012,   101])\n",
            "[2026-01-31 12:36:04,189][__main__][INFO] - Initializing new model\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "wandb: WARNING The anonymous setting has no effect and will be removed in a future version.\n",
            "Restoring states from the checkpoint path at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "Loading checkpoint at 400\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/content/bd3lms/outputs/lm1b/2026.01.30/230437/checkpoints' to '/content/outputs/lm1b/2026.01.31/123500/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type           | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | backbone | DIT            | 22.8 M | train\n",
            "1 | noise    | LogLinearNoise | 0      | train\n",
            "----------------------------------------------------\n",
            "22.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 M    Total params\n",
            "91.266    Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Restored all states from the checkpoint at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py:221: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "Epoch 13, global step 404: 'val/nll' reached 8.08172 (best 8.08172), saving model to '/content/outputs/lm1b/2026.01.31/123500/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 406: 'val/nll' reached 7.03025 (best 7.03025), saving model to '/content/outputs/lm1b/2026.01.31/123500/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 408: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 410: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 412: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 414: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 416: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 418: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 420: 'val/nll' reached 7.02756 (best 7.02756), saving model to '/content/outputs/lm1b/2026.01.31/123500/checkpoints/best.ckpt' as top 1\n",
            "Epoch 14, global step 422: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 424: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 426: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 428: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 430: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 432: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 434: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 436: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 438: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 440: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 442: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 444: 'val/nll' reached 6.99615 (best 6.99615), saving model to '/content/outputs/lm1b/2026.01.31/123500/checkpoints/best.ckpt' as top 1\n",
            "Epoch 15, global step 446: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 448: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 450: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 452: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 454: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 456: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 458: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 460: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 462: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 464: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 466: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 468: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 470: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 472: 'val/nll' reached 6.80730 (best 6.80730), saving model to '/content/outputs/lm1b/2026.01.31/123500/checkpoints/best.ckpt' as top 1\n",
            "Epoch 16, global step 474: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 476: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 478: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 480: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 482: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 484: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 486: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 488: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 490: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 492: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 494: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 496: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 498: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 500: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=500` reached.\n",
            "Checkpoint for config: 0 saved at /content/outputs/lm1b/2026.01.31/123500/checkpoints/last.ckpt\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── train                                                                   \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: 500                                                  \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.3                                                   \n",
            "│       sampling_eps_max: 0.8                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/outputs/lm1b/2026.01.31/124329/checkpoints/las\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 500                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.01.31/124329                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/drive/MyDrive/DIFF_2/rework/petran_new_pretra\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/124329/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/124329/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-01-31 12:43:30,125][__main__][INFO] - Starting Training.\n",
            "[2026-01-31 12:43:30,133][__main__][INFO] - Resuming training at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "[2026-01-31 12:43:30,158][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_train_bs128_wrapped.dat\n",
            "[2026-01-31 12:43:30,158][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 500 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 500 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 1259.17 examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 1252.27 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 22782.01 examples/s]\n",
            "[2026-01-31 12:43:32,437][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-01-31 12:43:32,437][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 100 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 100 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 2108.82 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 13553.18 examples/s]\n",
            "Printing train dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] while athletes in different professions dealt with doping scandals and other controversies, woods continued to do what he did best : dominate the field of professional golf and rake in endorsements. [CLS] the minutes could shed light on an internal debate, which has been evident in fed officials'recent speeches, over when to consider raising rates.\n",
            "ids: tensor([  101,  2096,  7576,  1999,  2367, 22797,  9411,  2007, 23799, 29609,\n",
            "         1998,  2060, 25962,  1010,  5249,  2506,  2000,  2079,  2054,  2002,\n",
            "         2106,  2190,  1024, 16083,  1996,  2492,  1997,  2658,  5439,  1998,\n",
            "        26008,  1999, 20380,  2015,  1012,   101,  1996,  2781,  2071,  8328,\n",
            "         2422,  2006,  2019,  4722,  5981,  1010,  2029,  2038,  2042, 10358,\n",
            "         1999,  7349,  4584,  1005,  3522, 13867,  1010,  2058,  2043,  2000,\n",
            "         5136,  6274,  6165,  1012])\n",
            "Last 64 tokens: [CLS] but metro's definition of \" urbanite \" is not accurate, says matthew gandy, director of the urban laboratory at university college london. [CLS] goodrich petroleum is an independent oil and gas exploration and production company listed on the new york stock exchange. [CLS] the support from spears is to end on november 15 [CLS]\n",
            "ids: tensor([  101,  2021,  6005,  1005,  1055,  6210,  1997,  1000,  3923,  4221,\n",
            "         1000,  2003,  2025,  8321,  1010,  2758,  5487, 25957,  5149,  1010,\n",
            "         2472,  1997,  1996,  3923,  5911,  2012,  2118,  2267,  2414,  1012,\n",
            "          101,  2204, 13149, 11540,  2003,  2019,  2981,  3514,  1998,  3806,\n",
            "         8993,  1998,  2537,  2194,  3205,  2006,  1996,  2047,  2259,  4518,\n",
            "         3863,  1012,   101,  1996,  2490,  2013, 13957,  2003,  2000,  2203,\n",
            "         2006,  2281,  2321,   101])\n",
            "Printing valid dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] in medieval times, the townspeople from norcia were so practised at butchering pigs that they gained an in - depth knowledge of anatomy and were allowed to practise as surgeons along with members of the clergy. [CLS] the satellite weighs some 660 pounds, the israeli haaretz newspaper reported, citing unnamed company officials.\n",
            "ids: tensor([  101,  1999,  5781,  2335,  1010,  1996, 27938,  2013,  4496,  7405,\n",
            "         2020,  2061, 20439,  2012, 14998,  2075, 14695,  2008,  2027,  4227,\n",
            "         2019,  1999,  1011,  5995,  3716,  1997, 13336,  1998,  2020,  3039,\n",
            "         2000, 10975, 18908,  5562,  2004, 16804,  2247,  2007,  2372,  1997,\n",
            "         1996, 11646,  1012,   101,  1996,  5871, 21094,  2070, 20982,  7038,\n",
            "         1010,  1996,  5611,  5292, 12069,  5753,  3780,  2988,  1010,  8951,\n",
            "        13294,  2194,  4584,  1012])\n",
            "Last 64 tokens: [CLS] the situation has been further complicated by growing indications that the us treasury is preparing to make funds available for recapitalsing us banks. [CLS] as north korea's close neighbor, traditional ally and main provider of economic aid, china is widely believed to hold the key to solving the north korea conundrum. [CLS]\n",
            "ids: tensor([  101,  1996,  3663,  2038,  2042,  2582,  8552,  2011,  3652, 24936,\n",
            "         2008,  1996,  2149,  9837,  2003,  8225,  2000,  2191,  5029,  2800,\n",
            "         2005, 28667,  9331, 18400,  7741,  2149,  5085,  1012,   101,  2004,\n",
            "         2167,  4420,  1005,  1055,  2485, 11429,  1010,  3151,  9698,  1998,\n",
            "         2364, 10802,  1997,  3171,  4681,  1010,  2859,  2003,  4235,  3373,\n",
            "         2000,  2907,  1996,  3145,  2000, 13729,  1996,  2167,  4420,  9530,\n",
            "         8630,  6824,  1012,   101])\n",
            "[2026-01-31 12:44:31,483][__main__][INFO] - Initializing new model\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "wandb: WARNING The anonymous setting has no effect and will be removed in a future version.\n",
            "Restoring states from the checkpoint path at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "Loading checkpoint at 400\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.3, max=0.8\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/content/bd3lms/outputs/lm1b/2026.01.30/230437/checkpoints' to '/content/outputs/lm1b/2026.01.31/124329/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type           | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | backbone | DIT            | 22.8 M | train\n",
            "1 | noise    | LogLinearNoise | 0      | train\n",
            "----------------------------------------------------\n",
            "22.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 M    Total params\n",
            "91.266    Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Restored all states from the checkpoint at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py:221: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.\n",
            "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: Encountered `nan` values in tensor. Will be removed.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "Epoch 13, global step 404: 'val/nll' reached 8.08144 (best 8.08144), saving model to '/content/outputs/lm1b/2026.01.31/124329/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 406: 'val/nll' reached 7.03238 (best 7.03238), saving model to '/content/outputs/lm1b/2026.01.31/124329/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 408: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 410: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 412: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 414: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 416: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 418: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 420: 'val/nll' reached 7.02655 (best 7.02655), saving model to '/content/outputs/lm1b/2026.01.31/124329/checkpoints/best.ckpt' as top 1\n",
            "Epoch 14, global step 422: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 424: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 426: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 428: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 430: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 432: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 434: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 436: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 438: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 440: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 442: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 444: 'val/nll' reached 6.99272 (best 6.99272), saving model to '/content/outputs/lm1b/2026.01.31/124329/checkpoints/best.ckpt' as top 1\n",
            "Epoch 15, global step 446: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 448: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 450: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 452: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 454: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 456: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 458: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 460: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 462: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 464: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 466: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 468: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 470: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 472: 'val/nll' reached 6.80342 (best 6.80342), saving model to '/content/outputs/lm1b/2026.01.31/124329/checkpoints/best.ckpt' as top 1\n",
            "Epoch 16, global step 474: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 476: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 478: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 480: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 482: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 484: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 486: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 488: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 490: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 492: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 494: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 496: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 498: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 500: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=500` reached.\n",
            "Checkpoint for config: 1 saved at /content/outputs/lm1b/2026.01.31/124329/checkpoints/last.ckpt\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── train                                                                   \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: 500                                                  \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1                                                     \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/outputs/lm1b/2026.01.31/125348/checkpoints/las\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 500                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - cosine                                                                \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.01.31/125348                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/drive/MyDrive/DIFF_2/rework/petran_new_pretra\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/125348/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/125348/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: cosine                                                            \n",
            "│       eps: 0.001                                                              \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-01-31 12:53:49,167][__main__][INFO] - Starting Training.\n",
            "[2026-01-31 12:53:49,176][__main__][INFO] - Resuming training at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "[2026-01-31 12:53:49,200][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_train_bs128_wrapped.dat\n",
            "[2026-01-31 12:53:49,200][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 500 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 500 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 1248.73 examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 1242.50 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 21467.64 examples/s]\n",
            "[2026-01-31 12:53:51,506][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-01-31 12:53:51,507][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 100 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 100 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 2181.64 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 22842.30 examples/s]\n",
            "Printing train dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] while athletes in different professions dealt with doping scandals and other controversies, woods continued to do what he did best : dominate the field of professional golf and rake in endorsements. [CLS] the minutes could shed light on an internal debate, which has been evident in fed officials'recent speeches, over when to consider raising rates.\n",
            "ids: tensor([  101,  2096,  7576,  1999,  2367, 22797,  9411,  2007, 23799, 29609,\n",
            "         1998,  2060, 25962,  1010,  5249,  2506,  2000,  2079,  2054,  2002,\n",
            "         2106,  2190,  1024, 16083,  1996,  2492,  1997,  2658,  5439,  1998,\n",
            "        26008,  1999, 20380,  2015,  1012,   101,  1996,  2781,  2071,  8328,\n",
            "         2422,  2006,  2019,  4722,  5981,  1010,  2029,  2038,  2042, 10358,\n",
            "         1999,  7349,  4584,  1005,  3522, 13867,  1010,  2058,  2043,  2000,\n",
            "         5136,  6274,  6165,  1012])\n",
            "Last 64 tokens: [CLS] but metro's definition of \" urbanite \" is not accurate, says matthew gandy, director of the urban laboratory at university college london. [CLS] goodrich petroleum is an independent oil and gas exploration and production company listed on the new york stock exchange. [CLS] the support from spears is to end on november 15 [CLS]\n",
            "ids: tensor([  101,  2021,  6005,  1005,  1055,  6210,  1997,  1000,  3923,  4221,\n",
            "         1000,  2003,  2025,  8321,  1010,  2758,  5487, 25957,  5149,  1010,\n",
            "         2472,  1997,  1996,  3923,  5911,  2012,  2118,  2267,  2414,  1012,\n",
            "          101,  2204, 13149, 11540,  2003,  2019,  2981,  3514,  1998,  3806,\n",
            "         8993,  1998,  2537,  2194,  3205,  2006,  1996,  2047,  2259,  4518,\n",
            "         3863,  1012,   101,  1996,  2490,  2013, 13957,  2003,  2000,  2203,\n",
            "         2006,  2281,  2321,   101])\n",
            "Printing valid dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] in medieval times, the townspeople from norcia were so practised at butchering pigs that they gained an in - depth knowledge of anatomy and were allowed to practise as surgeons along with members of the clergy. [CLS] the satellite weighs some 660 pounds, the israeli haaretz newspaper reported, citing unnamed company officials.\n",
            "ids: tensor([  101,  1999,  5781,  2335,  1010,  1996, 27938,  2013,  4496,  7405,\n",
            "         2020,  2061, 20439,  2012, 14998,  2075, 14695,  2008,  2027,  4227,\n",
            "         2019,  1999,  1011,  5995,  3716,  1997, 13336,  1998,  2020,  3039,\n",
            "         2000, 10975, 18908,  5562,  2004, 16804,  2247,  2007,  2372,  1997,\n",
            "         1996, 11646,  1012,   101,  1996,  5871, 21094,  2070, 20982,  7038,\n",
            "         1010,  1996,  5611,  5292, 12069,  5753,  3780,  2988,  1010,  8951,\n",
            "        13294,  2194,  4584,  1012])\n",
            "Last 64 tokens: [CLS] the situation has been further complicated by growing indications that the us treasury is preparing to make funds available for recapitalsing us banks. [CLS] as north korea's close neighbor, traditional ally and main provider of economic aid, china is widely believed to hold the key to solving the north korea conundrum. [CLS]\n",
            "ids: tensor([  101,  1996,  3663,  2038,  2042,  2582,  8552,  2011,  3652, 24936,\n",
            "         2008,  1996,  2149,  9837,  2003,  8225,  2000,  2191,  5029,  2800,\n",
            "         2005, 28667,  9331, 18400,  7741,  2149,  5085,  1012,   101,  2004,\n",
            "         2167,  4420,  1005,  1055,  2485, 11429,  1010,  3151,  9698,  1998,\n",
            "         2364, 10802,  1997,  3171,  4681,  1010,  2859,  2003,  4235,  3373,\n",
            "         2000,  2907,  1996,  3145,  2000, 13729,  1996,  2167,  4420,  9530,\n",
            "         8630,  6824,  1012,   101])\n",
            "[2026-01-31 12:54:51,891][__main__][INFO] - Initializing new model\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "wandb: WARNING The anonymous setting has no effect and will be removed in a future version.\n",
            "Restoring states from the checkpoint path at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "Loading checkpoint at 400\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/content/bd3lms/outputs/lm1b/2026.01.30/230437/checkpoints' to '/content/outputs/lm1b/2026.01.31/125348/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type        | Params | Mode \n",
            "-------------------------------------------------\n",
            "0 | backbone | DIT         | 22.8 M | train\n",
            "1 | noise    | CosineNoise | 0      | train\n",
            "-------------------------------------------------\n",
            "22.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 M    Total params\n",
            "91.266    Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Restored all states from the checkpoint at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py:221: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "Epoch 13, global step 404: 'val/nll' reached 7.86265 (best 7.86265), saving model to '/content/outputs/lm1b/2026.01.31/125348/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 406: 'val/nll' reached 7.27024 (best 7.27024), saving model to '/content/outputs/lm1b/2026.01.31/125348/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 408: 'val/nll' reached 7.15293 (best 7.15293), saving model to '/content/outputs/lm1b/2026.01.31/125348/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 410: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 412: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 414: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 416: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 418: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 420: 'val/nll' reached 7.01879 (best 7.01879), saving model to '/content/outputs/lm1b/2026.01.31/125348/checkpoints/best.ckpt' as top 1\n",
            "Epoch 14, global step 422: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 424: 'val/nll' reached 6.89202 (best 6.89202), saving model to '/content/outputs/lm1b/2026.01.31/125348/checkpoints/best.ckpt' as top 1\n",
            "Epoch 14, global step 426: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 428: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 430: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 432: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 434: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 436: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 438: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 440: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 442: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 444: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 446: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 448: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 450: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 452: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 454: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 456: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 458: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 460: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 462: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 464: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 466: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 468: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 470: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 472: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 474: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 476: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 478: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 480: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 482: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 484: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 486: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 488: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 490: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 492: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 494: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 496: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 498: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 500: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=500` reached.\n",
            "Checkpoint for config: 2 saved at /content/outputs/lm1b/2026.01.31/125348/checkpoints/last.ckpt\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── train                                                                   \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: 500                                                  \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.3                                                   \n",
            "│       sampling_eps_max: 0.8                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/outputs/lm1b/2026.01.31/130429/checkpoints/las\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 500                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - cosine                                                                \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.01.31/130429                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/drive/MyDrive/DIFF_2/rework/petran_new_pretra\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/130429/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/130429/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: cosine                                                            \n",
            "│       eps: 0.001                                                              \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-01-31 13:04:29,604][__main__][INFO] - Starting Training.\n",
            "[2026-01-31 13:04:29,613][__main__][INFO] - Resuming training at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "[2026-01-31 13:04:29,637][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_train_bs128_wrapped.dat\n",
            "[2026-01-31 13:04:29,637][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 500 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 500 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 2106.87 examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 2094.39 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 38157.79 examples/s]\n",
            "[2026-01-31 13:04:31,611][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-01-31 13:04:31,611][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 100 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 100 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 1240.89 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 14225.70 examples/s]\n",
            "Printing train dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] while athletes in different professions dealt with doping scandals and other controversies, woods continued to do what he did best : dominate the field of professional golf and rake in endorsements. [CLS] the minutes could shed light on an internal debate, which has been evident in fed officials'recent speeches, over when to consider raising rates.\n",
            "ids: tensor([  101,  2096,  7576,  1999,  2367, 22797,  9411,  2007, 23799, 29609,\n",
            "         1998,  2060, 25962,  1010,  5249,  2506,  2000,  2079,  2054,  2002,\n",
            "         2106,  2190,  1024, 16083,  1996,  2492,  1997,  2658,  5439,  1998,\n",
            "        26008,  1999, 20380,  2015,  1012,   101,  1996,  2781,  2071,  8328,\n",
            "         2422,  2006,  2019,  4722,  5981,  1010,  2029,  2038,  2042, 10358,\n",
            "         1999,  7349,  4584,  1005,  3522, 13867,  1010,  2058,  2043,  2000,\n",
            "         5136,  6274,  6165,  1012])\n",
            "Last 64 tokens: [CLS] but metro's definition of \" urbanite \" is not accurate, says matthew gandy, director of the urban laboratory at university college london. [CLS] goodrich petroleum is an independent oil and gas exploration and production company listed on the new york stock exchange. [CLS] the support from spears is to end on november 15 [CLS]\n",
            "ids: tensor([  101,  2021,  6005,  1005,  1055,  6210,  1997,  1000,  3923,  4221,\n",
            "         1000,  2003,  2025,  8321,  1010,  2758,  5487, 25957,  5149,  1010,\n",
            "         2472,  1997,  1996,  3923,  5911,  2012,  2118,  2267,  2414,  1012,\n",
            "          101,  2204, 13149, 11540,  2003,  2019,  2981,  3514,  1998,  3806,\n",
            "         8993,  1998,  2537,  2194,  3205,  2006,  1996,  2047,  2259,  4518,\n",
            "         3863,  1012,   101,  1996,  2490,  2013, 13957,  2003,  2000,  2203,\n",
            "         2006,  2281,  2321,   101])\n",
            "Printing valid dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] in medieval times, the townspeople from norcia were so practised at butchering pigs that they gained an in - depth knowledge of anatomy and were allowed to practise as surgeons along with members of the clergy. [CLS] the satellite weighs some 660 pounds, the israeli haaretz newspaper reported, citing unnamed company officials.\n",
            "ids: tensor([  101,  1999,  5781,  2335,  1010,  1996, 27938,  2013,  4496,  7405,\n",
            "         2020,  2061, 20439,  2012, 14998,  2075, 14695,  2008,  2027,  4227,\n",
            "         2019,  1999,  1011,  5995,  3716,  1997, 13336,  1998,  2020,  3039,\n",
            "         2000, 10975, 18908,  5562,  2004, 16804,  2247,  2007,  2372,  1997,\n",
            "         1996, 11646,  1012,   101,  1996,  5871, 21094,  2070, 20982,  7038,\n",
            "         1010,  1996,  5611,  5292, 12069,  5753,  3780,  2988,  1010,  8951,\n",
            "        13294,  2194,  4584,  1012])\n",
            "Last 64 tokens: [CLS] the situation has been further complicated by growing indications that the us treasury is preparing to make funds available for recapitalsing us banks. [CLS] as north korea's close neighbor, traditional ally and main provider of economic aid, china is widely believed to hold the key to solving the north korea conundrum. [CLS]\n",
            "ids: tensor([  101,  1996,  3663,  2038,  2042,  2582,  8552,  2011,  3652, 24936,\n",
            "         2008,  1996,  2149,  9837,  2003,  8225,  2000,  2191,  5029,  2800,\n",
            "         2005, 28667,  9331, 18400,  7741,  2149,  5085,  1012,   101,  2004,\n",
            "         2167,  4420,  1005,  1055,  2485, 11429,  1010,  3151,  9698,  1998,\n",
            "         2364, 10802,  1997,  3171,  4681,  1010,  2859,  2003,  4235,  3373,\n",
            "         2000,  2907,  1996,  3145,  2000, 13729,  1996,  2167,  4420,  9530,\n",
            "         8630,  6824,  1012,   101])\n",
            "[2026-01-31 13:05:31,626][__main__][INFO] - Initializing new model\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "wandb: WARNING The anonymous setting has no effect and will be removed in a future version.\n",
            "Restoring states from the checkpoint path at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "Loading checkpoint at 400\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.3, max=0.8\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/content/bd3lms/outputs/lm1b/2026.01.30/230437/checkpoints' to '/content/outputs/lm1b/2026.01.31/130429/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type        | Params | Mode \n",
            "-------------------------------------------------\n",
            "0 | backbone | DIT         | 22.8 M | train\n",
            "1 | noise    | CosineNoise | 0      | train\n",
            "-------------------------------------------------\n",
            "22.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 M    Total params\n",
            "91.266    Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Restored all states from the checkpoint at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py:221: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "Epoch 13, global step 404: 'val/nll' reached 7.86236 (best 7.86236), saving model to '/content/outputs/lm1b/2026.01.31/130429/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 406: 'val/nll' reached 7.27196 (best 7.27196), saving model to '/content/outputs/lm1b/2026.01.31/130429/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 408: 'val/nll' reached 7.15338 (best 7.15338), saving model to '/content/outputs/lm1b/2026.01.31/130429/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 410: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 412: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 414: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 416: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 418: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 420: 'val/nll' reached 7.01778 (best 7.01778), saving model to '/content/outputs/lm1b/2026.01.31/130429/checkpoints/best.ckpt' as top 1\n",
            "Epoch 14, global step 422: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 424: 'val/nll' reached 6.89096 (best 6.89096), saving model to '/content/outputs/lm1b/2026.01.31/130429/checkpoints/best.ckpt' as top 1\n",
            "Epoch 14, global step 426: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 428: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 430: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 432: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 434: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 436: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 438: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 440: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 442: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 444: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 446: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 448: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 450: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 452: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 454: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 456: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 458: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 460: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 462: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 464: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 466: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 468: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 470: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 472: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 474: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 476: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 478: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 480: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 482: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 484: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 486: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 488: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 490: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 492: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 494: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 496: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 498: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 500: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=500` reached.\n",
            "Checkpoint for config: 3 saved at /content/outputs/lm1b/2026.01.31/130429/checkpoints/last.ckpt\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── train                                                                   \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: 500                                                  \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1                                                     \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/outputs/lm1b/2026.01.31/131345/checkpoints/las\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 500                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - gaussian                                                              \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.01.31/131345                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/drive/MyDrive/DIFF_2/rework/petran_new_pretra\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/131345/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/131345/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: gaussian                                                          \n",
            "│       mu: 0.5                                                                 \n",
            "│       sigma: 0.1                                                              \n",
            "│       p_min: 1.0e-05                                                          \n",
            "│       u_eps: 1.0e-06                                                          \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-01-31 13:13:45,904][__main__][INFO] - Starting Training.\n",
            "[2026-01-31 13:13:45,913][__main__][INFO] - Resuming training at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "[2026-01-31 13:13:45,937][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_train_bs128_wrapped.dat\n",
            "[2026-01-31 13:13:45,937][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 500 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 500 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 2211.25 examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 2197.99 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 25777.16 examples/s]\n",
            "[2026-01-31 13:13:48,111][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-01-31 13:13:48,111][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 100 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 100 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 2093.90 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 21680.47 examples/s]\n",
            "Printing train dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] while athletes in different professions dealt with doping scandals and other controversies, woods continued to do what he did best : dominate the field of professional golf and rake in endorsements. [CLS] the minutes could shed light on an internal debate, which has been evident in fed officials'recent speeches, over when to consider raising rates.\n",
            "ids: tensor([  101,  2096,  7576,  1999,  2367, 22797,  9411,  2007, 23799, 29609,\n",
            "         1998,  2060, 25962,  1010,  5249,  2506,  2000,  2079,  2054,  2002,\n",
            "         2106,  2190,  1024, 16083,  1996,  2492,  1997,  2658,  5439,  1998,\n",
            "        26008,  1999, 20380,  2015,  1012,   101,  1996,  2781,  2071,  8328,\n",
            "         2422,  2006,  2019,  4722,  5981,  1010,  2029,  2038,  2042, 10358,\n",
            "         1999,  7349,  4584,  1005,  3522, 13867,  1010,  2058,  2043,  2000,\n",
            "         5136,  6274,  6165,  1012])\n",
            "Last 64 tokens: [CLS] but metro's definition of \" urbanite \" is not accurate, says matthew gandy, director of the urban laboratory at university college london. [CLS] goodrich petroleum is an independent oil and gas exploration and production company listed on the new york stock exchange. [CLS] the support from spears is to end on november 15 [CLS]\n",
            "ids: tensor([  101,  2021,  6005,  1005,  1055,  6210,  1997,  1000,  3923,  4221,\n",
            "         1000,  2003,  2025,  8321,  1010,  2758,  5487, 25957,  5149,  1010,\n",
            "         2472,  1997,  1996,  3923,  5911,  2012,  2118,  2267,  2414,  1012,\n",
            "          101,  2204, 13149, 11540,  2003,  2019,  2981,  3514,  1998,  3806,\n",
            "         8993,  1998,  2537,  2194,  3205,  2006,  1996,  2047,  2259,  4518,\n",
            "         3863,  1012,   101,  1996,  2490,  2013, 13957,  2003,  2000,  2203,\n",
            "         2006,  2281,  2321,   101])\n",
            "Printing valid dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] in medieval times, the townspeople from norcia were so practised at butchering pigs that they gained an in - depth knowledge of anatomy and were allowed to practise as surgeons along with members of the clergy. [CLS] the satellite weighs some 660 pounds, the israeli haaretz newspaper reported, citing unnamed company officials.\n",
            "ids: tensor([  101,  1999,  5781,  2335,  1010,  1996, 27938,  2013,  4496,  7405,\n",
            "         2020,  2061, 20439,  2012, 14998,  2075, 14695,  2008,  2027,  4227,\n",
            "         2019,  1999,  1011,  5995,  3716,  1997, 13336,  1998,  2020,  3039,\n",
            "         2000, 10975, 18908,  5562,  2004, 16804,  2247,  2007,  2372,  1997,\n",
            "         1996, 11646,  1012,   101,  1996,  5871, 21094,  2070, 20982,  7038,\n",
            "         1010,  1996,  5611,  5292, 12069,  5753,  3780,  2988,  1010,  8951,\n",
            "        13294,  2194,  4584,  1012])\n",
            "Last 64 tokens: [CLS] the situation has been further complicated by growing indications that the us treasury is preparing to make funds available for recapitalsing us banks. [CLS] as north korea's close neighbor, traditional ally and main provider of economic aid, china is widely believed to hold the key to solving the north korea conundrum. [CLS]\n",
            "ids: tensor([  101,  1996,  3663,  2038,  2042,  2582,  8552,  2011,  3652, 24936,\n",
            "         2008,  1996,  2149,  9837,  2003,  8225,  2000,  2191,  5029,  2800,\n",
            "         2005, 28667,  9331, 18400,  7741,  2149,  5085,  1012,   101,  2004,\n",
            "         2167,  4420,  1005,  1055,  2485, 11429,  1010,  3151,  9698,  1998,\n",
            "         2364, 10802,  1997,  3171,  4681,  1010,  2859,  2003,  4235,  3373,\n",
            "         2000,  2907,  1996,  3145,  2000, 13729,  1996,  2167,  4420,  9530,\n",
            "         8630,  6824,  1012,   101])\n",
            "[2026-01-31 13:14:48,165][__main__][INFO] - Initializing new model\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "wandb: WARNING The anonymous setting has no effect and will be removed in a future version.\n",
            "Restoring states from the checkpoint path at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "Loading checkpoint at 400\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/content/bd3lms/outputs/lm1b/2026.01.30/230437/checkpoints' to '/content/outputs/lm1b/2026.01.31/131345/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type          | Params | Mode \n",
            "---------------------------------------------------\n",
            "0 | backbone | DIT           | 22.8 M | train\n",
            "1 | noise    | GaussianNoise | 0      | train\n",
            "---------------------------------------------------\n",
            "22.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 M    Total params\n",
            "91.266    Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Restored all states from the checkpoint at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py:221: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "Epoch 13, global step 404: 'val/nll' reached 5.82275 (best 5.82275), saving model to '/content/outputs/lm1b/2026.01.31/131345/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 406: 'val/nll' reached 3.87180 (best 3.87180), saving model to '/content/outputs/lm1b/2026.01.31/131345/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 408: 'val/nll' reached 3.77045 (best 3.77045), saving model to '/content/outputs/lm1b/2026.01.31/131345/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 410: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 412: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 414: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 416: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 418: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 420: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 422: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 424: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 426: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 428: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 430: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 432: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 434: 'val/nll' reached 3.69472 (best 3.69472), saving model to '/content/outputs/lm1b/2026.01.31/131345/checkpoints/best.ckpt' as top 1\n",
            "Epoch 14, global step 436: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 438: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 440: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 442: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 444: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 446: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 448: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 450: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 452: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 454: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 456: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 458: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 460: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 462: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 464: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 466: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 468: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 470: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 472: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 474: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 476: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 478: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 480: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 482: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 484: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 486: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 488: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 490: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 492: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 494: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 496: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 498: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 500: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=500` reached.\n",
            "Checkpoint for config: 4 saved at /content/outputs/lm1b/2026.01.31/131345/checkpoints/last.ckpt\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── train                                                                   \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: 500                                                  \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1                                                     \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/outputs/lm1b/2026.01.31/132227/checkpoints/las\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 500                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - gaussian                                                              \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.01.31/132227                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/drive/MyDrive/DIFF_2/rework/petran_new_pretra\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/132227/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/132227/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: gaussian                                                          \n",
            "│       mu: 0.6                                                                 \n",
            "│       sigma: 0.1                                                              \n",
            "│       p_min: 1.0e-05                                                          \n",
            "│       u_eps: 1.0e-06                                                          \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-01-31 13:22:27,528][__main__][INFO] - Starting Training.\n",
            "[2026-01-31 13:22:27,537][__main__][INFO] - Resuming training at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "[2026-01-31 13:22:27,561][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_train_bs128_wrapped.dat\n",
            "[2026-01-31 13:22:27,561][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 500 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 500 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 2148.98 examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 2136.19 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 36360.28 examples/s]\n",
            "[2026-01-31 13:22:29,490][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-01-31 13:22:29,490][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 100 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 100 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 2161.64 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 21493.82 examples/s]\n",
            "Printing train dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] while athletes in different professions dealt with doping scandals and other controversies, woods continued to do what he did best : dominate the field of professional golf and rake in endorsements. [CLS] the minutes could shed light on an internal debate, which has been evident in fed officials'recent speeches, over when to consider raising rates.\n",
            "ids: tensor([  101,  2096,  7576,  1999,  2367, 22797,  9411,  2007, 23799, 29609,\n",
            "         1998,  2060, 25962,  1010,  5249,  2506,  2000,  2079,  2054,  2002,\n",
            "         2106,  2190,  1024, 16083,  1996,  2492,  1997,  2658,  5439,  1998,\n",
            "        26008,  1999, 20380,  2015,  1012,   101,  1996,  2781,  2071,  8328,\n",
            "         2422,  2006,  2019,  4722,  5981,  1010,  2029,  2038,  2042, 10358,\n",
            "         1999,  7349,  4584,  1005,  3522, 13867,  1010,  2058,  2043,  2000,\n",
            "         5136,  6274,  6165,  1012])\n",
            "Last 64 tokens: [CLS] but metro's definition of \" urbanite \" is not accurate, says matthew gandy, director of the urban laboratory at university college london. [CLS] goodrich petroleum is an independent oil and gas exploration and production company listed on the new york stock exchange. [CLS] the support from spears is to end on november 15 [CLS]\n",
            "ids: tensor([  101,  2021,  6005,  1005,  1055,  6210,  1997,  1000,  3923,  4221,\n",
            "         1000,  2003,  2025,  8321,  1010,  2758,  5487, 25957,  5149,  1010,\n",
            "         2472,  1997,  1996,  3923,  5911,  2012,  2118,  2267,  2414,  1012,\n",
            "          101,  2204, 13149, 11540,  2003,  2019,  2981,  3514,  1998,  3806,\n",
            "         8993,  1998,  2537,  2194,  3205,  2006,  1996,  2047,  2259,  4518,\n",
            "         3863,  1012,   101,  1996,  2490,  2013, 13957,  2003,  2000,  2203,\n",
            "         2006,  2281,  2321,   101])\n",
            "Printing valid dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] in medieval times, the townspeople from norcia were so practised at butchering pigs that they gained an in - depth knowledge of anatomy and were allowed to practise as surgeons along with members of the clergy. [CLS] the satellite weighs some 660 pounds, the israeli haaretz newspaper reported, citing unnamed company officials.\n",
            "ids: tensor([  101,  1999,  5781,  2335,  1010,  1996, 27938,  2013,  4496,  7405,\n",
            "         2020,  2061, 20439,  2012, 14998,  2075, 14695,  2008,  2027,  4227,\n",
            "         2019,  1999,  1011,  5995,  3716,  1997, 13336,  1998,  2020,  3039,\n",
            "         2000, 10975, 18908,  5562,  2004, 16804,  2247,  2007,  2372,  1997,\n",
            "         1996, 11646,  1012,   101,  1996,  5871, 21094,  2070, 20982,  7038,\n",
            "         1010,  1996,  5611,  5292, 12069,  5753,  3780,  2988,  1010,  8951,\n",
            "        13294,  2194,  4584,  1012])\n",
            "Last 64 tokens: [CLS] the situation has been further complicated by growing indications that the us treasury is preparing to make funds available for recapitalsing us banks. [CLS] as north korea's close neighbor, traditional ally and main provider of economic aid, china is widely believed to hold the key to solving the north korea conundrum. [CLS]\n",
            "ids: tensor([  101,  1996,  3663,  2038,  2042,  2582,  8552,  2011,  3652, 24936,\n",
            "         2008,  1996,  2149,  9837,  2003,  8225,  2000,  2191,  5029,  2800,\n",
            "         2005, 28667,  9331, 18400,  7741,  2149,  5085,  1012,   101,  2004,\n",
            "         2167,  4420,  1005,  1055,  2485, 11429,  1010,  3151,  9698,  1998,\n",
            "         2364, 10802,  1997,  3171,  4681,  1010,  2859,  2003,  4235,  3373,\n",
            "         2000,  2907,  1996,  3145,  2000, 13729,  1996,  2167,  4420,  9530,\n",
            "         8630,  6824,  1012,   101])\n",
            "[2026-01-31 13:23:29,483][__main__][INFO] - Initializing new model\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "wandb: WARNING The anonymous setting has no effect and will be removed in a future version.\n",
            "Restoring states from the checkpoint path at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "Loading checkpoint at 400\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/content/bd3lms/outputs/lm1b/2026.01.30/230437/checkpoints' to '/content/outputs/lm1b/2026.01.31/132227/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type          | Params | Mode \n",
            "---------------------------------------------------\n",
            "0 | backbone | DIT           | 22.8 M | train\n",
            "1 | noise    | GaussianNoise | 0      | train\n",
            "---------------------------------------------------\n",
            "22.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 M    Total params\n",
            "91.266    Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Restored all states from the checkpoint at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py:221: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "Epoch 13, global step 404: 'val/nll' reached 5.74883 (best 5.74883), saving model to '/content/outputs/lm1b/2026.01.31/132227/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 406: 'val/nll' reached 3.92609 (best 3.92609), saving model to '/content/outputs/lm1b/2026.01.31/132227/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 408: 'val/nll' reached 3.73000 (best 3.73000), saving model to '/content/outputs/lm1b/2026.01.31/132227/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 410: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 412: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 414: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 416: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 418: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 420: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 422: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 424: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 426: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 428: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 430: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 432: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 434: 'val/nll' reached 3.66036 (best 3.66036), saving model to '/content/outputs/lm1b/2026.01.31/132227/checkpoints/best.ckpt' as top 1\n",
            "Epoch 14, global step 436: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 438: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 440: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 442: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 444: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 446: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 448: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 450: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 452: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 454: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 456: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 458: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 460: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 462: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 464: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 466: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 468: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 470: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 472: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 474: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 476: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 478: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 480: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 482: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 484: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 486: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 488: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 490: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 492: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 494: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 496: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 498: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 500: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=500` reached.\n",
            "Checkpoint for config: 5 saved at /content/outputs/lm1b/2026.01.31/132227/checkpoints/last.ckpt\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── train                                                                   \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: 500                                                  \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1                                                     \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/outputs/lm1b/2026.01.31/133105/checkpoints/las\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 500                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - bimodal_gaussian                                                      \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.01.31/133105                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/drive/MyDrive/DIFF_2/rework/petran_new_pretra\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/133105/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/133105/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: bimodal_gaussian                                                  \n",
            "│       w1: 0.6                                                                 \n",
            "│       mu1: 0.3                                                                \n",
            "│       sigma1: 0.02                                                            \n",
            "│       sigma2: 0.08                                                            \n",
            "│       m_start: 0.4                                                            \n",
            "│       m_end: 0.85                                                             \n",
            "│       tau_scale: 3.0                                                          \n",
            "│       p_min: 1.0e-05                                                          \n",
            "│       u_eps: 1.0e-06                                                          \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-01-31 13:31:05,715][__main__][INFO] - Starting Training.\n",
            "[2026-01-31 13:31:05,723][__main__][INFO] - Resuming training at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "[2026-01-31 13:31:05,748][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_train_bs128_wrapped.dat\n",
            "[2026-01-31 13:31:05,748][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 500 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 500 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 2241.70 examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 2227.04 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 34323.27 examples/s]\n",
            "[2026-01-31 13:31:07,846][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-01-31 13:31:07,846][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 100 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 100 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 2102.09 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 17623.87 examples/s]\n",
            "Printing train dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] while athletes in different professions dealt with doping scandals and other controversies, woods continued to do what he did best : dominate the field of professional golf and rake in endorsements. [CLS] the minutes could shed light on an internal debate, which has been evident in fed officials'recent speeches, over when to consider raising rates.\n",
            "ids: tensor([  101,  2096,  7576,  1999,  2367, 22797,  9411,  2007, 23799, 29609,\n",
            "         1998,  2060, 25962,  1010,  5249,  2506,  2000,  2079,  2054,  2002,\n",
            "         2106,  2190,  1024, 16083,  1996,  2492,  1997,  2658,  5439,  1998,\n",
            "        26008,  1999, 20380,  2015,  1012,   101,  1996,  2781,  2071,  8328,\n",
            "         2422,  2006,  2019,  4722,  5981,  1010,  2029,  2038,  2042, 10358,\n",
            "         1999,  7349,  4584,  1005,  3522, 13867,  1010,  2058,  2043,  2000,\n",
            "         5136,  6274,  6165,  1012])\n",
            "Last 64 tokens: [CLS] but metro's definition of \" urbanite \" is not accurate, says matthew gandy, director of the urban laboratory at university college london. [CLS] goodrich petroleum is an independent oil and gas exploration and production company listed on the new york stock exchange. [CLS] the support from spears is to end on november 15 [CLS]\n",
            "ids: tensor([  101,  2021,  6005,  1005,  1055,  6210,  1997,  1000,  3923,  4221,\n",
            "         1000,  2003,  2025,  8321,  1010,  2758,  5487, 25957,  5149,  1010,\n",
            "         2472,  1997,  1996,  3923,  5911,  2012,  2118,  2267,  2414,  1012,\n",
            "          101,  2204, 13149, 11540,  2003,  2019,  2981,  3514,  1998,  3806,\n",
            "         8993,  1998,  2537,  2194,  3205,  2006,  1996,  2047,  2259,  4518,\n",
            "         3863,  1012,   101,  1996,  2490,  2013, 13957,  2003,  2000,  2203,\n",
            "         2006,  2281,  2321,   101])\n",
            "Printing valid dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] in medieval times, the townspeople from norcia were so practised at butchering pigs that they gained an in - depth knowledge of anatomy and were allowed to practise as surgeons along with members of the clergy. [CLS] the satellite weighs some 660 pounds, the israeli haaretz newspaper reported, citing unnamed company officials.\n",
            "ids: tensor([  101,  1999,  5781,  2335,  1010,  1996, 27938,  2013,  4496,  7405,\n",
            "         2020,  2061, 20439,  2012, 14998,  2075, 14695,  2008,  2027,  4227,\n",
            "         2019,  1999,  1011,  5995,  3716,  1997, 13336,  1998,  2020,  3039,\n",
            "         2000, 10975, 18908,  5562,  2004, 16804,  2247,  2007,  2372,  1997,\n",
            "         1996, 11646,  1012,   101,  1996,  5871, 21094,  2070, 20982,  7038,\n",
            "         1010,  1996,  5611,  5292, 12069,  5753,  3780,  2988,  1010,  8951,\n",
            "        13294,  2194,  4584,  1012])\n",
            "Last 64 tokens: [CLS] the situation has been further complicated by growing indications that the us treasury is preparing to make funds available for recapitalsing us banks. [CLS] as north korea's close neighbor, traditional ally and main provider of economic aid, china is widely believed to hold the key to solving the north korea conundrum. [CLS]\n",
            "ids: tensor([  101,  1996,  3663,  2038,  2042,  2582,  8552,  2011,  3652, 24936,\n",
            "         2008,  1996,  2149,  9837,  2003,  8225,  2000,  2191,  5029,  2800,\n",
            "         2005, 28667,  9331, 18400,  7741,  2149,  5085,  1012,   101,  2004,\n",
            "         2167,  4420,  1005,  1055,  2485, 11429,  1010,  3151,  9698,  1998,\n",
            "         2364, 10802,  1997,  3171,  4681,  1010,  2859,  2003,  4235,  3373,\n",
            "         2000,  2907,  1996,  3145,  2000, 13729,  1996,  2167,  4420,  9530,\n",
            "         8630,  6824,  1012,   101])\n",
            "[2026-01-31 13:32:08,230][__main__][INFO] - Initializing new model\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "wandb: WARNING The anonymous setting has no effect and will be removed in a future version.\n",
            "Restoring states from the checkpoint path at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "Loading checkpoint at 400\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/content/bd3lms/outputs/lm1b/2026.01.30/230437/checkpoints' to '/content/outputs/lm1b/2026.01.31/133105/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type                 | Params | Mode \n",
            "----------------------------------------------------------\n",
            "0 | backbone | DIT                  | 22.8 M | train\n",
            "1 | noise    | BimodalGaussianNoise | 0      | train\n",
            "----------------------------------------------------------\n",
            "22.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 M    Total params\n",
            "91.266    Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Restored all states from the checkpoint at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py:221: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "Epoch 13, global step 404: 'val/nll' reached 5.16619 (best 5.16619), saving model to '/content/outputs/lm1b/2026.01.31/133105/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 406: 'val/nll' reached 3.69186 (best 3.69186), saving model to '/content/outputs/lm1b/2026.01.31/133105/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 408: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 410: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 412: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 414: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 416: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 418: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 420: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 422: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 424: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 426: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 428: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 430: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 432: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 434: 'val/nll' reached 3.49794 (best 3.49794), saving model to '/content/outputs/lm1b/2026.01.31/133105/checkpoints/best.ckpt' as top 1\n",
            "Epoch 14, global step 436: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 438: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 440: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 442: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 444: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 446: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 448: 'val/nll' reached 3.48638 (best 3.48638), saving model to '/content/outputs/lm1b/2026.01.31/133105/checkpoints/best.ckpt' as top 1\n",
            "Epoch 15, global step 450: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 452: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 454: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 456: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 458: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 460: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 462: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 464: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 466: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 468: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 470: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 472: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 474: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 476: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 478: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 480: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 482: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 484: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 486: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 488: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 490: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 492: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 494: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 496: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 498: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 500: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=500` reached.\n",
            "Checkpoint for config: 6 saved at /content/outputs/lm1b/2026.01.31/133105/checkpoints/last.ckpt\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── train                                                                   \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: 500                                                  \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1                                                     \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/outputs/lm1b/2026.01.31/133948/checkpoints/las\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 500                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - bimodal_gaussian                                                      \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.01.31/133948                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/drive/MyDrive/DIFF_2/rework/petran_new_pretra\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/133948/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/133948/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: bimodal_gaussian                                                  \n",
            "│       w1: 0.6                                                                 \n",
            "│       mu1: 0.1                                                                \n",
            "│       sigma1: 0.02                                                            \n",
            "│       sigma2: 0.08                                                            \n",
            "│       m_start: 0.4                                                            \n",
            "│       m_end: 0.85                                                             \n",
            "│       tau_scale: 3.0                                                          \n",
            "│       p_min: 1.0e-05                                                          \n",
            "│       u_eps: 1.0e-06                                                          \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-01-31 13:39:48,659][__main__][INFO] - Starting Training.\n",
            "[2026-01-31 13:39:48,667][__main__][INFO] - Resuming training at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "[2026-01-31 13:39:48,692][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_train_bs128_wrapped.dat\n",
            "[2026-01-31 13:39:48,692][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 500 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 500 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 2248.31 examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 2234.35 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 36992.68 examples/s]\n",
            "[2026-01-31 13:39:50,678][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-01-31 13:39:50,678][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 100 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 100 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 2109.75 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 22324.38 examples/s]\n",
            "Printing train dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] while athletes in different professions dealt with doping scandals and other controversies, woods continued to do what he did best : dominate the field of professional golf and rake in endorsements. [CLS] the minutes could shed light on an internal debate, which has been evident in fed officials'recent speeches, over when to consider raising rates.\n",
            "ids: tensor([  101,  2096,  7576,  1999,  2367, 22797,  9411,  2007, 23799, 29609,\n",
            "         1998,  2060, 25962,  1010,  5249,  2506,  2000,  2079,  2054,  2002,\n",
            "         2106,  2190,  1024, 16083,  1996,  2492,  1997,  2658,  5439,  1998,\n",
            "        26008,  1999, 20380,  2015,  1012,   101,  1996,  2781,  2071,  8328,\n",
            "         2422,  2006,  2019,  4722,  5981,  1010,  2029,  2038,  2042, 10358,\n",
            "         1999,  7349,  4584,  1005,  3522, 13867,  1010,  2058,  2043,  2000,\n",
            "         5136,  6274,  6165,  1012])\n",
            "Last 64 tokens: [CLS] but metro's definition of \" urbanite \" is not accurate, says matthew gandy, director of the urban laboratory at university college london. [CLS] goodrich petroleum is an independent oil and gas exploration and production company listed on the new york stock exchange. [CLS] the support from spears is to end on november 15 [CLS]\n",
            "ids: tensor([  101,  2021,  6005,  1005,  1055,  6210,  1997,  1000,  3923,  4221,\n",
            "         1000,  2003,  2025,  8321,  1010,  2758,  5487, 25957,  5149,  1010,\n",
            "         2472,  1997,  1996,  3923,  5911,  2012,  2118,  2267,  2414,  1012,\n",
            "          101,  2204, 13149, 11540,  2003,  2019,  2981,  3514,  1998,  3806,\n",
            "         8993,  1998,  2537,  2194,  3205,  2006,  1996,  2047,  2259,  4518,\n",
            "         3863,  1012,   101,  1996,  2490,  2013, 13957,  2003,  2000,  2203,\n",
            "         2006,  2281,  2321,   101])\n",
            "Printing valid dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] in medieval times, the townspeople from norcia were so practised at butchering pigs that they gained an in - depth knowledge of anatomy and were allowed to practise as surgeons along with members of the clergy. [CLS] the satellite weighs some 660 pounds, the israeli haaretz newspaper reported, citing unnamed company officials.\n",
            "ids: tensor([  101,  1999,  5781,  2335,  1010,  1996, 27938,  2013,  4496,  7405,\n",
            "         2020,  2061, 20439,  2012, 14998,  2075, 14695,  2008,  2027,  4227,\n",
            "         2019,  1999,  1011,  5995,  3716,  1997, 13336,  1998,  2020,  3039,\n",
            "         2000, 10975, 18908,  5562,  2004, 16804,  2247,  2007,  2372,  1997,\n",
            "         1996, 11646,  1012,   101,  1996,  5871, 21094,  2070, 20982,  7038,\n",
            "         1010,  1996,  5611,  5292, 12069,  5753,  3780,  2988,  1010,  8951,\n",
            "        13294,  2194,  4584,  1012])\n",
            "Last 64 tokens: [CLS] the situation has been further complicated by growing indications that the us treasury is preparing to make funds available for recapitalsing us banks. [CLS] as north korea's close neighbor, traditional ally and main provider of economic aid, china is widely believed to hold the key to solving the north korea conundrum. [CLS]\n",
            "ids: tensor([  101,  1996,  3663,  2038,  2042,  2582,  8552,  2011,  3652, 24936,\n",
            "         2008,  1996,  2149,  9837,  2003,  8225,  2000,  2191,  5029,  2800,\n",
            "         2005, 28667,  9331, 18400,  7741,  2149,  5085,  1012,   101,  2004,\n",
            "         2167,  4420,  1005,  1055,  2485, 11429,  1010,  3151,  9698,  1998,\n",
            "         2364, 10802,  1997,  3171,  4681,  1010,  2859,  2003,  4235,  3373,\n",
            "         2000,  2907,  1996,  3145,  2000, 13729,  1996,  2167,  4420,  9530,\n",
            "         8630,  6824,  1012,   101])\n",
            "[2026-01-31 13:40:50,919][__main__][INFO] - Initializing new model\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "wandb: WARNING The anonymous setting has no effect and will be removed in a future version.\n",
            "Restoring states from the checkpoint path at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "Loading checkpoint at 400\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/content/bd3lms/outputs/lm1b/2026.01.30/230437/checkpoints' to '/content/outputs/lm1b/2026.01.31/133948/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type                 | Params | Mode \n",
            "----------------------------------------------------------\n",
            "0 | backbone | DIT                  | 22.8 M | train\n",
            "1 | noise    | BimodalGaussianNoise | 0      | train\n",
            "----------------------------------------------------------\n",
            "22.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 M    Total params\n",
            "91.266    Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Restored all states from the checkpoint at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py:221: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "Epoch 13, global step 404: 'val/nll' reached 5.21665 (best 5.21665), saving model to '/content/outputs/lm1b/2026.01.31/133948/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 406: 'val/nll' reached 3.73282 (best 3.73282), saving model to '/content/outputs/lm1b/2026.01.31/133948/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 408: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 410: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 412: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 414: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 416: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 418: 'val/nll' reached 3.72711 (best 3.72711), saving model to '/content/outputs/lm1b/2026.01.31/133948/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 420: 'val/nll' reached 3.65553 (best 3.65553), saving model to '/content/outputs/lm1b/2026.01.31/133948/checkpoints/best.ckpt' as top 1\n",
            "Epoch 14, global step 422: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 424: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 426: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 428: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 430: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 432: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 434: 'val/nll' reached 3.53998 (best 3.53998), saving model to '/content/outputs/lm1b/2026.01.31/133948/checkpoints/best.ckpt' as top 1\n",
            "Epoch 14, global step 436: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 438: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 440: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 442: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 444: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 446: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 448: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 450: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 452: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 454: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 456: 'val/nll' reached 3.51877 (best 3.51877), saving model to '/content/outputs/lm1b/2026.01.31/133948/checkpoints/best.ckpt' as top 1\n",
            "Epoch 15, global step 458: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 460: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 462: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 464: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 466: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 468: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 470: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 472: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 474: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 476: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 478: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 480: 'val/nll' reached 3.49229 (best 3.49229), saving model to '/content/outputs/lm1b/2026.01.31/133948/checkpoints/best.ckpt' as top 1\n",
            "Epoch 16, global step 482: 'val/nll' reached 3.46946 (best 3.46946), saving model to '/content/outputs/lm1b/2026.01.31/133948/checkpoints/best.ckpt' as top 1\n",
            "Epoch 16, global step 484: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 486: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 488: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 490: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 492: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 494: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 496: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 498: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 500: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=500` reached.\n",
            "Checkpoint for config: 7 saved at /content/outputs/lm1b/2026.01.31/133948/checkpoints/last.ckpt\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── train                                                                   \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 8                                                                       \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: 500                                                  \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize8                               \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 0.5                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/outputs/lm1b/2026.01.31/134513/checkpoints/las\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 500                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.01.31/134513                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/drive/MyDrive/DIFF_2/rework/petran_new_pretra\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/134513/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.01.31/134513/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-01-31 13:45:14,051][__main__][INFO] - Starting Training.\n",
            "[2026-01-31 13:45:14,060][__main__][INFO] - Resuming training at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "[2026-01-31 13:45:14,084][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_train_bs128_wrapped.dat\n",
            "[2026-01-31 13:45:14,084][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 500 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 500 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 1928.14 examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 1917.97 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/500 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 500/500 [00:00<00:00, 35894.77 examples/s]\n",
            "[2026-01-31 13:45:16,097][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-01-31 13:45:16,097][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 100 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 100 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 1977.53 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 20944.29 examples/s]\n",
            "Printing train dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] while athletes in different professions dealt with doping scandals and other controversies, woods continued to do what he did best : dominate the field of professional golf and rake in endorsements. [CLS] the minutes could shed light on an internal debate, which has been evident in fed officials'recent speeches, over when to consider raising rates.\n",
            "ids: tensor([  101,  2096,  7576,  1999,  2367, 22797,  9411,  2007, 23799, 29609,\n",
            "         1998,  2060, 25962,  1010,  5249,  2506,  2000,  2079,  2054,  2002,\n",
            "         2106,  2190,  1024, 16083,  1996,  2492,  1997,  2658,  5439,  1998,\n",
            "        26008,  1999, 20380,  2015,  1012,   101,  1996,  2781,  2071,  8328,\n",
            "         2422,  2006,  2019,  4722,  5981,  1010,  2029,  2038,  2042, 10358,\n",
            "         1999,  7349,  4584,  1005,  3522, 13867,  1010,  2058,  2043,  2000,\n",
            "         5136,  6274,  6165,  1012])\n",
            "Last 64 tokens: [CLS] but metro's definition of \" urbanite \" is not accurate, says matthew gandy, director of the urban laboratory at university college london. [CLS] goodrich petroleum is an independent oil and gas exploration and production company listed on the new york stock exchange. [CLS] the support from spears is to end on november 15 [CLS]\n",
            "ids: tensor([  101,  2021,  6005,  1005,  1055,  6210,  1997,  1000,  3923,  4221,\n",
            "         1000,  2003,  2025,  8321,  1010,  2758,  5487, 25957,  5149,  1010,\n",
            "         2472,  1997,  1996,  3923,  5911,  2012,  2118,  2267,  2414,  1012,\n",
            "          101,  2204, 13149, 11540,  2003,  2019,  2981,  3514,  1998,  3806,\n",
            "         8993,  1998,  2537,  2194,  3205,  2006,  1996,  2047,  2259,  4518,\n",
            "         3863,  1012,   101,  1996,  2490,  2013, 13957,  2003,  2000,  2203,\n",
            "         2006,  2281,  2321,   101])\n",
            "Printing valid dataloader batch.\n",
            "Batch input_ids.shape torch.Size([4, 128])\n",
            "First 64 tokens: [CLS] in medieval times, the townspeople from norcia were so practised at butchering pigs that they gained an in - depth knowledge of anatomy and were allowed to practise as surgeons along with members of the clergy. [CLS] the satellite weighs some 660 pounds, the israeli haaretz newspaper reported, citing unnamed company officials.\n",
            "ids: tensor([  101,  1999,  5781,  2335,  1010,  1996, 27938,  2013,  4496,  7405,\n",
            "         2020,  2061, 20439,  2012, 14998,  2075, 14695,  2008,  2027,  4227,\n",
            "         2019,  1999,  1011,  5995,  3716,  1997, 13336,  1998,  2020,  3039,\n",
            "         2000, 10975, 18908,  5562,  2004, 16804,  2247,  2007,  2372,  1997,\n",
            "         1996, 11646,  1012,   101,  1996,  5871, 21094,  2070, 20982,  7038,\n",
            "         1010,  1996,  5611,  5292, 12069,  5753,  3780,  2988,  1010,  8951,\n",
            "        13294,  2194,  4584,  1012])\n",
            "Last 64 tokens: [CLS] the situation has been further complicated by growing indications that the us treasury is preparing to make funds available for recapitalsing us banks. [CLS] as north korea's close neighbor, traditional ally and main provider of economic aid, china is widely believed to hold the key to solving the north korea conundrum. [CLS]\n",
            "ids: tensor([  101,  1996,  3663,  2038,  2042,  2582,  8552,  2011,  3652, 24936,\n",
            "         2008,  1996,  2149,  9837,  2003,  8225,  2000,  2191,  5029,  2800,\n",
            "         2005, 28667,  9331, 18400,  7741,  2149,  5085,  1012,   101,  2004,\n",
            "         2167,  4420,  1005,  1055,  2485, 11429,  1010,  3151,  9698,  1998,\n",
            "         2364, 10802,  1997,  3171,  4681,  1010,  2859,  2003,  4235,  3373,\n",
            "         2000,  2907,  1996,  3145,  2000, 13729,  1996,  2167,  4420,  9530,\n",
            "         8630,  6824,  1012,   101])\n",
            "[2026-01-31 13:46:16,336][__main__][INFO] - Initializing new model\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "wandb: WARNING The anonymous setting has no effect and will be removed in a future version.\n",
            "Restoring states from the checkpoint path at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "Loading checkpoint at 400\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=0.5\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/content/bd3lms/outputs/lm1b/2026.01.30/230437/checkpoints' to '/content/outputs/lm1b/2026.01.31/134513/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type           | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | backbone | DIT            | 22.8 M | train\n",
            "1 | noise    | LogLinearNoise | 0      | train\n",
            "----------------------------------------------------\n",
            "22.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 M    Total params\n",
            "91.266    Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Restored all states from the checkpoint at /content/drive/MyDrive/DIFF_2/rework/petran_new_pretrained/last.ckpt\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py:221: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.\n",
            "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: Encountered `nan` values in tensor. Will be removed.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "Epoch 13, global step 404: 'val/nll' reached 8.12482 (best 8.12482), saving model to '/content/outputs/lm1b/2026.01.31/134513/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 406: 'val/nll' reached 7.00428 (best 7.00428), saving model to '/content/outputs/lm1b/2026.01.31/134513/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 408: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 410: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 412: 'val/nll' reached 6.96647 (best 6.96647), saving model to '/content/outputs/lm1b/2026.01.31/134513/checkpoints/best.ckpt' as top 1\n",
            "Epoch 13, global step 414: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 416: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 418: 'val/nll' was not in top 1\n",
            "Epoch 13, global step 420: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 422: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 424: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 426: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 428: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 430: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 432: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 434: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 436: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 438: 'val/nll' was not in top 1\n",
            "Epoch 14, global step 440: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 442: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 444: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 446: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 448: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 450: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 452: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 454: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 456: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 458: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 460: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 462: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 464: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 466: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 468: 'val/nll' was not in top 1\n",
            "Epoch 15, global step 470: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 472: 'val/nll' reached 6.87211 (best 6.87211), saving model to '/content/outputs/lm1b/2026.01.31/134513/checkpoints/best.ckpt' as top 1\n",
            "Epoch 16, global step 474: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 476: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 478: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 480: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 482: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 484: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 486: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 488: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 490: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 492: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 494: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 496: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 498: 'val/nll' was not in top 1\n",
            "Epoch 16, global step 500: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=500` reached.\n",
            "Checkpoint for config: 8 saved at /content/outputs/lm1b/2026.01.31/134513/checkpoints/last.ckpt\n"
          ]
        }
      ],
      "source": [
        "BASE_DIR = '/content/drive/MyDrive/DIFF_2/rework'\n",
        "results = []\n",
        "\n",
        "model_ckpt = train_model(\n",
        "    algo=\"bd3lm\",\n",
        "    block_size=16,\n",
        "    trainer_max_steps=500,\n",
        "    sampling_eps_min = 0.3,\n",
        "    sampling_eps_max = 0.8,\n",
        "    resume_from_ckpt=True,\n",
        "    resume_ckpt_path=BASE_DIR,\n",
        "    max_train_samples=500,\n",
        "    max_valid_samples=100,\n",
        "    max_test_samples=100,\n",
        ")\n",
        "\n",
        "print(f\"Checkpoint for Block size: 16 saved at {model_ckpt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dABwxTbvGXcJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dABwxTbvGXcJ",
        "outputId": "fe3f90b5-8fb4-4f76-cfdd-0ddc07a5422c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sending incremental file list\n",
            "./\n",
            "lm1b/\n",
            "lm1b/2026.02.01/\n",
            "lm1b/2026.02.01/091539/\n",
            "lm1b/2026.02.01/091539/config_tree.txt\n",
            "lm1b/2026.02.01/091539/main.log\n",
            "lm1b/2026.02.01/091539/.hydra/\n",
            "lm1b/2026.02.01/091539/.hydra/config.yaml\n",
            "lm1b/2026.02.01/091539/.hydra/hydra.yaml\n",
            "lm1b/2026.02.01/091539/.hydra/overrides.yaml\n",
            "lm1b/2026.02.01/091539/checkpoints/\n",
            "lm1b/2026.02.01/091539/checkpoints/16-500.ckpt\n",
            "lm1b/2026.02.01/091539/checkpoints/best.ckpt\n",
            "lm1b/2026.02.01/091539/checkpoints/last.ckpt\n",
            "lm1b/2026.02.01/095950/\n",
            "lm1b/2026.02.01/095950/config_tree.txt\n",
            "lm1b/2026.02.01/095950/main.log\n",
            "lm1b/2026.02.01/095950/.hydra/\n",
            "lm1b/2026.02.01/095950/.hydra/config.yaml\n",
            "lm1b/2026.02.01/095950/.hydra/hydra.yaml\n",
            "lm1b/2026.02.01/095950/.hydra/overrides.yaml\n",
            "lm1b/2026.02.01/095950/checkpoints/\n",
            "lm1b/2026.02.01/095950/checkpoints/16-500.ckpt\n",
            "lm1b/2026.02.01/095950/checkpoints/best.ckpt\n",
            "lm1b/2026.02.01/095950/checkpoints/last.ckpt\n",
            "lm1b/2026.02.01/102427/\n",
            "lm1b/2026.02.01/102427/config_tree.txt\n",
            "lm1b/2026.02.01/102427/main.log\n",
            "lm1b/2026.02.01/102427/.hydra/\n",
            "lm1b/2026.02.01/102427/.hydra/config.yaml\n",
            "lm1b/2026.02.01/102427/.hydra/hydra.yaml\n",
            "lm1b/2026.02.01/102427/.hydra/overrides.yaml\n",
            "lm1b/2026.02.01/102427/checkpoints/\n",
            "lm1b/2026.02.01/102427/checkpoints/16-500.ckpt\n",
            "lm1b/2026.02.01/102427/checkpoints/best.ckpt\n",
            "lm1b/2026.02.01/102427/checkpoints/last.ckpt\n",
            "\n",
            "sent 3,254,504,328 bytes  received 543 bytes  84,532,594.05 bytes/sec\n",
            "total size is 3,253,707,836  speedup is 1.00\n"
          ]
        }
      ],
      "source": [
        "!rsync -av \\\n",
        "  /content/outputs/ \\\n",
        "  /content/drive/MyDrive/DIFF_2/rework/synced/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VWKADjeuJCOV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWKADjeuJCOV",
        "outputId": "2846297b-7bfe-4d9d-eebc-d184cf053b0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── ppl_eval                                                                \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: null                                                 \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1.0                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/drive/MyDrive/DIFF_2/rework/checkpoints/petran\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 300                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.02.03/011914                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/outputs/lm1b/2026.02.03/011914/checkpoints/la\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/011914/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/011914/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-02-03 01:19:14,563][__main__][INFO] - Starting Eval.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "Loading checkpoint at 472\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1.0\n",
            "[2026-02-03 01:19:17,390][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-03 01:19:17,390][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 2102.62 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 20897.33 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-03 01:20:41.100583: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770081641.117602    6612 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770081641.122513    6612 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770081641.137053    6612 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770081641.137076    6612 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770081641.137079    6612 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770081641.137084    6612 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-03 01:20:41.141733: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│     sampling_eps_max      │            1.0            │\n",
            "│     sampling_eps_min      │            0.0            │\n",
            "│          val/bpd          │    10.332172393798828     │\n",
            "│          val/nll          │     7.161715984344482     │\n",
            "│          val/ppl          │    1289.1212158203125     │\n",
            "│     valid_var_0.0 - 1     │    10.492673873901367     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Result for config 1 is (1289.12, 10.49)\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── ppl_eval                                                                \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: null                                                 \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1.0                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/drive/MyDrive/DIFF_2/rework/checkpoints/petran\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 300                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.02.03/012105                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/outputs/lm1b/2026.02.03/012105/checkpoints/la\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/012105/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/012105/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-02-03 01:21:05,760][__main__][INFO] - Starting Eval.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "Loading checkpoint at 472\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1.0\n",
            "[2026-02-03 01:21:08,667][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-03 01:21:08,667][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 1984.99 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 20400.31 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-03 01:22:34.023772: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770081754.041106    7100 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770081754.045960    7100 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770081754.058667    7100 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770081754.058688    7100 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770081754.058691    7100 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770081754.058694    7100 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-03 01:22:34.062568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│     sampling_eps_max      │            1.0            │\n",
            "│     sampling_eps_min      │            0.0            │\n",
            "│          val/bpd          │    10.322482109069824     │\n",
            "│          val/nll          │     7.154999732971191     │\n",
            "│          val/ppl          │    1280.4920654296875     │\n",
            "│     valid_var_0.0 - 1     │    10.466012954711914     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Result for config 2 is (1280.49, 10.47)\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── ppl_eval                                                                \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: null                                                 \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1.0                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/drive/MyDrive/DIFF_2/rework/checkpoints/petran\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 300                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.02.03/012258                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/outputs/lm1b/2026.02.03/012258/checkpoints/la\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/012258/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/012258/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-02-03 01:22:58,637][__main__][INFO] - Starting Eval.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "Loading checkpoint at 424\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1.0\n",
            "[2026-02-03 01:23:01,420][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-03 01:23:01,420][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 1104.79 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 11246.59 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-03 01:24:25.662872: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770081865.679789    7593 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770081865.684816    7593 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770081865.697682    7593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770081865.697703    7593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770081865.697706    7593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770081865.697708    7593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-03 01:24:25.701517: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│     sampling_eps_max      │            1.0            │\n",
            "│     sampling_eps_min      │            0.0            │\n",
            "│          val/bpd          │     10.26360034942627     │\n",
            "│          val/nll          │     7.114185810089111     │\n",
            "│          val/ppl          │    1229.2823486328125     │\n",
            "│     valid_var_0.0 - 1     │    10.231799125671387     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Result for config 3 is (1229.28, 10.23)\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── ppl_eval                                                                \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: null                                                 \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1.0                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/drive/MyDrive/DIFF_2/rework/checkpoints/petran\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 300                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.02.03/012449                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/outputs/lm1b/2026.02.03/012449/checkpoints/la\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/012449/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/012449/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-02-03 01:24:49,602][__main__][INFO] - Starting Eval.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "Loading checkpoint at 424\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1.0\n",
            "[2026-02-03 01:24:52,311][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-03 01:24:52,311][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 2125.55 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 20619.95 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-03 01:26:15.500124: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770081975.518588    8088 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770081975.523698    8088 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770081975.536706    8088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770081975.536727    8088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770081975.536730    8088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770081975.536733    8088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-03 01:26:15.540601: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│     sampling_eps_max      │            1.0            │\n",
            "│     sampling_eps_min      │            0.0            │\n",
            "│          val/bpd          │     10.26093578338623     │\n",
            "│          val/nll          │     7.112339019775391     │\n",
            "│          val/ppl          │     1227.01416015625      │\n",
            "│     valid_var_0.0 - 1     │    10.223197937011719     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Result for config 4 is (1227.01, 10.22)\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── ppl_eval                                                                \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: null                                                 \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1.0                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/drive/MyDrive/DIFF_2/rework/checkpoints/petran\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 300                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.02.03/012639                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/outputs/lm1b/2026.02.03/012639/checkpoints/la\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/012639/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/012639/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-02-03 01:26:40,237][__main__][INFO] - Starting Eval.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "Loading checkpoint at 434\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1.0\n",
            "[2026-02-03 01:26:42,299][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-03 01:26:42,299][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 2109.46 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 20679.93 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-03 01:28:36.372619: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770082116.389791    8588 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770082116.394789    8588 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770082116.407704    8588 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082116.407726    8588 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082116.407729    8588 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082116.407731    8588 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-03 01:28:36.411668: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│     sampling_eps_max      │            1.0            │\n",
            "│     sampling_eps_min      │            0.0            │\n",
            "│          val/bpd          │    10.266191482543945     │\n",
            "│          val/nll          │    7.1159820556640625     │\n",
            "│          val/ppl          │     1231.492431640625     │\n",
            "│     valid_var_0.0 - 1     │    10.256610870361328     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Result for config 5 is (1231.49, 10.26)\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── ppl_eval                                                                \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: null                                                 \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1.0                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/drive/MyDrive/DIFF_2/rework/checkpoints/petran\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 300                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.02.03/012900                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/outputs/lm1b/2026.02.03/012900/checkpoints/la\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/012900/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/012900/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-02-03 01:29:00,941][__main__][INFO] - Starting Eval.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "Loading checkpoint at 434\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1.0\n",
            "[2026-02-03 01:29:03,711][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-03 01:29:03,711][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 2126.93 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 20237.90 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-03 01:30:29.243250: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770082229.262571    9223 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770082229.268188    9223 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770082229.281429    9223 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082229.281447    9223 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082229.281450    9223 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082229.281454    9223 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-03 01:30:29.285381: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│     sampling_eps_max      │            1.0            │\n",
            "│     sampling_eps_min      │            0.0            │\n",
            "│          val/bpd          │    10.274116516113281     │\n",
            "│          val/nll          │     7.121474742889404     │\n",
            "│          val/ppl          │    1238.2752685546875     │\n",
            "│     valid_var_0.0 - 1     │    10.285325050354004     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Result for config 6 is (1238.28, 10.29)\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── ppl_eval                                                                \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: null                                                 \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1.0                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/drive/MyDrive/DIFF_2/rework/checkpoints/petran\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 300                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.02.03/013053                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/outputs/lm1b/2026.02.03/013053/checkpoints/la\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/013053/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/013053/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-02-03 01:30:53,945][__main__][INFO] - Starting Eval.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "Loading checkpoint at 448\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1.0\n",
            "[2026-02-03 01:30:56,664][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-03 01:30:56,664][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 1583.96 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 20195.02 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-03 01:32:19.808177: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770082339.825550    9739 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770082339.830722    9739 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770082339.843602    9739 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082339.843625    9739 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082339.843632    9739 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082339.843634    9739 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-03 01:32:19.847566: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│     sampling_eps_max      │            1.0            │\n",
            "│     sampling_eps_min      │            0.0            │\n",
            "│          val/bpd          │    10.291533470153809     │\n",
            "│          val/nll          │     7.133547782897949     │\n",
            "│          val/ppl          │    1253.3155517578125     │\n",
            "│     valid_var_0.0 - 1     │      10.372802734375      │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Result for config 7 is (1253.32, 10.37)\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── ppl_eval                                                                \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 16                                                                      \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: null                                                 \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize16                              \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1.0                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/drive/MyDrive/DIFF_2/rework/checkpoints/petran\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 300                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.02.03/013244                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/outputs/lm1b/2026.02.03/013244/checkpoints/la\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/013244/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/013244/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-02-03 01:32:44,436][__main__][INFO] - Starting Eval.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "Loading checkpoint at 482\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1.0\n",
            "[2026-02-03 01:32:46,809][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-03 01:32:46,809][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 1163.13 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 13260.94 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-03 01:34:54.324988: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770082494.354438   10245 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770082494.362210   10245 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770082494.382466   10245 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082494.382540   10245 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082494.382562   10245 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082494.382579   10245 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-03 01:34:54.388859: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│     sampling_eps_max      │            1.0            │\n",
            "│     sampling_eps_min      │            0.0            │\n",
            "│          val/bpd          │    10.350111961364746     │\n",
            "│          val/nll          │    7.1741509437561035     │\n",
            "│          val/ppl          │     1305.25146484375      │\n",
            "│     valid_var_0.0 - 1     │    10.561878204345703     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Result for config 8 is (1305.25, 10.56)\n",
            "Seed set to 1\n",
            "CONFIG\n",
            "├── mode\n",
            "│   └── ppl_eval                                                                \n",
            "├── diffusion\n",
            "│   └── absorbing_state                                                         \n",
            "├── seed\n",
            "│   └── 1                                                                       \n",
            "├── block_size\n",
            "│   └── 8                                                                       \n",
            "├── data\n",
            "│   └── max_samples: null                                                       \n",
            "│       max_train_samples: null                                                 \n",
            "│       max_valid_samples: 100                                                  \n",
            "│       max_test_samples: 100                                                   \n",
            "│       train: lm1b                                                             \n",
            "│       valid: lm1b                                                             \n",
            "│       tokenizer_name_or_path: bert-base-uncased                               \n",
            "│       cache_dir: /share/kuleshov/ssahoo/textdiffusion/data                    \n",
            "│       wrap: true                                                              \n",
            "│       streaming: true                                                         \n",
            "│                                                                               \n",
            "├── loader\n",
            "│   └── global_batch_size: 4                                                    \n",
            "│       eval_global_batch_size: 4                                               \n",
            "│       batch_size: 4                                                           \n",
            "│       eval_batch_size: 4                                                      \n",
            "│       num_workers: 1                                                          \n",
            "│       pin_memory: true                                                        \n",
            "│                                                                               \n",
            "├── sampling\n",
            "│   └── noise_removal: false                                                    \n",
            "│       num_sample_batches: 1                                                   \n",
            "│       var_length: false                                                       \n",
            "│       logdir: ./samples_bd3lm_len128_blocksize8                               \n",
            "│       nucleus_p: 1.0                                                          \n",
            "│       first_hitting: true                                                     \n",
            "│       kv_cache: false                                                         \n",
            "│                                                                               \n",
            "├── training\n",
            "│   └── ema: 0.9999                                                             \n",
            "│       antithetic_sampling: true                                               \n",
            "│       sampling_eps: 0.001                                                     \n",
            "│       coeff_clip: -1.0                                                        \n",
            "│       resample: false                                                         \n",
            "│       sampling_eps_min: 0.001                                                 \n",
            "│       sampling_eps_max: 1.0                                                   \n",
            "│       nll_diagram: false                                                      \n",
            "│       from_pretrained: null                                                   \n",
            "│       eval_nll: true                                                          \n",
            "│                                                                               \n",
            "├── eval\n",
            "│   └── checkpoint_path: /content/drive/MyDrive/DIFF_2/rework/checkpoints/petran\n",
            "│       disable_ema: false                                                      \n",
            "│       perplexity_batch_size: 8                                                \n",
            "│       compute_perplexity_on_sanity: false                                     \n",
            "│       gen_ppl_eval_model_name_or_path: gpt2-large                             \n",
            "│       generate_samples: false                                                 \n",
            "│                                                                               \n",
            "├── optim\n",
            "│   └── weight_decay: 0                                                         \n",
            "│       lr: 0.0003                                                              \n",
            "│       beta1: 0.9                                                              \n",
            "│       beta2: 0.999                                                            \n",
            "│       eps: 1.0e-08                                                            \n",
            "│                                                                               \n",
            "├── trainer\n",
            "│   └── _target_: lightning.Trainer                                             \n",
            "│       accelerator: cuda                                                       \n",
            "│       num_nodes: 1                                                            \n",
            "│       devices: 1                                                              \n",
            "│       accumulate_grad_batches: 1                                              \n",
            "│       gradient_clip_val: 1.0                                                  \n",
            "│       enable_progress_bar: false                                              \n",
            "│       precision: bf16                                                         \n",
            "│       num_sanity_val_steps: 2                                                 \n",
            "│       max_steps: 300                                                          \n",
            "│       log_every_n_steps: 1000                                                 \n",
            "│       limit_train_batches: 1.0                                                \n",
            "│       limit_val_batches: 1.0                                                  \n",
            "│       val_check_interval: 2                                                   \n",
            "│                                                                               \n",
            "├── wandb\n",
            "│   └── project: BD3-LMs                                                        \n",
            "│       notes: Block Denoising Discrete Diffusion Language Models               \n",
            "│       group: null                                                             \n",
            "│       job_type: null                                                          \n",
            "│       name: null                                                              \n",
            "│       id: None_1                                                              \n",
            "│       tags:                                                                   \n",
            "│       - loglinear                                                             \n",
            "│       - lm1b                                                                  \n",
            "│       - lm1b                                                                  \n",
            "│                                                                               \n",
            "├── checkpointing\n",
            "│   └── save_dir: /content/outputs/lm1b/2026.02.03/013518                       \n",
            "│       resume_from_ckpt: true                                                  \n",
            "│       resume_ckpt_path: /content/outputs/lm1b/2026.02.03/013518/checkpoints/la\n",
            "│                                                                               \n",
            "├── callbacks\n",
            "│   └── checkpoint_every_n_steps:                                               \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         save_top_k: -1                                                        \n",
            "│         save_last: true                                                       \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/013518/checkpoints          \n",
            "│         verbose: true                                                         \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         every_n_train_steps: 500                                              \n",
            "│       checkpoint_monitor:                                                     \n",
            "│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 \n",
            "│         monitor: val/nll                                                      \n",
            "│         mode: min                                                             \n",
            "│         save_top_k: 1                                                         \n",
            "│         save_last: false                                                      \n",
            "│         dirpath: /content/outputs/lm1b/2026.02.03/013518/checkpoints          \n",
            "│         filename: best                                                        \n",
            "│         auto_insert_metric_name: false                                        \n",
            "│         verbose: true                                                         \n",
            "│       learning_rate_monitor:                                                  \n",
            "│         _target_: lightning.pytorch.callbacks.LearningRateMonitor             \n",
            "│         logging_interval: step                                                \n",
            "│                                                                               \n",
            "├── model\n",
            "│   └── name: tiny                                                              \n",
            "│       type: ddit                                                              \n",
            "│       hidden_size: 256                                                        \n",
            "│       cond_dim: 64                                                            \n",
            "│       length: 128                                                             \n",
            "│       n_blocks: 8                                                             \n",
            "│       n_heads: 8                                                              \n",
            "│       scale_by_sigma: true                                                    \n",
            "│       dropout: 0.1                                                            \n",
            "│       tie_word_embeddings: true                                               \n",
            "│       adaln: false                                                            \n",
            "│       attn_backend: sdpa                                                      \n",
            "│                                                                               \n",
            "├── strategy\n",
            "│   └── _target_: lightning.pytorch.strategies.DDPStrategy                      \n",
            "│       find_unused_parameters: false                                           \n",
            "│                                                                               \n",
            "├── noise\n",
            "│   └── type: loglinear                                                         \n",
            "│       sigma_min: 0.0001                                                       \n",
            "│       sigma_max: 20                                                           \n",
            "│                                                                               \n",
            "├── lr_scheduler\n",
            "│   └── _target_: transformers.get_constant_schedule_with_warmup                \n",
            "│       num_warmup_steps: 2500                                                  \n",
            "│                                                                               \n",
            "└── algo\n",
            "    └── name: bd3lm                                                             \n",
            "        backbone: dit                                                           \n",
            "        parameterization: subs                                                  \n",
            "        time_conditioning: false                                                \n",
            "        T: 0                                                                    \n",
            "        causal_attention: false                                                 \n",
            "        dropout: 0.0                                                            \n",
            "        ignore_bos: true                                                        \n",
            "        cross_attn: true                                                        \n",
            "        var_min: true                                                           \n",
            "        clip_search_delta: 0.05                                                 \n",
            "        clip_search_widths: []                                                  \n",
            "        fix_clipping: false                                                     \n",
            "        sampler: semi_ar                                                        \n",
            "        mdlm_loss_scale: false                                                  \n",
            "        reweight_loss: false                                                    \n",
            "        w_type: simple                                                          \n",
            "                                                                                \n",
            "[2026-02-03 01:35:18,472][__main__][INFO] - Starting Eval.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/fabric/connector.py:572: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "Loading checkpoint at 472\n",
            "✓ Overriding sampling_eps in checkpoint before load: min=0.001, max=1.0\n",
            "[2026-02-03 01:35:21,717][dataloader][INFO] - Generating new data at: /share/kuleshov/ssahoo/textdiffusion/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-03 01:35:21,717][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 1155.81 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 100/100 [00:00<00:00, 13074.11 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-03 01:37:28.430118: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770082648.457949   10940 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770082648.466292   10940 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770082648.486278   10940 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082648.486384   10940 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082648.486401   10940 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770082648.486417   10940 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-03 01:37:28.492787: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_min', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_min': ...})` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('sampling_eps_max', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'sampling_eps_max': ...})` instead.\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│     sampling_eps_max      │            1.0            │\n",
            "│     sampling_eps_min      │            0.0            │\n",
            "│          val/bpd          │     10.7750883102417      │\n",
            "│          val/nll          │     7.468721866607666     │\n",
            "│          val/ppl          │     1752.365478515625     │\n",
            "│     valid_var_0.0 - 1     │     35.94629669189453     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "Result for config 9 is (1752.37, 35.95)\n",
            "+--------+--------------+---------+------------+\n",
            "| Config | block_size_L | val_ppl | var. NELBO |\n",
            "+--------+--------------+---------+------------+\n",
            "| {1}    | 16           | 1289.12 | 10.49      |\n",
            "| {2}    | 16           | 1280.49 | 10.47      |\n",
            "| {3}    | 16           | 1229.28 | 10.23      |\n",
            "| {4}    | 16           | 1227.01 | 10.22      |\n",
            "| {5}    | 16           | 1231.49 | 10.26      |\n",
            "| {6}    | 16           | 1238.28 | 10.29      |\n",
            "| {7}    | 16           | 1253.32 | 10.37      |\n",
            "| {8}    | 16           | 1305.25 | 10.56      |\n",
            "| {9}    | 16           | 1752.37 | 35.95      |\n",
            "+--------+--------------+---------+------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "result = eval_model(\n",
        "    algo=\"bd3lm\",\n",
        "    block_size=16,\n",
        "    checkpoint_path=BASE_DIR,\n",
        "    max_valid_samples=100,\n",
        "    max_test_samples=100,\n",
        ")\n",
        "\n",
        "results.append({\"block_size_L\": 16, \"val_ppl\": result[0], \"var. NELBO\": result[1]})\n",
        "\n",
        "print_table(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
