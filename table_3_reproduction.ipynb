{"cells":[{"cell_type":"code","execution_count":1,"id":"f834b118","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1355,"status":"ok","timestamp":1769471854178,"user":{"displayName":"Sitanium Games","userId":"11646386209711617651"},"user_tz":-120},"id":"f834b118","outputId":"95107f53-13ea-4ca4-91a2-6968166fd393"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'bd3lms'...\n","remote: Enumerating objects: 882, done.\u001b[K\n","remote: Counting objects: 100% (340/340), done.\u001b[K\n","remote: Compressing objects: 100% (134/134), done.\u001b[K\n","remote: Total 882 (delta 261), reused 248 (delta 205), pack-reused 542 (from 1)\u001b[K\n","Receiving objects: 100% (882/882), 3.13 MiB | 31.07 MiB/s, done.\n","Resolving deltas: 100% (553/553), done.\n"]}],"source":["!cd /content && git clone https://github.com/ntua-el21050/bd3lms.git"]},{"cell_type":"code","execution_count":2,"id":"XMUalUkud3_x","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":85,"status":"ok","timestamp":1769471854268,"user":{"displayName":"Sitanium Games","userId":"11646386209711617651"},"user_tz":-120},"id":"XMUalUkud3_x","outputId":"f9179515-25df-46d9-811b-68af1ee36467"},"outputs":[{"output_type":"stream","name":"stdout","text":["total 5040\n","-rw-r--r-- 1 root root  862037 Jan 26 23:57  2503.09573v3.pdf\n","drwxr-xr-x 9 root root    4096 Jan 26 23:57  configs\n","-rw-r--r-- 1 root root   33814 Jan 26 23:57  dataloader.py\n","-rw-r--r-- 1 root root   46696 Jan 26 23:57  diffusion.py\n","-rw-r--r-- 1 root root   56775 Jan 26 23:57  extension1_edm.ipynb\n","-rw-r--r-- 1 root root   58511 Jan 26 23:57  extension1_fm.ipynb\n","-rw-r--r-- 1 root root   58620 Jan 26 23:57  extension1_iddpm.ipynb\n","-rw-r--r-- 1 root root   58457 Jan 26 23:57  extension1_sigmoid.ipynb\n","-rw-r--r-- 1 root root   59098 Jan 26 23:57  extension1_sigmoid_val.ipynb\n","-rw-r--r-- 1 root root   58505 Jan 26 23:57  extension1_simple.ipynb\n","drwxr-xr-x 2 root root    4096 Jan 26 23:57  final_presentation\n","-rw-r--r-- 1 root root  225205 Jan 26 23:57  graphical_abstract.png\n","-rw-r--r-- 1 root root   11357 Jan 26 23:57  LICENSE\n","-rw-r--r-- 1 root root    7873 Jan 26 23:57  main.py\n","-rw-r--r-- 1 root root    8405 Jan 26 23:57  metrics.py\n","-rw-r--r-- 1 root root  257463 Jan 26 23:57 'midterm_presentation_pattern_recognition (5).pdf'\n","drwxr-xr-x 3 root root    4096 Jan 26 23:57  models\n","-rw-r--r-- 1 root root    9371 Jan 26 23:57  noise_schedule.py\n","-rw-r--r-- 1 root root    1449 Jan 26 23:57  push_to_hf.py\n","-rw-r--r-- 1 root root   10070 Jan 26 23:57  README.md\n","-rw-r--r-- 1 root root     363 Jan 26 23:57  requirements.txt\n","drwxr-xr-x 7 root root    4096 Jan 26 23:57  scripts\n","drwxr-xr-x 4 root root    4096 Jan 26 23:57  ssd-lm\n","-rw-r--r-- 1 root root  327057 Jan 26 23:57  table_1_diagram_2.ipynb\n","-rw-r--r-- 1 root root  525005 Jan 26 23:57  table_2_reproduction.ipynb\n","-rw-r--r-- 1 root root  102002 Jan 26 23:57  table_3_reproduction.ipynb\n","-rw-r--r-- 1 root root  105567 Jan 26 23:57  table4_initial_reproduction.ipynb\n","-rw-r--r-- 1 root root  103948 Jan 26 23:57 'table4_more_training _initial_reproduction.ipynb'\n","-rw-r--r-- 1 root root  304906 Jan 26 23:57  table_5_reproduction_initial.ipynb\n","-rw-r--r-- 1 root root  333461 Jan 26 23:57  Table_6_WORKING_initial.ipynb\n","-rw-r--r-- 1 root root 1262898 Jan 26 23:57  table_7_reproduction.ipynb\n","-rw-r--r-- 1 root root   86421 Jan 26 23:57  table_8_reproduction_corrected_initial_reproduction.ipynb\n","-rw-r--r-- 1 root root   96726 Jan 26 23:57  table_8_v2_more_training_initial_reproduction.ipynb\n","-rw-r--r-- 1 root root    7162 Jan 26 23:57  utils.py\n"]}],"source":["!ls -l /content/bd3lms"]},{"cell_type":"code","execution_count":3,"id":"iDMyFkBed9i1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35989,"status":"ok","timestamp":1769471890261,"user":{"displayName":"Sitanium Games","userId":"11646386209711617651"},"user_tz":-120},"id":"iDMyFkBed9i1","outputId":"c4ab3b96-6f30-472c-cb24-586f07e45ba0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m142.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m135.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m150.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.1 which is incompatible.\n","tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","google-cloud-bigquery 3.40.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n","umap-learn 0.5.11 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\n","hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n","jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\n","db-dtypes 1.5.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n","xarray 2025.12.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n","jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q \\\n","    torchmetrics==1.6.2 \\\n","    datasets==3.3.2 \\\n","    einops==0.8.1 \\\n","    fsspec==2024.2.0 \\\n","    hydra-core==1.3.2 \\\n","    lightning==2.5.0.post0 \\\n","    omegaconf==2.3.0 \\\n","    packaging==23.2 \\\n","    pandas==2.2.1 \\\n","    rich==13.7.1 \\\n","    scikit-learn==1.5.1 \\\n","    timm==0.9.16 \\\n","    transformers==4.49.0 \\\n","    matplotlib==3.10.0 \\\n","    wandb"]},{"cell_type":"code","execution_count":4,"id":"11c247d1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37968,"status":"ok","timestamp":1769471928245,"user":{"displayName":"Sitanium Games","userId":"11646386209711617651"},"user_tz":-120},"id":"11c247d1","outputId":"3c4a734b-611d-4a5e-9580-050b6a5a20fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Local checkpoint save root: /content/repro_runs\n","Drive sync root: /content/drive/MyDrive/DIFF_2\n"]}],"source":["# Mount Google Drive (Colab) and choose where to save checkpoints\n","from pathlib import Path\n","import os\n","\n","try:\n","    from google.colab import drive  # type: ignore\n","    drive.mount('/content/drive')\n","    IN_COLAB = True\n","except Exception as e:\n","    IN_COLAB = False\n","    print(\"Not running in Colab (Drive not mounted):\", e)\n","\n","# Always save checkpoints to local runtime (Drive mount can break atomic saves).\n","SAVE_ROOT = Path(\"/content/repro_runs\")\n","SAVE_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","# Optional: sync copies of runs to Drive after training completes.\n","DRIVE_SAVE_ROOT = None\n","if IN_COLAB:\n","    DRIVE_SAVE_ROOT = Path(\"/content/drive/MyDrive/DIFF_2\")\n","    DRIVE_SAVE_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","print(\"Local checkpoint save root:\", SAVE_ROOT)\n","print(\"Drive sync root:\", DRIVE_SAVE_ROOT)"]},{"cell_type":"code","execution_count":5,"id":"58ff9ba0","metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1769471928291,"user":{"displayName":"Sitanium Games","userId":"11646386209711617651"},"user_tz":-120},"id":"58ff9ba0"},"outputs":[],"source":["# Helpers to run Hydra commands and parse val/ppl from output\n","import os\n","import sys\n","import shutil\n","import subprocess\n","import re\n","from pathlib import Path\n","\n","# If a previous cell defined SAVE_ROOT/DRIVE_SAVE_ROOT, keep them; otherwise default to local runtime.\n","if \"SAVE_ROOT\" not in globals():\n","    SAVE_ROOT = Path(os.environ.get(\"BD3LMS_SAVE_ROOT\", \"/content/repro_runs\"))\n","SAVE_ROOT = Path(SAVE_ROOT)\n","SAVE_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","if \"DRIVE_SAVE_ROOT\" not in globals():\n","    DRIVE_SAVE_ROOT = None\n","if DRIVE_SAVE_ROOT is not None:\n","    DRIVE_SAVE_ROOT = Path(DRIVE_SAVE_ROOT)\n","    DRIVE_SAVE_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","def _sync_tree(src: Path, dst: Path):\n","    \"\"\"Best-effort incremental sync from local runtime to Drive.\"\"\"\n","    src = Path(src)\n","    dst = Path(dst)\n","    for p in src.rglob(\"*\"):\n","        rel = p.relative_to(src)\n","        q = dst / rel\n","        if p.is_dir():\n","            q.mkdir(parents=True, exist_ok=True)\n","            continue\n","        if p.is_file():\n","            q.parent.mkdir(parents=True, exist_ok=True)\n","            try:\n","                if q.exists() and q.stat().st_size == p.stat().st_size:\n","                    continue\n","            except OSError:\n","                pass\n","            shutil.copy2(p, q)\n","\n","def sync_run_to_drive(run_dir: Path):\n","    \"\"\"Sync a single run directory to Drive (if mounted).\"\"\"\n","    if DRIVE_SAVE_ROOT is None:\n","        return\n","    run_dir = Path(run_dir)\n","    dst = DRIVE_SAVE_ROOT / run_dir.name\n","    dst.mkdir(parents=True, exist_ok=True)\n","    _sync_tree(run_dir, dst)\n","\n","def run_main(overrides, timeout=None):\n","    \"\"\"Run `python -u main.py ...` and return combined stdout/stderr text.\"\"\"\n","    env = dict(os.environ)\n","    env.setdefault(\"HYDRA_FULL_ERROR\", \"1\")\n","    cmd = [sys.executable, \"-u\", \"bd3lms/main.py\", *overrides]\n","    print(\"\\n$\", \" \".join(cmd))\n","    proc = subprocess.run(\n","        cmd,\n","        stdout=subprocess.PIPE,\n","        stderr=subprocess.STDOUT,\n","        text=True,\n","        timeout=timeout,\n","        check=False,\n","        env=env,\n","    )\n","    print(proc.stdout[-4000:])  # tail for quick visibility\n","    if proc.returncode != 0:\n","        raise RuntimeError(f\"Command failed with return code {proc.returncode}\")\n","    return proc.stdout\n","\n","_METRIC_PATTERNS = [\n","    # Key: value (some loggers print this)\n","    re.compile(r\"val/ppl\\s*[:=]\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n","    re.compile(r\"'val/ppl'\\s*:\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n","\n","    # Lightning \"rich\" table row (note the unicode box character │)\n","    re.compile(r\"val/ppl\\s*[│|]\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n","]\n","\n","def extract_val_ppl(log_text: str):\n","    # First try line-based parse from the end (most reliable for tables)\n","    for line in reversed(log_text.splitlines()):\n","        if \"val/ppl\" in line.lower():\n","            m = re.search(r\"val/ppl.*?([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", line, re.IGNORECASE)\n","            if m:\n","                return float(m.group(1))\n","\n","    # Fallback: scan entire text with known patterns\n","    hits = []\n","    for pat in _METRIC_PATTERNS:\n","        hits.extend(pat.findall(log_text))\n","    return float(hits[-1]) if hits else None\n","\n","def _small_loader_overrides(batch_size=8, num_workers=2):\n","    \"\"\"Overrides needed to avoid huge default batch sizes on Colab.\"\"\"\n","    return [\n","        f\"loader.global_batch_size={batch_size}\",\n","        f\"loader.eval_global_batch_size={batch_size}\",\n","        f\"loader.batch_size={batch_size}\",\n","        f\"loader.eval_batch_size={batch_size}\",\n","        f\"loader.num_workers={num_workers}\",\n","        \"trainer.accumulate_grad_batches=1\",\n","    ]\n","\n","def train_run(\n","    run_name,\n","    algo,\n","    block_size=None,\n","    from_pretrained=None,\n","    resume_ckpt_path=None,\n","    max_steps=500,\n","    extra_overrides=None,\n","    timeout=None,\n","    ):\n","    \"\"\"\n","    Train a model and return the last.ckpt path.\n","\n","    - If `resume_ckpt_path` is set, this uses Lightning resume (`checkpointing.resume_from_ckpt=true`)\n","      and continues training state (optimizer/scheduler/global_step) from that checkpoint.\n","    - Otherwise, if `from_pretrained` is set, this loads weights only (`training.from_pretrained=...`).\n","\n","    Checkpoints are saved under SAVE_ROOT/<run_name>/checkpoints/last.ckpt.\n","    \"\"\"\n","    save_dir = Path(SAVE_ROOT) / run_name\n","    if save_dir.exists():\n","        shutil.rmtree(save_dir)\n","    save_dir.mkdir(parents=True, exist_ok=True)\n","\n","    overrides = [\n","        \"mode=train\",\n","        \"data=lm1b-wrap\",\n","        \"data.cache_dir=/content/bd3lms/data\",\n","        \"data.streaming=true\",\n","        \"data.max_train_samples=5000\",\n","        # For LM1B, validation uses the 'test' split in this codebase\n","        \"model=tiny\",\n","        \"model.length=128\",\n","        \"model.attn_backend=sdpa\",\n","        f\"algo={algo}\",\n","        \"trainer.accelerator=gpu\",\n","        \"trainer.devices=1\",\n","        \"trainer.num_nodes=1\",\n","        \"trainer.precision=16-mixed\",\n","        \"trainer.num_sanity_val_steps=0\",\n","        \"trainer.log_every_n_steps=10\",\n","        \"trainer.val_check_interval=50\",\n","        f\"trainer.max_steps={max_steps}\",\n","        \"data.max_valid_samples=500\",\n","        \"data.max_test_samples=500\",\n","        f\"checkpointing.save_dir={save_dir}\",\n","        \"wandb=null\",\n","    ]\n","    overrides.extend(_small_loader_overrides(batch_size=8, num_workers=2))\n","    if block_size is not None:\n","        overrides.append(f\"block_size={block_size}\")\n","\n","    if resume_ckpt_path is not None:\n","        overrides.append(\"checkpointing.resume_from_ckpt=true\")\n","        overrides.append(f\"checkpointing.resume_ckpt_path={resume_ckpt_path}\")\n","    else:\n","        overrides.append(\"checkpointing.resume_from_ckpt=false\")\n","        if from_pretrained is not None:\n","            overrides.append(f\"training.from_pretrained={from_pretrained}\")\n","\n","    if extra_overrides:\n","        overrides.extend(extra_overrides)\n","\n","    _ = run_main(overrides, timeout=timeout)\n","    ckpt = save_dir / \"checkpoints\" / \"last.ckpt\"\n","    if not ckpt.exists():\n","        raise FileNotFoundError(f\"Expected checkpoint not found: {ckpt}\")\n","\n","    # Best-effort sync to Drive (if available).\n","    sync_run_to_drive(save_dir)\n","    return str(ckpt)\n","\n","def eval_run(algo, checkpoint_path, block_size=None, extra_overrides=None):\n","    \"\"\"Evaluate perplexity (val/ppl) for a given checkpoint.\"\"\"\n","    overrides = [\n","        \"mode=ppl_eval\",\n","        \"data=lm1b-wrap\",\n","        \"data.cache_dir=/content/bd3lms/data\",\n","        \"data.streaming=true\",\n","        # For LM1B, `get_dataloaders` maps validation to the 'test' split\n","        \"data.max_test_samples=1000\",\n","        \"model=tiny\",\n","        \"model.length=128\",\n","        \"model.attn_backend=sdpa\",\n","        f\"algo={algo}\",\n","        f\"eval.checkpoint_path={checkpoint_path}\",\n","        \"trainer.accelerator=gpu\",\n","        \"trainer.devices=1\",\n","        \"trainer.num_nodes=1\",\n","        \"trainer.precision=16-mixed\",\n","        \"trainer.num_sanity_val_steps=0\",\n","        \"wandb=null\",\n","        \"data.max_valid_samples=100\",\n","        \"data.max_test_samples=100\",\n","    ]\n","\n","    overrides.extend(_small_loader_overrides(batch_size=8, num_workers=2))\n","    if block_size is not None:\n","        overrides.append(f\"block_size={block_size}\")\n","    if extra_overrides:\n","        overrides.extend(extra_overrides)\n","\n","    log_text = run_main(overrides)\n","    ppl = extract_val_ppl(log_text)\n","    if ppl is None:\n","        raise ValueError(\"Could not parse val/ppl from output. Try increasing the log tail or printing full logs.\")\n","    return ppl"]},{"cell_type":"markdown","id":"90af12d4","metadata":{"id":"90af12d4"},"source":["## 3) Run experiments and build a mini Table 3\n","This now mirrors the paper’s training procedure, but at tiny scale:\n","- Train a base BD3-LM at L'=L (here 128) once.\n","- Finetune that base for block sizes L' in {16, 8, 4} with noise-schedule-style resampling enabled.\n","- Also run AR, SEDD, and MDLM tiny baselines.\n","Only the scale (steps, samples, model size) is reduced for Colab feasibility."]},{"cell_type":"code","execution_count":6,"id":"0d085996","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":843045,"status":"error","timestamp":1769472771341,"user":{"displayName":"Sitanium Games","userId":"11646386209711617651"},"user_tz":-120},"id":"0d085996","outputId":"3015c6fc-b112-4465-eeaa-17fe83d04939"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=ar trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=500 data.max_valid_samples=500 data.max_test_samples=500 checkpointing.save_dir=/content/repro_runs/ar_tiny_len128 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 checkpointing.resume_from_ckpt=false\n","0% of the batches per epoch will be used..\n","`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n","Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","----------------------------------------------------------------------------------------------------\n","distributed_backend=nccl\n","All distributed processes registered. Starting with 1 processes\n","----------------------------------------------------------------------------------------------------\n","\n","2026-01-26 23:59:35.746033: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1769471975.777669    1514 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1769471975.787335    1514 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1769471975.810687    1514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769471975.810713    1514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769471975.810717    1514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769471975.810719    1514 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2026-01-26 23:59:35.817883: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name     | Type           | Params | Mode \n","----------------------------------------------------\n","0 | backbone | DIT            | 22.0 M | train\n","1 | noise    | LogLinearNoise | 0      | train\n","----------------------------------------------------\n","22.0 M    Trainable params\n","0         Non-trainable params\n","22.0 M    Total params\n","87.855    Total estimated model params size (MB)\n","96        Modules in train mode\n","0         Modules in eval mode\n","/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n","  warnings.warn(  # warn only once\n","Epoch 0, global step 50: 'val/nll' reached 10.29777 (best 10.29777), saving model to '/content/repro_runs/ar_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 0, global step 100: 'val/nll' reached 10.18571 (best 10.18571), saving model to '/content/repro_runs/ar_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 1, global step 196: 'val/nll' reached 9.79458 (best 9.79458), saving model to '/content/repro_runs/ar_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 1, global step 246: 'val/nll' reached 9.53195 (best 9.53195), saving model to '/content/repro_runs/ar_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 2, global step 342: 'val/nll' reached 8.95977 (best 8.95977), saving model to '/content/repro_runs/ar_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 2, global step 392: 'val/nll' reached 8.63722 (best 8.63722), saving model to '/content/repro_runs/ar_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 3, global step 488: 'val/nll' reached 8.01649 (best 8.01649), saving model to '/content/repro_runs/ar_tiny_len128/checkpoints/best.ckpt' as top 1\n","`Trainer.fit` stopped: `max_steps=500` reached.\n","\n","\n","$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=ar eval.checkpoint_path=/content/repro_runs/ar_tiny_len128/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null data.max_valid_samples=100 data.max_test_samples=100 loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1\n","            \n","        w_type: simple                                                          \n","                                                                                \n","[2026-01-27 00:01:46,398][__main__][INFO] - Starting Eval.\n","Loading checkpoint at 500\n","Using 16bit Automatic Mixed Precision (AMP)\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n","`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n","`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n","Seed set to 1\n","[2026-01-27 00:01:49,111][dataloader][INFO] - Generating new data at: /content/bd3lms/data/jdeschena/lm1b_test_bs128_wrapped.dat\n","[2026-01-27 00:01:49,111][dataloader][INFO] - streaming=True\n","\n","Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 100/100 [00:00<00:00, 1399.67 examples/s]\n","\n","Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 100/100 [00:00<00:00, 11998.12 examples/s]\n","Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","----------------------------------------------------------------------------------------------------\n","distributed_backend=nccl\n","All distributed processes registered. Starting with 1 processes\n","----------------------------------------------------------------------------------------------------\n","\n","2026-01-27 00:01:57.223674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1769472117.246846    2397 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1769472117.253892    2397 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1769472117.271997    2397 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472117.272018    2397 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472117.272021    2397 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472117.272023    2397 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2026-01-27 00:01:57.278627: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n","/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n","  warnings.warn(  # warn only once\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃      Validate metric      ┃       DataLoader 0        ┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│          val/bpd          │    11.507224082946777     │\n","│          val/nll          │     7.976200103759766     │\n","│          val/ppl          │     2910.84912109375      │\n","└───────────────────────────┴───────────────────────────┘\n","\n","\n","$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=sedd trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=500 data.max_valid_samples=500 data.max_test_samples=500 checkpointing.save_dir=/content/repro_runs/sedd_tiny_len128 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 checkpointing.resume_from_ckpt=false training.resample=false algo.var_min=false algo.clip_search_widths=[]\n","hes per epoch will be used..\n","`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n","Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","----------------------------------------------------------------------------------------------------\n","distributed_backend=nccl\n","All distributed processes registered. Starting with 1 processes\n","----------------------------------------------------------------------------------------------------\n","\n","2026-01-27 00:02:39.660465: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1769472159.695312    2617 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1769472159.705283    2617 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1769472159.728939    2617 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472159.728985    2617 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472159.728993    2617 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472159.729000    2617 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2026-01-27 00:02:39.735899: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name     | Type           | Params | Mode \n","----------------------------------------------------\n","0 | backbone | DIT            | 22.8 M | train\n","1 | noise    | LogLinearNoise | 0      | train\n","----------------------------------------------------\n","22.8 M    Trainable params\n","0         Non-trainable params\n","22.8 M    Total params\n","91.266    Total estimated model params size (MB)\n","110       Modules in train mode\n","0         Modules in eval mode\n","/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n","  warnings.warn(  # warn only once\n","Epoch 0, global step 50: 'val/nll' reached 10.30917 (best 10.30917), saving model to '/content/repro_runs/sedd_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 0, global step 100: 'val/nll' reached 10.15049 (best 10.15049), saving model to '/content/repro_runs/sedd_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 1, global step 196: 'val/nll' reached 9.98554 (best 9.98554), saving model to '/content/repro_runs/sedd_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 1, global step 246: 'val/nll' reached 9.22939 (best 9.22939), saving model to '/content/repro_runs/sedd_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 2, global step 342: 'val/nll' reached 8.56776 (best 8.56776), saving model to '/content/repro_runs/sedd_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 2, global step 392: 'val/nll' reached 7.31594 (best 7.31594), saving model to '/content/repro_runs/sedd_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 3, global step 488: 'val/nll' reached 7.21938 (best 7.21938), saving model to '/content/repro_runs/sedd_tiny_len128/checkpoints/best.ckpt' as top 1\n","`Trainer.fit` stopped: `max_steps=500` reached.\n","\n","\n","$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=sedd eval.checkpoint_path=/content/repro_runs/sedd_tiny_len128/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null data.max_valid_samples=100 data.max_test_samples=100 loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 algo.var_min=false\n","            \n","        w_type: simple                                                          \n","                                                                                \n","[2026-01-27 00:05:06,670][__main__][INFO] - Starting Eval.\n","Loading checkpoint at 500\n","Using 16bit Automatic Mixed Precision (AMP)\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n","`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n","`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n","Seed set to 1\n","[2026-01-27 00:05:08,383][dataloader][INFO] - Generating new data at: /content/bd3lms/data/jdeschena/lm1b_test_bs128_wrapped.dat\n","[2026-01-27 00:05:08,384][dataloader][INFO] - streaming=True\n","\n","Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 100/100 [00:00<00:00, 2274.83 examples/s]\n","\n","Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 100/100 [00:00<00:00, 21285.48 examples/s]\n","Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","----------------------------------------------------------------------------------------------------\n","distributed_backend=nccl\n","All distributed processes registered. Starting with 1 processes\n","----------------------------------------------------------------------------------------------------\n","\n","2026-01-27 00:05:16.537415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1769472316.555486    3449 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1769472316.560346    3449 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1769472316.573105    3449 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472316.573126    3449 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472316.573128    3449 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472316.573130    3449 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2026-01-27 00:05:16.576924: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n","/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n","  warnings.warn(  # warn only once\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃      Validate metric      ┃       DataLoader 0        ┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│          val/bpd          │    10.670113563537598     │\n","│          val/nll          │     7.395959377288818     │\n","│          val/ppl          │     1629.387451171875     │\n","└───────────────────────────┴───────────────────────────┘\n","\n","\n","$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=mdlm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=500 data.max_valid_samples=500 data.max_test_samples=500 checkpointing.save_dir=/content/repro_runs/mdlm_tiny_len128 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 checkpointing.resume_from_ckpt=false training.resample=false algo.var_min=false algo.clip_search_widths=[]\n","available: False, using: 0 HPUs\n","`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n","`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n","Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","----------------------------------------------------------------------------------------------------\n","distributed_backend=nccl\n","All distributed processes registered. Starting with 1 processes\n","----------------------------------------------------------------------------------------------------\n","\n","2026-01-27 00:05:58.889920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1769472358.909475    3668 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1769472358.915298    3668 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1769472358.929972    3668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472358.929996    3668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472358.929999    3668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472358.930001    3668 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2026-01-27 00:05:58.934457: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name     | Type           | Params | Mode \n","----------------------------------------------------\n","0 | backbone | DIT            | 22.8 M | train\n","1 | noise    | LogLinearNoise | 0      | train\n","----------------------------------------------------\n","22.8 M    Trainable params\n","0         Non-trainable params\n","22.8 M    Total params\n","91.266    Total estimated model params size (MB)\n","110       Modules in train mode\n","0         Modules in eval mode\n","/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n","  warnings.warn(  # warn only once\n","Epoch 0, global step 50: 'val/nll' reached 10.30882 (best 10.30882), saving model to '/content/repro_runs/mdlm_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 0, global step 100: 'val/nll' reached 10.14439 (best 10.14439), saving model to '/content/repro_runs/mdlm_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 1, global step 196: 'val/nll' reached 9.90903 (best 9.90903), saving model to '/content/repro_runs/mdlm_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 1, global step 246: 'val/nll' reached 9.07911 (best 9.07911), saving model to '/content/repro_runs/mdlm_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 2, global step 342: 'val/nll' reached 8.12496 (best 8.12496), saving model to '/content/repro_runs/mdlm_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 2, global step 392: 'val/nll' reached 7.05614 (best 7.05614), saving model to '/content/repro_runs/mdlm_tiny_len128/checkpoints/best.ckpt' as top 1\n","Epoch 3, global step 488: 'val/nll' was not in top 1\n","`Trainer.fit` stopped: `max_steps=500` reached.\n","\n","\n","$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=mdlm eval.checkpoint_path=/content/repro_runs/mdlm_tiny_len128/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null data.max_valid_samples=100 data.max_test_samples=100 loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 algo.var_min=false\n","            \n","        w_type: simple                                                          \n","                                                                                \n","[2026-01-27 00:08:14,052][__main__][INFO] - Starting Eval.\n","Loading checkpoint at 500\n","Using 16bit Automatic Mixed Precision (AMP)\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n","`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n","`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n","Seed set to 1\n","[2026-01-27 00:08:16,953][dataloader][INFO] - Generating new data at: /content/bd3lms/data/jdeschena/lm1b_test_bs128_wrapped.dat\n","[2026-01-27 00:08:16,953][dataloader][INFO] - streaming=True\n","\n","Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 100/100 [00:00<00:00, 2140.78 examples/s]\n","\n","Map:   0%|          | 0/100 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 100/100 [00:00<00:00, 21307.11 examples/s]\n","Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","----------------------------------------------------------------------------------------------------\n","distributed_backend=nccl\n","All distributed processes registered. Starting with 1 processes\n","----------------------------------------------------------------------------------------------------\n","\n","2026-01-27 00:08:25.663934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1769472505.680463    4454 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1769472505.685372    4454 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1769472505.697901    4454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472505.697921    4454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472505.697924    4454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472505.697926    4454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2026-01-27 00:08:25.701635: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n","/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n","  warnings.warn(  # warn only once\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃      Validate metric      ┃       DataLoader 0        ┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│          val/bpd          │    10.610881805419922     │\n","│          val/nll          │     7.354903221130371     │\n","│          val/ppl          │      1563.845703125       │\n","└───────────────────────────┴───────────────────────────┘\n","\n","\n","$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=500 data.max_valid_samples=500 data.max_test_samples=500 checkpointing.save_dir=/content/repro_runs/bd3lm_base_len128 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=128 checkpointing.resume_from_ckpt=false training.resample=false algo.var_min=false algo.clip_search_widths=[] training.sampling_eps_min=1e-3 training.sampling_eps_max=0.5\n"," epoch will be used..\n","`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n","Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","----------------------------------------------------------------------------------------------------\n","distributed_backend=nccl\n","All distributed processes registered. Starting with 1 processes\n","----------------------------------------------------------------------------------------------------\n","\n","2026-01-27 00:09:06.704148: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1769472546.723351    4669 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1769472546.729292    4669 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1769472546.745545    4669 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472546.745574    4669 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472546.745577    4669 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472546.745579    4669 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2026-01-27 00:09:06.752582: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name     | Type           | Params | Mode \n","----------------------------------------------------\n","0 | backbone | DIT            | 22.8 M | train\n","1 | noise    | LogLinearNoise | 0      | train\n","----------------------------------------------------\n","22.8 M    Trainable params\n","0         Non-trainable params\n","22.8 M    Total params\n","91.266    Total estimated model params size (MB)\n","110       Modules in train mode\n","0         Modules in eval mode\n","/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n","  warnings.warn(  # warn only once\n","Epoch 0, global step 50: 'val/nll' reached 10.66869 (best 10.66869), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n","Epoch 0, global step 100: 'val/nll' reached 10.20723 (best 10.20723), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n","Epoch 1, global step 196: 'val/nll' reached 9.86683 (best 9.86683), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n","Epoch 1, global step 246: 'val/nll' reached 9.61278 (best 9.61278), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n","Epoch 2, global step 342: 'val/nll' reached 8.18769 (best 8.18769), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n","Epoch 2, global step 392: 'val/nll' reached 7.48070 (best 7.48070), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n","Epoch 3, global step 488: 'val/nll' reached 7.32605 (best 7.32605), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n","`Trainer.fit` stopped: `max_steps=500` reached.\n","\n","\n","$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=600 data.max_valid_samples=500 data.max_test_samples=500 checkpointing.save_dir=/content/repro_runs/bd3lm_finetune_Lp16 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=16 checkpointing.resume_from_ckpt=true checkpointing.resume_ckpt_path=/content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt training.resample=true algo.var_min=false algo.clip_search_widths=[] training.sampling_eps_min=0.3 training.sampling_eps_max=0.5\n","n_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n","`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n","Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n","----------------------------------------------------------------------------------------------------\n","distributed_backend=nccl\n","All distributed processes registered. Starting with 1 processes\n","----------------------------------------------------------------------------------------------------\n","\n","2026-01-27 00:12:13.443498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1769472733.465087    5599 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1769472733.471454    5599 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1769472733.489556    5599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472733.489583    5599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472733.489586    5599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769472733.489588    5599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2026-01-27 00:12:13.494489: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Restoring states from the checkpoint path at /content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt\n","Loading checkpoint at 500\n","/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from '/content/repro_runs/bd3lm_base_len128/checkpoints' to '/content/repro_runs/bd3lm_finetune_Lp16/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name     | Type           | Params | Mode \n","----------------------------------------------------\n","0 | backbone | DIT            | 22.8 M | train\n","1 | noise    | LogLinearNoise | 0      | train\n","----------------------------------------------------\n","22.8 M    Trainable params\n","0         Non-trainable params\n","22.8 M    Total params\n","91.266    Total estimated model params size (MB)\n","110       Modules in train mode\n","0         Modules in eval mode\n","Restored all states from the checkpoint at /content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt\n","/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n","  warnings.warn(  # warn only once\n","/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/training_epoch_loop.py:221: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.\n","`Trainer.fit` stopped: `max_steps=600` reached.\n","\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"Expected checkpoint not found: /content/repro_runs/bd3lm_finetune_Lp16/checkpoints/last.ckpt","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3947221594.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;34m\"training.sampling_eps_max=1.0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         ])\n\u001b[0;32m---> 68\u001b[0;31m     finetune_ckpt = train_run(\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mfinetune_run\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bd3lm\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1083592083.py\u001b[0m in \u001b[0;36mtrain_run\u001b[0;34m(run_name, algo, block_size, from_pretrained, resume_ckpt_path, max_steps, extra_overrides, timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"checkpoints\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"last.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected checkpoint not found: {ckpt}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Best-effort sync to Drive (if available).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Expected checkpoint not found: /content/repro_runs/bd3lm_finetune_Lp16/checkpoints/last.ckpt"]}],"source":["from pathlib import Path\n","import os\n","import shutil\n","import sys\n","# import pandas as pd\n","\n","results = []\n","\n","# 1) Autoregressive baseline (tiny, scratch)\n","ar_run = \"ar_tiny_len128\"\n","ar_ckpt = train_run(ar_run, algo=\"ar\")\n","ar_ppl = eval_run(algo=\"ar\", checkpoint_path=ar_ckpt)\n","results.append({\"model\": \"Autoregressive\", \"block_size_Lprime\": \"-\", \"val_ppl\": ar_ppl})\n","\n","# 2) Diffusion baselines (tiny, scratch): SEDD + MDLM\n","for algo_name, display_name in [(\"sedd\", \"SEDD\"), (\"mdlm\", \"MDLM\")]:\n","    run_name = f\"{algo_name}_tiny_len128\"\n","    ckpt = train_run(\n","        run_name,\n","        algo=algo_name,\n","        extra_overrides=[\n","            \"training.resample=false\",\n","            \"algo.var_min=false\",\n","            \"algo.clip_search_widths=[]\",\n","        ],\n","    )\n","    ppl = eval_run(\n","        algo=algo_name,\n","        checkpoint_path=ckpt,\n","        extra_overrides=[\n","            \"algo.var_min=false\",\n","        ],\n","    )\n","    results.append({\"model\": display_name, \"block_size_Lprime\": \"-\", \"val_ppl\": ppl})\n","\n","# 3) BD3LM methodology match (small-scale):\n","bd3lm_base_run = \"bd3lm_base_len128\"\n","bd3lm_base_ckpt = train_run(\n","    bd3lm_base_run,\n","    algo=\"bd3lm\",\n","    block_size=128,\n","    extra_overrides=[\n","        \"training.resample=false\",\n","        \"algo.var_min=false\",\n","        \"algo.clip_search_widths=[]\",\n","        \"training.sampling_eps_min=1e-3\",\n","        \"training.sampling_eps_max=0.5\",\n","    ],\n",")\n","\n","for Lprime in [16, 8, 4]:\n","    finetune_run = f\"bd3lm_finetune_Lp{Lprime}\"\n","    finetune_overrides = [\n","        \"training.resample=true\",\n","        \"algo.var_min=false\",\n","        \"algo.clip_search_widths=[]\",\n","    ]\n","    if Lprime == 16:\n","        finetune_overrides.extend([\n","            \"training.sampling_eps_min=0.3\",\n","            \"training.sampling_eps_max=0.5\",\n","        ])\n","    elif Lprime == 4:\n","        finetune_overrides.extend([\n","            \"training.sampling_eps_min=0.5\",\n","            \"training.sampling_eps_max=1.0\",\n","        ])\n","    finetune_ckpt = train_run(\n","        finetune_run,\n","        algo=\"bd3lm\",\n","        block_size=Lprime,\n","        max_steps=600,\n","        resume_ckpt_path=bd3lm_base_ckpt,\n","        extra_overrides=finetune_overrides,\n","    )\n","    ppl = eval_run(\n","        algo=\"bd3lm\",\n","        checkpoint_path=finetune_ckpt,\n","        block_size=Lprime,\n","        extra_overrides=[\n","            \"algo.var_min=false\",\n","        ],\n","    )\n","    results.append({\"model\": \"Block diffusion (BD3LM)\", \"block_size_Lprime\": Lprime, \"val_ppl\": ppl})"]},{"cell_type":"code","execution_count":null,"id":"61037b8b","metadata":{"executionInfo":{"elapsed":918825,"status":"aborted","timestamp":1769472771302,"user":{"displayName":"Sitanium Games","userId":"11646386209711617651"},"user_tz":-120},"id":"61037b8b"},"outputs":[],"source":["def print_table(rows):\n","    if not rows:\n","        print(\"No data to display.\")\n","        return\n","\n","    columns = list(rows[0].keys())\n","\n","    str_rows = [\n","        {col: str(row.get(col, \"\")) for col in columns}\n","        for row in rows\n","    ]\n","\n","    widths = {\n","        col: max(len(col), max(len(row[col]) for row in str_rows))\n","        for col in columns\n","    }\n","\n","    def print_separator():\n","        print(\"+\" + \"+\".join(\"-\" * (widths[col] + 2) for col in columns) + \"+\")\n","\n","    def print_row(row):\n","        print(\n","            \"| \" +\n","            \" | \".join(row[col].ljust(widths[col]) for col in columns) +\n","            \" |\"\n","        )\n","\n","    # Print table\n","    print_separator()\n","    print_row({col: col for col in columns})\n","    print_separator()\n","    for row in str_rows:\n","        print_row(row)\n","    print_separator()\n","\n","print_table(results)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}