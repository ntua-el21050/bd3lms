{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f834b118",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1116,
     "status": "ok",
     "timestamp": 1767735663184,
     "user": {
      "displayName": "Ηλίας Μάκρας",
      "userId": "16381513743156120405"
     },
     "user_tz": -120
    },
    "id": "f834b118",
    "outputId": "3a7d40b3-dd68-45f2-cc2d-2e195277c8aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bd3lms'...\n",
      "remote: Enumerating objects: 759, done.\u001b[K\n",
      "remote: Counting objects: 100% (222/222), done.\u001b[K\n",
      "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
      "remote: Total 759 (delta 196), reused 174 (delta 171), pack-reused 537 (from 2)\u001b[K\n",
      "Receiving objects: 100% (759/759), 1.75 MiB | 5.62 MiB/s, done.\n",
      "Resolving deltas: 100% (489/489), done.\n"
     ]
    }
   ],
   "source": [
    "!cd /content && git clone https://github.com/ntua-el21050/bd3lms.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "XMUalUkud3_x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1767735663300,
     "user": {
      "displayName": "Ηλίας Μάκρας",
      "userId": "16381513743156120405"
     },
     "user_tz": -120
    },
    "id": "XMUalUkud3_x",
    "outputId": "622565e9-a7b8-4cd5-b312-17e2cd81fd99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1740\n",
      "-rw-r--r-- 1 root root 862037 Jan  6 21:41 2503.09573v3.pdf\n",
      "drwxr-xr-x 9 root root   4096 Jan  6 21:41 configs\n",
      "-rw-r--r-- 1 root root  33535 Jan  6 21:41 dataloader.py\n",
      "-rw-r--r-- 1 root root  44163 Jan  6 21:41 diffusion.py\n",
      "-rw-r--r-- 1 root root 225205 Jan  6 21:41 graphical_abstract.png\n",
      "-rw-r--r-- 1 root root  11357 Jan  6 21:41 LICENSE\n",
      "-rw-r--r-- 1 root root   7873 Jan  6 21:41 main.py\n",
      "-rw-r--r-- 1 root root   8405 Jan  6 21:41 metrics.py\n",
      "drwxr-xr-x 3 root root   4096 Jan  6 21:41 models\n",
      "-rw-r--r-- 1 root root   2538 Jan  6 21:41 noise_schedule.py\n",
      "-rw-r--r-- 1 root root   1449 Jan  6 21:41 push_to_hf.py\n",
      "-rw-r--r-- 1 root root  10070 Jan  6 21:41 README.md\n",
      "-rw-r--r-- 1 root root    363 Jan  6 21:41 requirements.txt\n",
      "drwxr-xr-x 7 root root   4096 Jan  6 21:41 scripts\n",
      "drwxr-xr-x 4 root root   4096 Jan  6 21:41 ssd-lm\n",
      "-rw-r--r-- 1 root root 525005 Jan  6 21:41 table2_final.ipynb\n",
      "-rw-r--r-- 1 root root   7162 Jan  6 21:41 utils.py\n"
     ]
    }
   ],
   "source": [
    "!ls -l /content/bd3lms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "iDMyFkBed9i1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36684,
     "status": "ok",
     "timestamp": 1767735699990,
     "user": {
      "displayName": "Ηλίας Μάκρας",
      "userId": "16381513743156120405"
     },
     "user_tz": -120
    },
    "id": "iDMyFkBed9i1",
    "outputId": "2bb6db0a-bd6c-4d88-bf38-81afe29d431a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m129.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.1 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\n",
      "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
      "xarray 2025.12.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
      "db-dtypes 1.4.4 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\n",
      "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q \\\n",
    "    torchmetrics==1.6.2 \\\n",
    "    datasets==3.3.2 \\\n",
    "    einops==0.8.1 \\\n",
    "    fsspec==2024.2.0 \\\n",
    "    hydra-core==1.3.2 \\\n",
    "    lightning==2.5.0.post0 \\\n",
    "    omegaconf==2.3.0 \\\n",
    "    packaging==23.2 \\\n",
    "    pandas==2.2.1 \\\n",
    "    rich==13.7.1 \\\n",
    "    scikit-learn==1.5.1 \\\n",
    "    timm==0.9.16 \\\n",
    "    transformers==4.49.0 \\\n",
    "    matplotlib==3.10.0 \\\n",
    "    wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff9ba0",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1767735700014,
     "user": {
      "displayName": "Ηλίας Μάκρας",
      "userId": "16381513743156120405"
     },
     "user_tz": -120
    },
    "id": "58ff9ba0"
   },
   "outputs": [],
   "source": [
    "# Helpers to run Hydra commands and parse val/ppl from output\n",
    "import subprocess\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def run_main(overrides, timeout=None):\n",
    "    \"\"\"Run `python -u main.py ...` and return combined stdout/stderr text.\"\"\"\n",
    "    env = dict(os.environ)\n",
    "    env.setdefault(\"HYDRA_FULL_ERROR\", \"1\")\n",
    "    cmd = [sys.executable, \"-u\", \"bd3lms/main.py\", *overrides]\n",
    "    print(\"\\n$\", \" \".join(cmd))\n",
    "    proc = subprocess.run(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        timeout=timeout,\n",
    "        check=False,\n",
    "        env=env,\n",
    "    )\n",
    "    print(proc.stdout[-4000:])  # tail for quick visibility\n",
    "    if proc.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed with return code {proc.returncode}\")\n",
    "    return proc.stdout\n",
    "\n",
    "_METRIC_PATTERNS = [\n",
    "    # Key: value (some loggers print this)\n",
    "    re.compile(r\"val/ppl\\s*[:=]\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
    "    re.compile(r\"'val/ppl'\\s*:\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
    "\n",
    "    # Lightning \"rich\" table row (note the unicode box character │)\n",
    "    re.compile(r\"val/ppl\\s*[│|]\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
    "]\n",
    "\n",
    "def extract_val_ppl(log_text: str):\n",
    "    # First try line-based parse from the end (most reliable for tables)\n",
    "    for line in reversed(log_text.splitlines()):\n",
    "        if \"val/ppl\" in line.lower():\n",
    "            m = re.search(r\"val/ppl.*?([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", line, re.IGNORECASE)\n",
    "            if m:\n",
    "                return float(m.group(1))\n",
    "\n",
    "    # Fallback: scan entire text with known patterns\n",
    "    hits = []\n",
    "    for pat in _METRIC_PATTERNS:\n",
    "        hits.extend(pat.findall(log_text))\n",
    "    return float(hits[-1]) if hits else None\n",
    "\n",
    "def _small_loader_overrides(batch_size=8, num_workers=2):\n",
    "    \"\"\"Overrides needed to avoid huge default batch sizes on Colab.\"\"\"\n",
    "    return [\n",
    "        f\"loader.global_batch_size={batch_size}\",\n",
    "        f\"loader.eval_global_batch_size={batch_size}\",\n",
    "        f\"loader.batch_size={batch_size}\",\n",
    "        f\"loader.eval_batch_size={batch_size}\",\n",
    "        f\"loader.num_workers={num_workers}\",\n",
    "        \"trainer.accumulate_grad_batches=1\",\n",
    "    ]\n",
    "\n",
    "def train_run(run_name, algo, block_size=None, from_pretrained=None, max_steps=800, extra_overrides=None):\n",
    "    \"\"\"Train a model (optionally from a base checkpoint) and return the last.ckpt path.\"\"\"\n",
    "    save_dir = Path(\"/content/repro_runs\") / run_name\n",
    "    if save_dir.exists():\n",
    "        shutil.rmtree(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    overrides = [\n",
    "        \"mode=train\",\n",
    "        \"data=lm1b-wrap\",\n",
    "        \"data.cache_dir=/content/bd3lms/data\",\n",
    "        \"data.streaming=true\",\n",
    "        \"data.max_train_samples=5000\",\n",
    "        # For LM1B, validation uses the 'test' split in this codebase\n",
    "        \"model=tiny\",\n",
    "        \"model.length=128\",\n",
    "        \"model.attn_backend=sdpa\",\n",
    "        f\"algo={algo}\",\n",
    "        \"trainer.accelerator=gpu\",\n",
    "        \"trainer.devices=1\",\n",
    "        \"trainer.num_nodes=1\",\n",
    "        \"trainer.precision=16-mixed\",\n",
    "        \"trainer.num_sanity_val_steps=0\",\n",
    "        \"trainer.log_every_n_steps=10\",\n",
    "        \"trainer.val_check_interval=50\",\n",
    "        f\"trainer.max_steps={max_steps}\",\n",
    "        \"data.max_valid_samples=100\",\n",
    "        \"data.max_test_samples=100\",\n",
    "        f\"checkpointing.save_dir=/content/repro_runs/{run_name}\",\n",
    "        \"checkpointing.resume_from_ckpt=false\",\n",
    "        \"wandb=null\",\n",
    "    ]\n",
    "    overrides.extend(_small_loader_overrides(batch_size=8, num_workers=2))\n",
    "    if block_size is not None:\n",
    "        overrides.append(f\"block_size={block_size}\")\n",
    "    if from_pretrained is not None:\n",
    "        overrides.append(f\"training.from_pretrained={from_pretrained}\")\n",
    "    if extra_overrides:\n",
    "        overrides.extend(extra_overrides)\n",
    "\n",
    "    _ = run_main(overrides)\n",
    "    ckpt = save_dir / \"checkpoints\" / \"last.ckpt\"\n",
    "    if not ckpt.exists():\n",
    "        raise FileNotFoundError(f\"Expected checkpoint not found: {ckpt}\")\n",
    "    return str(ckpt)\n",
    "\n",
    "def eval_run(algo, checkpoint_path, block_size=None, extra_overrides=None):\n",
    "    \"\"\"Evaluate perplexity (val/ppl) for a given checkpoint.\"\"\"\n",
    "    overrides = [\n",
    "        \"mode=ppl_eval\",\n",
    "        \"data=lm1b-wrap\",\n",
    "        \"data.cache_dir=/content/bd3lms/data\",\n",
    "        \"data.streaming=true\",\n",
    "        # For LM1B, `get_dataloaders` maps validation to the 'test' split\n",
    "        \"data.max_test_samples=1000\",\n",
    "        \"model=tiny\",\n",
    "        \"model.length=128\",\n",
    "        \"model.attn_backend=sdpa\",\n",
    "        f\"algo={algo}\",\n",
    "        f\"eval.checkpoint_path={checkpoint_path}\",\n",
    "        \"trainer.accelerator=gpu\",\n",
    "        \"trainer.devices=1\",\n",
    "        \"trainer.num_nodes=1\",\n",
    "        \"trainer.precision=16-mixed\",\n",
    "        \"trainer.num_sanity_val_steps=0\",\n",
    "        \"wandb=null\",\n",
    "    ]\n",
    "    overrides.extend(_small_loader_overrides(batch_size=8, num_workers=2))\n",
    "    if block_size is not None:\n",
    "        overrides.append(f\"block_size={block_size}\")\n",
    "    if extra_overrides:\n",
    "        overrides.extend(extra_overrides)\n",
    "\n",
    "    log_text = run_main(overrides)\n",
    "    ppl = extract_val_ppl(log_text)\n",
    "    if ppl is None:\n",
    "        raise ValueError(\"Could not parse val/ppl from output. Try increasing the log tail or printing full logs.\")\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af12d4",
   "metadata": {
    "id": "90af12d4"
   },
   "source": [
    "## 3) Run experiments and build a mini Table 3\n",
    "This now mirrors the paper’s training procedure, but at tiny scale:\n",
    "- Train a base BD3-LM at L'=L (here 128) once.\n",
    "- Finetune that base for block sizes L' in {16, 8, 4} with noise-schedule-style resampling enabled.\n",
    "- Also run AR, SEDD, and MDLM tiny baselines.\n",
    "Only the scale (steps, samples, model size) is reduced for Colab feasibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d085996",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4683090,
     "status": "ok",
     "timestamp": 1767740383111,
     "user": {
      "displayName": "Ηλίας Μάκρας",
      "userId": "16381513743156120405"
     },
     "user_tz": -120
    },
    "id": "0d085996",
    "outputId": "2af14423-97b2-4aab-b85d-09b4d68a037c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=ar trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/ar_tiny_len128 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1\n",
      "|████████▏ | 120/146 [01:53<00:24,  1.05it/s, v_num=0]\n",
      "Epoch 3:  96%|█████████▌| 140/146 [01:55<00:04,  1.22it/s, v_num=0]\n",
      "Epoch 3:  96%|█████████▌| 140/146 [01:55<00:04,  1.22it/s, v_num=0]\n",
      "Epoch 3: 100%|██████████| 146/146 [01:55<00:00,  1.26it/s, v_num=0]\n",
      "Epoch 3: 100%|██████████| 146/146 [01:55<00:00,  1.26it/s, v_num=0]\n",
      "Epoch 3: 100%|██████████| 146/146 [01:55<00:00,  1.26it/s, v_num=0]\n",
      "Epoch 3:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
      "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
      "Epoch 4:  14%|█▎        | 20/146 [00:01<00:08, 14.89it/s, v_num=0]\n",
      "Epoch 4:  14%|█▎        | 20/146 [00:01<00:08, 14.88it/s, v_num=0]\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:02<00:07, 14.95it/s, v_num=0]\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:02<00:07, 14.95it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 49.05it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:03<00:09, 11.28it/s, v_num=0]Epoch 4, global step 634: 'val/nll' reached 7.27593 (best 7.27593), saving model to '/content/repro_runs/ar_tiny_len128/checkpoints/best.ckpt' as top 1\n",
      "\n",
      "Epoch 4:  41%|████      | 60/146 [00:11<00:16,  5.21it/s, v_num=0]\n",
      "Epoch 4:  41%|████      | 60/146 [00:11<00:16,  5.21it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [00:12<00:10,  6.22it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [00:12<00:10,  6.22it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:14<00:06,  7.04it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:14<00:06,  7.04it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 48.83it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:14<00:06,  6.94it/s, v_num=0]Epoch 4, global step 684: 'val/nll' reached 7.18295 (best 7.18295), saving model to '/content/repro_runs/ar_tiny_len128/checkpoints/best.ckpt' as top 1\n",
      "\n",
      "Epoch 4:  82%|████████▏ | 120/146 [00:24<00:05,  4.97it/s, v_num=0]\n",
      "Epoch 4:  82%|████████▏ | 120/146 [00:24<00:05,  4.97it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [00:25<00:01,  5.50it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [00:25<00:01,  5.50it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [00:25<00:00,  5.64it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [00:25<00:00,  5.64it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [00:25<00:00,  5.64it/s, v_num=0]\n",
      "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
      "Epoch 5:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:01<00:08, 14.63it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:01<00:08, 14.62it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:02<00:07, 14.76it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:02<00:07, 14.76it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 49.03it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:03<00:09, 11.13it/s, v_num=0]Epoch 5, global step 780: 'val/nll' reached 7.10079 (best 7.10079), saving model to '/content/repro_runs/ar_tiny_len128/checkpoints/best.ckpt' as top 1\n",
      "\n",
      "Epoch 5:  41%|████      | 60/146 [00:58<01:23,  1.03it/s, v_num=0]\n",
      "Epoch 5:  41%|████      | 60/146 [00:58<01:23,  1.03it/s, v_num=0]\n",
      "Epoch 5:  41%|████      | 60/146 [00:59<01:24,  1.02it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
      "\n",
      "Epoch 5:  41%|████      | 60/146 [00:59<01:24,  1.02it/s, v_num=0]\n",
      "\n",
      "\n",
      "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=ar eval.checkpoint_path=/content/repro_runs/ar_tiny_len128/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1\n",
      "ed.dat\n",
      "[2026-01-06 21:50:07,668][dataloader][INFO] - streaming=True\n",
      "\n",
      "============================================================\n",
      "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
      "  Requested: 1000 samples\n",
      "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
      "  Returning dataset with 1000 samples\n",
      "============================================================\n",
      "\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1358.37 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1354.56 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 20085.16 examples/s]\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2026-01-06 21:51:32.872294: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767736292.902149    2745 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767736292.913120    2745 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767736292.934172    2745 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767736292.934196    2745 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767736292.934200    2745 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767736292.934202    2745 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-06 21:51:32.940683: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:00<00:00, 22.04it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 25.64it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 24.39it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃      Validate metric      ┃       DataLoader 0        ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│          val/bpd          │     10.2543363571167      │\n",
      "│          val/nll          │     7.10776424407959      │\n",
      "│          val/ppl          │    1221.4136962890625     │\n",
      "└───────────────────────────┴───────────────────────────┘\n",
      "\n",
      "\n",
      "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=sedd trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/sedd_tiny_len128 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 training.resample=false algo.var_min=false algo.clip_search_widths=[]\n",
      "not in top 1\n",
      "\n",
      "Epoch 3:  82%|████████▏ | 120/146 [01:55<00:24,  1.04it/s, v_num=0]\n",
      "Epoch 3:  82%|████████▏ | 120/146 [01:55<00:24,  1.04it/s, v_num=0]\n",
      "Epoch 3:  96%|█████████▌| 140/146 [01:56<00:05,  1.20it/s, v_num=0]\n",
      "Epoch 3:  96%|█████████▌| 140/146 [01:56<00:05,  1.20it/s, v_num=0]\n",
      "Epoch 3: 100%|██████████| 146/146 [01:57<00:00,  1.24it/s, v_num=0]\n",
      "Epoch 3: 100%|██████████| 146/146 [01:57<00:00,  1.24it/s, v_num=0]\n",
      "Epoch 3: 100%|██████████| 146/146 [01:57<00:00,  1.24it/s, v_num=0]\n",
      "Epoch 3:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
      "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
      "Epoch 4:  14%|█▎        | 20/146 [00:01<00:10, 12.18it/s, v_num=0]\n",
      "Epoch 4:  14%|█▎        | 20/146 [00:01<00:10, 12.18it/s, v_num=0]\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:03<00:08, 12.25it/s, v_num=0]\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:03<00:08, 12.25it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 38.20it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:04<00:11,  9.26it/s, v_num=0]Epoch 4, global step 634: 'val/nll' reached 7.02512 (best 7.02512), saving model to '/content/repro_runs/sedd_tiny_len128/checkpoints/best.ckpt' as top 1\n",
      "\n",
      "Epoch 4:  41%|████      | 60/146 [00:17<00:24,  3.45it/s, v_num=0]\n",
      "Epoch 4:  41%|████      | 60/146 [00:17<00:24,  3.45it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [00:19<00:15,  4.21it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [00:19<00:15,  4.21it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:20<00:09,  4.83it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:20<00:09,  4.83it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 33.50it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:21<00:09,  4.76it/s, v_num=0]Epoch 4, global step 684: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 4:  82%|████████▏ | 120/146 [01:10<00:15,  1.70it/s, v_num=0]\n",
      "Epoch 4:  82%|████████▏ | 120/146 [01:10<00:15,  1.70it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [01:12<00:03,  1.94it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [01:12<00:03,  1.94it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [01:12<00:00,  2.01it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [01:12<00:00,  2.01it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [01:12<00:00,  2.01it/s, v_num=0]\n",
      "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
      "Epoch 5:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:01<00:10, 12.59it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:01<00:10, 12.59it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:03<00:08, 12.55it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:03<00:08, 12.55it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 38.82it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:04<00:11,  9.43it/s, v_num=0]Epoch 5, global step 780: 'val/nll' reached 6.68058 (best 6.68058), saving model to '/content/repro_runs/sedd_tiny_len128/checkpoints/best.ckpt' as top 1\n",
      "\n",
      "Epoch 5:  41%|████      | 60/146 [00:09<00:13,  6.48it/s, v_num=0]\n",
      "Epoch 5:  41%|████      | 60/146 [00:09<00:13,  6.48it/s, v_num=0]\n",
      "Epoch 5:  41%|████      | 60/146 [00:10<00:14,  5.95it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
      "\n",
      "Epoch 5:  41%|████      | 60/146 [00:10<00:14,  5.95it/s, v_num=0]\n",
      "\n",
      "\n",
      "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=sedd eval.checkpoint_path=/content/repro_runs/sedd_tiny_len128/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 algo.var_min=false\n",
      "ed.dat\n",
      "[2026-01-06 22:00:57,415][dataloader][INFO] - streaming=True\n",
      "\n",
      "============================================================\n",
      "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
      "  Requested: 1000 samples\n",
      "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
      "  Returning dataset with 1000 samples\n",
      "============================================================\n",
      "\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2335.51 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2328.35 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 43585.34 examples/s]\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2026-01-06 22:02:24.280576: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767736944.297852    5598 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767736944.302930    5598 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767736944.315762    5598 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767736944.315782    5598 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767736944.315785    5598 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767736944.315787    5598 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-06 22:02:24.319740: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:00<00:00, 20.74it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 24.34it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 23.80it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃      Validate metric      ┃       DataLoader 0        ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│          val/bpd          │    10.454853057861328     │\n",
      "│          val/nll          │    7.2467522621154785     │\n",
      "│          val/ppl          │       1403.5390625        │\n",
      "└───────────────────────────┴───────────────────────────┘\n",
      "\n",
      "\n",
      "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=mdlm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/mdlm_tiny_len128 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 training.resample=false algo.var_min=false algo.clip_search_widths=[]\n",
      "not in top 1\n",
      "\n",
      "Epoch 3:  82%|████████▏ | 120/146 [01:10<00:15,  1.70it/s, v_num=0]\n",
      "Epoch 3:  82%|████████▏ | 120/146 [01:10<00:15,  1.70it/s, v_num=0]\n",
      "Epoch 3:  96%|█████████▌| 140/146 [01:12<00:03,  1.93it/s, v_num=0]\n",
      "Epoch 3:  96%|█████████▌| 140/146 [01:12<00:03,  1.93it/s, v_num=0]\n",
      "Epoch 3: 100%|██████████| 146/146 [01:12<00:00,  2.00it/s, v_num=0]\n",
      "Epoch 3: 100%|██████████| 146/146 [01:12<00:00,  2.00it/s, v_num=0]\n",
      "Epoch 3: 100%|██████████| 146/146 [01:12<00:00,  2.00it/s, v_num=0]\n",
      "Epoch 3:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
      "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
      "Epoch 4:  14%|█▎        | 20/146 [00:01<00:11, 10.96it/s, v_num=0]\n",
      "Epoch 4:  14%|█▎        | 20/146 [00:01<00:11, 10.96it/s, v_num=0]\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:03<00:09, 11.02it/s, v_num=0]\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:03<00:09, 11.02it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 36.47it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:04<00:12,  8.34it/s, v_num=0]Epoch 4, global step 634: 'val/nll' reached 6.99224 (best 6.99224), saving model to '/content/repro_runs/mdlm_tiny_len128/checkpoints/best.ckpt' as top 1\n",
      "\n",
      "Epoch 4:  41%|████      | 60/146 [00:25<00:36,  2.37it/s, v_num=0]\n",
      "Epoch 4:  41%|████      | 60/146 [00:25<00:36,  2.37it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [00:27<00:22,  2.95it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [00:27<00:22,  2.95it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:28<00:13,  3.46it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:28<00:13,  3.46it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 36.44it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:29<00:13,  3.43it/s, v_num=0]Epoch 4, global step 684: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 4:  82%|████████▏ | 120/146 [00:38<00:08,  3.08it/s, v_num=0]\n",
      "Epoch 4:  82%|████████▏ | 120/146 [00:38<00:08,  3.08it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [00:40<00:01,  3.44it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [00:40<00:01,  3.44it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [00:41<00:00,  3.54it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [00:41<00:00,  3.54it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [00:41<00:00,  3.54it/s, v_num=0]\n",
      "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
      "Epoch 5:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:01<00:11, 10.96it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:01<00:11, 10.96it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:03<00:10, 10.54it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:03<00:10, 10.54it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 36.75it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:04<00:13,  8.05it/s, v_num=0]Epoch 5, global step 780: 'val/nll' reached 6.66481 (best 6.66481), saving model to '/content/repro_runs/mdlm_tiny_len128/checkpoints/best.ckpt' as top 1\n",
      "\n",
      "Epoch 5:  41%|████      | 60/146 [00:56<01:21,  1.05it/s, v_num=0]\n",
      "Epoch 5:  41%|████      | 60/146 [00:56<01:21,  1.05it/s, v_num=0]\n",
      "Epoch 5:  41%|████      | 60/146 [00:57<01:22,  1.04it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
      "\n",
      "Epoch 5:  41%|████      | 60/146 [00:57<01:22,  1.04it/s, v_num=0]\n",
      "\n",
      "\n",
      "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=mdlm eval.checkpoint_path=/content/repro_runs/mdlm_tiny_len128/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 algo.var_min=false\n",
      "ed.dat\n",
      "[2026-01-06 22:13:09,214][dataloader][INFO] - streaming=True\n",
      "\n",
      "============================================================\n",
      "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
      "  Requested: 1000 samples\n",
      "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
      "  Returning dataset with 1000 samples\n",
      "============================================================\n",
      "\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1641.04 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1637.42 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 41776.35 examples/s]\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2026-01-06 22:14:35.633716: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767737675.651177    8915 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767737675.656208    8915 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767737675.669058    8915 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767737675.669077    8915 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767737675.669080    8915 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767737675.669082    8915 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-06 22:14:35.672924: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 18.69it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 22.21it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 21.78it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃      Validate metric      ┃       DataLoader 0        ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│          val/bpd          │    10.420144081115723     │\n",
      "│          val/nll          │     7.22269344329834      │\n",
      "│          val/ppl          │     1370.174560546875     │\n",
      "└───────────────────────────┴───────────────────────────┘\n",
      "\n",
      "\n",
      "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_base_len128 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=128 training.resample=false algo.var_min=false algo.clip_search_widths=[]\n",
      "d 6.91885 (best 6.91885), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n",
      "\n",
      "Epoch 3:  82%|████████▏ | 120/146 [01:40<00:21,  1.19it/s, v_num=0]\n",
      "Epoch 3:  82%|████████▏ | 120/146 [01:40<00:21,  1.19it/s, v_num=0]\n",
      "Epoch 3:  96%|█████████▌| 140/146 [01:43<00:04,  1.36it/s, v_num=0]\n",
      "Epoch 3:  96%|█████████▌| 140/146 [01:43<00:04,  1.36it/s, v_num=0]\n",
      "Epoch 3: 100%|██████████| 146/146 [01:44<00:00,  1.40it/s, v_num=0]\n",
      "Epoch 3: 100%|██████████| 146/146 [01:44<00:00,  1.40it/s, v_num=0]\n",
      "Epoch 3: 100%|██████████| 146/146 [01:44<00:00,  1.40it/s, v_num=0]\n",
      "Epoch 3:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
      "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
      "Epoch 4:  14%|█▎        | 20/146 [00:02<00:17,  7.24it/s, v_num=0]\n",
      "Epoch 4:  14%|█▎        | 20/146 [00:02<00:17,  7.24it/s, v_num=0]\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:05<00:14,  7.12it/s, v_num=0]\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:05<00:14,  7.12it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 23.68it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:07<00:19,  5.48it/s, v_num=0]Epoch 4, global step 634: 'val/nll' reached 6.44872 (best 6.44872), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n",
      "\n",
      "Epoch 4:  41%|████      | 60/146 [00:56<01:21,  1.05it/s, v_num=0]\n",
      "Epoch 4:  41%|████      | 60/146 [00:56<01:21,  1.05it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [00:59<00:49,  1.34it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [00:59<00:49,  1.34it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [01:02<00:28,  1.60it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [01:02<00:28,  1.60it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 22.04it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  68%|██████▊   | 100/146 [01:02<00:28,  1.59it/s, v_num=0]Epoch 4, global step 684: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 4:  82%|████████▏ | 120/146 [01:12<00:15,  1.64it/s, v_num=0]\n",
      "Epoch 4:  82%|████████▏ | 120/146 [01:12<00:15,  1.64it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [01:15<00:03,  1.85it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [01:15<00:03,  1.85it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [01:16<00:00,  1.91it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [01:16<00:00,  1.91it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [01:16<00:00,  1.91it/s, v_num=0]\n",
      "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
      "Epoch 5:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.22it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.22it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:05<00:14,  7.21it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:05<00:14,  7.21it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 24.29it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:07<00:19,  5.52it/s, v_num=0]Epoch 5, global step 780: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 5:  41%|████      | 60/146 [00:45<01:05,  1.31it/s, v_num=0]\n",
      "Epoch 5:  41%|████      | 60/146 [00:45<01:05,  1.31it/s, v_num=0]\n",
      "Epoch 5:  41%|████      | 60/146 [00:47<01:07,  1.27it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
      "\n",
      "Epoch 5:  41%|████      | 60/146 [00:47<01:07,  1.27it/s, v_num=0]\n",
      "\n",
      "\n",
      "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_finetune_Lp16 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=16 training.from_pretrained=/content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt training.resample=true algo.var_min=false algo.clip_search_widths=[]\n",
      "och 4:  27%|██▋       | 40/146 [00:05<00:15,  7.04it/s, v_num=0]\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:05<00:15,  7.04it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 24.26it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:07<00:19,  5.42it/s, v_num=0]Epoch 4, global step 584: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 4:  41%|████      | 60/146 [00:11<00:16,  5.15it/s, v_num=0]\n",
      "Epoch 4:  41%|████      | 60/146 [00:11<00:16,  5.15it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [00:14<00:11,  5.50it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [00:14<00:11,  5.50it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:17<00:07,  5.76it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:17<00:07,  5.76it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:17<00:08,  5.66it/s, v_num=0]Epoch 4, global step 634: 'val/nll' reached 6.71454 (best 6.71454), saving model to '/content/repro_runs/bd3lm_finetune_Lp16/checkpoints/best.ckpt' as top 1\n",
      "\n",
      "Epoch 4:  82%|████████▏ | 120/146 [01:28<00:19,  1.36it/s, v_num=0]\n",
      "Epoch 4:  82%|████████▏ | 120/146 [01:28<00:19,  1.36it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [01:31<00:03,  1.53it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [01:31<00:03,  1.53it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [01:32<00:00,  1.59it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [01:32<00:00,  1.59it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [01:32<00:00,  1.59it/s, v_num=0]\n",
      "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
      "Epoch 5:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.07it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.07it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:05<00:14,  7.08it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:05<00:14,  7.08it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 24.03it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:07<00:19,  5.42it/s, v_num=0]Epoch 5, global step 730: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 5:  41%|████      | 60/146 [00:19<00:27,  3.12it/s, v_num=0]\n",
      "Epoch 5:  41%|████      | 60/146 [00:19<00:27,  3.12it/s, v_num=0]\n",
      "Epoch 5:  55%|█████▍    | 80/146 [00:22<00:18,  3.63it/s, v_num=0]\n",
      "Epoch 5:  55%|█████▍    | 80/146 [00:22<00:18,  3.63it/s, v_num=0]\n",
      "Epoch 5:  68%|██████▊   | 100/146 [00:24<00:11,  4.02it/s, v_num=0]\n",
      "Epoch 5:  68%|██████▊   | 100/146 [00:24<00:11,  4.02it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 22.73it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 5:  68%|██████▊   | 100/146 [00:25<00:11,  3.96it/s, v_num=0]Epoch 5, global step 780: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 5:  82%|████████▏ | 120/146 [01:01<00:13,  1.94it/s, v_num=0]\n",
      "Epoch 5:  82%|████████▏ | 120/146 [01:01<00:13,  1.94it/s, v_num=0]\n",
      "Epoch 5:  82%|████████▏ | 120/146 [01:01<00:13,  1.94it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
      "\n",
      "Epoch 5:  82%|████████▏ | 120/146 [01:01<00:13,  1.94it/s, v_num=0]\n",
      "\n",
      "\n",
      "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_finetune_Lp16/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=16 algo.var_min=false\n",
      "ed.dat\n",
      "[2026-01-06 22:35:26,912][dataloader][INFO] - streaming=True\n",
      "\n",
      "============================================================\n",
      "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
      "  Requested: 1000 samples\n",
      "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
      "  Returning dataset with 1000 samples\n",
      "============================================================\n",
      "\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2317.72 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2310.26 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 40131.12 examples/s]\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2026-01-06 22:36:51.699138: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767739011.716218   14982 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767739011.721352   14982 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767739011.734211   14982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767739011.734232   14982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767739011.734235   14982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767739011.734237   14982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-06 22:36:51.738072: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 13.89it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 15.78it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 15.54it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃      Validate metric      ┃       DataLoader 0        ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│          val/bpd          │    10.393876075744629     │\n",
      "│          val/nll          │     7.204485893249512     │\n",
      "│          val/ppl          │     1345.452880859375     │\n",
      "└───────────────────────────┴───────────────────────────┘\n",
      "\n",
      "\n",
      "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_finetune_Lp8 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=8 training.from_pretrained=/content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt training.resample=true algo.var_min=false algo.clip_search_widths=[]\n",
      " [00:02<00:17,  7.04it/s, v_num=0]\n",
      "Epoch 4:  14%|█▎        | 20/146 [00:02<00:17,  7.04it/s, v_num=0]\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:05<00:15,  6.97it/s, v_num=0]\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:05<00:15,  6.97it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 24.34it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:07<00:19,  5.36it/s, v_num=0]Epoch 4, global step 584: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 4:  41%|████      | 60/146 [00:15<00:22,  3.87it/s, v_num=0]\n",
      "Epoch 4:  41%|████      | 60/146 [00:15<00:22,  3.87it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [00:18<00:15,  4.35it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [00:18<00:15,  4.35it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:21<00:09,  4.71it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:21<00:09,  4.71it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 23.62it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  68%|██████▊   | 100/146 [00:21<00:09,  4.64it/s, v_num=0]Epoch 4, global step 634: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 4:  82%|████████▏ | 120/146 [00:31<00:06,  3.84it/s, v_num=0]\n",
      "Epoch 4:  82%|████████▏ | 120/146 [00:31<00:06,  3.84it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [00:34<00:01,  4.11it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [00:34<00:01,  4.11it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [00:34<00:00,  4.18it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [00:34<00:00,  4.18it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [00:34<00:00,  4.18it/s, v_num=0]\n",
      "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
      "Epoch 5:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.04it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.04it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:05<00:15,  6.91it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:05<00:15,  6.91it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 24.33it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:07<00:19,  5.32it/s, v_num=0]Epoch 5, global step 730: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 5:  41%|████      | 60/146 [00:55<01:20,  1.07it/s, v_num=0]\n",
      "Epoch 5:  41%|████      | 60/146 [00:55<01:20,  1.07it/s, v_num=0]\n",
      "Epoch 5:  55%|█████▍    | 80/146 [00:58<00:48,  1.36it/s, v_num=0]\n",
      "Epoch 5:  55%|█████▍    | 80/146 [00:58<00:48,  1.36it/s, v_num=0]\n",
      "Epoch 5:  68%|██████▊   | 100/146 [01:01<00:28,  1.63it/s, v_num=0]\n",
      "Epoch 5:  68%|██████▊   | 100/146 [01:01<00:28,  1.63it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 22.44it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 5:  68%|██████▊   | 100/146 [01:01<00:28,  1.62it/s, v_num=0]Epoch 5, global step 780: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 5:  82%|████████▏ | 120/146 [01:09<00:15,  1.72it/s, v_num=0]\n",
      "Epoch 5:  82%|████████▏ | 120/146 [01:09<00:15,  1.72it/s, v_num=0]\n",
      "Epoch 5:  82%|████████▏ | 120/146 [01:09<00:15,  1.72it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
      "\n",
      "Epoch 5:  82%|████████▏ | 120/146 [01:09<00:15,  1.72it/s, v_num=0]\n",
      "\n",
      "\n",
      "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_finetune_Lp8/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=8 algo.var_min=false\n",
      "ed.dat\n",
      "[2026-01-06 22:47:07,290][dataloader][INFO] - streaming=True\n",
      "\n",
      "============================================================\n",
      "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
      "  Requested: 1000 samples\n",
      "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
      "  Returning dataset with 1000 samples\n",
      "============================================================\n",
      "\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2181.23 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2174.25 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 37721.95 examples/s]\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2026-01-06 22:48:30.829402: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767739710.846233   18163 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767739710.851208   18163 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767739710.863948   18163 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767739710.863968   18163 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767739710.863971   18163 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767739710.863973   18163 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-06 22:48:30.868152: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 14.03it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 16.00it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 15.76it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃      Validate metric      ┃       DataLoader 0        ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│          val/bpd          │    10.241820335388184     │\n",
      "│          val/nll          │     7.099088668823242     │\n",
      "│          val/ppl          │     1210.863037109375     │\n",
      "└───────────────────────────┴───────────────────────────┘\n",
      "\n",
      "\n",
      "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_finetune_Lp4 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 training.from_pretrained=/content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt training.resample=true algo.var_min=false algo.clip_search_widths=[]\n",
      "poch 4:  27%|██▋       | 40/146 [00:05<00:15,  6.91it/s, v_num=0]\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:05<00:15,  6.91it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 24.14it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  27%|██▋       | 40/146 [00:07<00:19,  5.32it/s, v_num=0]Epoch 4, global step 584: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 4:  41%|████      | 60/146 [01:00<01:26,  1.00it/s, v_num=0]\n",
      "Epoch 4:  41%|████      | 60/146 [01:00<01:26,  1.00it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [01:03<00:52,  1.27it/s, v_num=0]\n",
      "Epoch 4:  55%|█████▍    | 80/146 [01:03<00:52,  1.27it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [01:05<00:30,  1.52it/s, v_num=0]\n",
      "Epoch 4:  68%|██████▊   | 100/146 [01:05<00:30,  1.52it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 4:  68%|██████▊   | 100/146 [01:06<00:30,  1.51it/s, v_num=0]Epoch 4, global step 634: 'val/nll' reached 6.77226 (best 6.77226), saving model to '/content/repro_runs/bd3lm_finetune_Lp4/checkpoints/best.ckpt' as top 1\n",
      "\n",
      "Epoch 4:  82%|████████▏ | 120/146 [01:14<00:16,  1.60it/s, v_num=0]\n",
      "Epoch 4:  82%|████████▏ | 120/146 [01:14<00:16,  1.60it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [01:17<00:03,  1.80it/s, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 140/146 [01:17<00:03,  1.80it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [01:18<00:00,  1.86it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [01:18<00:00,  1.86it/s, v_num=0]\n",
      "Epoch 4: 100%|██████████| 146/146 [01:18<00:00,  1.86it/s, v_num=0]\n",
      "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
      "Epoch 5:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.09it/s, v_num=0]\n",
      "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.09it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:05<00:14,  7.07it/s, v_num=0]\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:05<00:14,  7.07it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 23.66it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 5:  27%|██▋       | 40/146 [00:07<00:19,  5.39it/s, v_num=0]Epoch 5, global step 730: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 5:  41%|████      | 60/146 [00:14<00:20,  4.16it/s, v_num=0]\n",
      "Epoch 5:  41%|████      | 60/146 [00:14<00:20,  4.16it/s, v_num=0]\n",
      "Epoch 5:  55%|█████▍    | 80/146 [00:17<00:14,  4.64it/s, v_num=0]\n",
      "Epoch 5:  55%|█████▍    | 80/146 [00:17<00:14,  4.64it/s, v_num=0]\n",
      "Epoch 5:  68%|██████▊   | 100/146 [00:20<00:09,  4.97it/s, v_num=0]\n",
      "Epoch 5:  68%|██████▊   | 100/146 [00:20<00:09,  4.97it/s, v_num=0]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 23.04it/s]\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\n",
      "Epoch 5:  68%|██████▊   | 100/146 [00:20<00:09,  4.86it/s, v_num=0]Epoch 5, global step 780: 'val/nll' was not in top 1\n",
      "\n",
      "Epoch 5:  82%|████████▏ | 120/146 [01:11<00:15,  1.68it/s, v_num=0]\n",
      "Epoch 5:  82%|████████▏ | 120/146 [01:11<00:15,  1.68it/s, v_num=0]\n",
      "Epoch 5:  82%|████████▏ | 120/146 [01:11<00:15,  1.68it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
      "\n",
      "Epoch 5:  82%|████████▏ | 120/146 [01:11<00:15,  1.68it/s, v_num=0]\n",
      "\n",
      "\n",
      "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_finetune_Lp4/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 algo.var_min=false\n",
      "ed.dat\n",
      "[2026-01-06 22:58:04,861][dataloader][INFO] - streaming=True\n",
      "\n",
      "============================================================\n",
      "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
      "  Requested: 1000 samples\n",
      "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
      "  Returning dataset with 1000 samples\n",
      "============================================================\n",
      "\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2309.48 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2301.82 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 40770.48 examples/s]\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2026-01-06 22:59:28.098308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767740368.115349   21168 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767740368.120276   21168 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767740368.133263   21168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767740368.133287   21168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767740368.133290   21168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767740368.133292   21168 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-06 22:59:28.137471: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "  warnings.warn(  # warn only once\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\n",
      "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 13.93it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 15.92it/s]\n",
      "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 15.68it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃      Validate metric      ┃       DataLoader 0        ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│          val/bpd          │     10.20077896118164     │\n",
      "│          val/nll          │     7.07064151763916      │\n",
      "│          val/ppl          │     1176.90283203125      │\n",
      "└───────────────────────────┴───────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "# import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1) Autoregressive baseline (tiny, scratch)\n",
    "ar_run = \"ar_tiny_len128\"\n",
    "ar_ckpt = train_run(ar_run, algo=\"ar\")\n",
    "ar_ppl = eval_run(algo=\"ar\", checkpoint_path=ar_ckpt)\n",
    "results.append({\"model\": \"Autoregressive\", \"block_size_Lprime\": \"-\", \"val_ppl\": ar_ppl})\n",
    "\n",
    "# 2) Diffusion baselines (tiny, scratch): SEDD + MDLM\n",
    "for algo_name, display_name in [(\"sedd\", \"SEDD\"), (\"mdlm\", \"MDLM\")]:\n",
    "    run_name = f\"{algo_name}_tiny_len128\"\n",
    "    ckpt = train_run(\n",
    "        run_name,\n",
    "        algo=algo_name,\n",
    "        extra_overrides=[\n",
    "            # lightweight run; keep consistent with other tiny runs\n",
    "            \"training.resample=false\",\n",
    "            \"algo.var_min=false\",\n",
    "            \"algo.clip_search_widths=[]\",\n",
    "        ],\n",
    "    )\n",
    "    ppl = eval_run(\n",
    "        algo=algo_name,\n",
    "        checkpoint_path=ckpt,\n",
    "        extra_overrides=[\n",
    "            \"algo.var_min=false\",\n",
    "        ],\n",
    "    )\n",
    "    results.append({\"model\": display_name, \"block_size_Lprime\": \"-\", \"val_ppl\": ppl})\n",
    "\n",
    "# 3) BD3LM methodology match (small-scale):\n",
    "bd3lm_base_run = \"bd3lm_base_len128\"\n",
    "bd3lm_base_ckpt = train_run(\n",
    "    bd3lm_base_run,\n",
    "    algo=\"bd3lm\",\n",
    "    block_size=128,\n",
    "    extra_overrides=[\n",
    "        \"training.resample=false\",\n",
    "        \"algo.var_min=false\",\n",
    "        \"algo.clip_search_widths=[]\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "for Lprime in [16, 8, 4]:\n",
    "    finetune_run = f\"bd3lm_finetune_Lp{Lprime}\"\n",
    "    finetune_ckpt = train_run(\n",
    "        finetune_run,\n",
    "        algo=\"bd3lm\",\n",
    "        block_size=Lprime,\n",
    "        from_pretrained=bd3lm_base_ckpt,\n",
    "        extra_overrides=[\n",
    "            \"training.resample=true\",\n",
    "            \"algo.var_min=false\",\n",
    "            \"algo.clip_search_widths=[]\",\n",
    "        ],\n",
    "    )\n",
    "    ppl = eval_run(\n",
    "        algo=\"bd3lm\",\n",
    "        checkpoint_path=finetune_ckpt,\n",
    "        block_size=Lprime,\n",
    "        extra_overrides=[\n",
    "            \"algo.var_min=false\",\n",
    "        ],\n",
    "    )\n",
    "    results.append({\"model\": \"Block diffusion (BD3LM)\", \"block_size_Lprime\": Lprime, \"val_ppl\": ppl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61037b8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1767740383117,
     "user": {
      "displayName": "Ηλίας Μάκρας",
      "userId": "16381513743156120405"
     },
     "user_tz": -120
    },
    "id": "61037b8b",
    "outputId": "fa64ffb1-3f7d-41ce-b228-6622b95768b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------+--------------------+\n",
      "| model                   | block_size_Lprime | val_ppl            |\n",
      "+-------------------------+-------------------+--------------------+\n",
      "| Autoregressive          | -                 | 1221.4136962890625 |\n",
      "| SEDD                    | -                 | 1403.5390625       |\n",
      "| MDLM                    | -                 | 1370.174560546875  |\n",
      "| Block diffusion (BD3LM) | 16                | 1345.452880859375  |\n",
      "| Block diffusion (BD3LM) | 8                 | 1210.863037109375  |\n",
      "| Block diffusion (BD3LM) | 4                 | 1176.90283203125   |\n",
      "+-------------------------+-------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "def print_table(rows):\n",
    "    if not rows:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "\n",
    "    columns = list(rows[0].keys())\n",
    "\n",
    "    str_rows = [\n",
    "        {col: str(row.get(col, \"\")) for col in columns}\n",
    "        for row in rows\n",
    "    ]\n",
    "\n",
    "    widths = {\n",
    "        col: max(len(col), max(len(row[col]) for row in str_rows))\n",
    "        for col in columns\n",
    "    }\n",
    "\n",
    "    def print_separator():\n",
    "        print(\"+\" + \"+\".join(\"-\" * (widths[col] + 2) for col in columns) + \"+\")\n",
    "\n",
    "    def print_row(row):\n",
    "        print(\n",
    "            \"| \" +\n",
    "            \" | \".join(row[col].ljust(widths[col]) for col in columns) +\n",
    "            \" |\"\n",
    "        )\n",
    "\n",
    "    # Print table\n",
    "    print_separator()\n",
    "    print_row({col: col for col in columns})\n",
    "    print_separator()\n",
    "    for row in str_rows:\n",
    "        print_row(row)\n",
    "    print_separator()\n",
    "\n",
    "print_table(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
