{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Vw2kb8OCsy"
      },
      "source": [
        "# Table 8 Reproduction \n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwpGl4erOCs0",
        "outputId": "d3ec7182-c73e-48b7-cbbd-af01c4fb9342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo already exists, skipping clone\n"
          ]
        }
      ],
      "source": [
        "# Clone project repo\n",
        "import os\n",
        "if not os.path.exists('/content/bd3lms'):\n",
        "    !cd /content && git clone https://github.com/ntua-el21050/bd3lms.git\n",
        "else:\n",
        "    print(\"Repo already exists, skipping clone\")\n",
        "\n",
        "!mkdir -p /content/repro_runs_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "L9i1Mc7OOCs1"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%%capture\n",
        "!pip install -r bd3lms/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv0iAbAEItnv"
      },
      "source": [
        "\n",
        "# Step 1: Patch diffusion.py (Print variance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6T7VosqIlHb",
        "outputId": "9410342d-37c6-40bd-b139-0c536869a15f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diffusion.py already patched.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import sys\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "diffusion_file = '/content/bd3lms/diffusion.py'\n",
        "with open(diffusion_file, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "if 'VARIANCE:' not in content:\n",
        "    print(\"üõ†Ô∏è Patching diffusion.py...\")\n",
        "    old_log = \"\"\"      self.log(f'valid_var_{round(eps_min, 2)} - {round(eps_max, 2)}',\n",
        "                all_vars / len(var),\n",
        "                on_epoch=True,\n",
        "                on_step=False,\n",
        "                sync_dist=True)\"\"\"\n",
        "\n",
        "    new_log = \"\"\"      _var_val = (all_vars / len(var)).item()\n",
        "      print(f'VARIANCE: valid_var_{round(eps_min, 2)} - {round(eps_max, 2)} = {_var_val:.4f}')\n",
        "      self.log(f'valid_var_{round(eps_min, 2)} - {round(eps_max, 2)}',\n",
        "                all_vars / len(var),\n",
        "                on_epoch=True,\n",
        "                on_step=False,\n",
        "                sync_dist=True)\"\"\"\n",
        "\n",
        "    content = content.replace(old_log, new_log)\n",
        "    with open(diffusion_file, 'w') as f:\n",
        "        f.write(content)\n",
        "    # Clear pycache\n",
        "    shutil.rmtree('/content/bd3lms/__pycache__', ignore_errors=True)\n",
        "    print(\"Patch applied.\")\n",
        "else:\n",
        "    print(\"diffusion.py already patched.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "C3sV8-lEI-iP"
      },
      "outputs": [],
      "source": [
        "# Utils\n",
        "\n",
        "BASE_MAX_STEPS = 5000\n",
        "BASE_MAX_SAMPLES = 10000\n",
        "FINETUNE_MAX_STEPS = 3000\n",
        "FINETUNE_MAX_SAMPLES = 5000\n",
        "\n",
        "def run_main(overrides):\n",
        "    env = dict(os.environ)\n",
        "    env[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
        "    cmd = [sys.executable, '-u', 'main.py'] + overrides\n",
        "    print(f\"\\n$ python main.py ... {' '.join(overrides[-3:])}\")\n",
        "\n",
        "    proc = subprocess.run(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        cwd='/content/bd3lms',\n",
        "        env=env\n",
        "    )\n",
        "    if proc.returncode != 0:\n",
        "        print(f\"FAILED:\\n{proc.stdout[-2000:]}\")\n",
        "        raise RuntimeError(f\"Command failed with code {proc.returncode}\")\n",
        "    return proc.stdout\n",
        "\n",
        "def train_run(run_name, algo, block_size, from_pretrained=None, extra_overrides=None, is_base=False):\n",
        "    save_dir = f\"/content/repro_runs_final/{run_name}\"\n",
        "    ckpt_path = f\"{save_dir}/checkpoints/last.ckpt\"\n",
        "\n",
        "    if os.path.exists(ckpt_path):\n",
        "        print(f\" Checkpoint exists: {run_name}\")\n",
        "        return ckpt_path, \"\"\n",
        "\n",
        "    # Set budget\n",
        "    steps = BASE_MAX_STEPS if is_base else FINETUNE_MAX_STEPS\n",
        "    samples = BASE_MAX_SAMPLES if is_base else FINETUNE_MAX_SAMPLES\n",
        "\n",
        "    overrides = [\n",
        "        \"mode=train\",\n",
        "        \"data=lm1b-wrap\", \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\", f\"data.max_train_samples={samples}\",\n",
        "        \"model=tiny\", \"model.length=128\", \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\",\n",
        "        \"trainer.accelerator=gpu\", \"trainer.devices=1\", \"trainer.num_nodes=1\",\n",
        "        \"trainer.precision=16-mixed\", \"trainer.num_sanity_val_steps=0\",\n",
        "        \"trainer.log_every_n_steps=20\", \"trainer.val_check_interval=100\",\n",
        "        f\"trainer.max_steps={steps}\",\n",
        "        \"data.max_valid_samples=800\", \"data.max_test_samples=100\",\n",
        "        f\"checkpointing.save_dir={save_dir}\",\n",
        "        \"checkpointing.resume_from_ckpt=false\",\n",
        "        \"wandb=null\",\n",
        "        \"loader.global_batch_size=8\", \"loader.eval_global_batch_size=8\",\n",
        "        \"loader.batch_size=8\", \"loader.eval_batch_size=8\",\n",
        "        \"loader.num_workers=2\", \"trainer.accumulate_grad_batches=1\",\n",
        "        f\"block_size={block_size}\",\n",
        "    ]\n",
        "\n",
        "    if from_pretrained:\n",
        "        overrides.append(f\"training.from_pretrained={from_pretrained}\")\n",
        "\n",
        "    if extra_overrides:\n",
        "        overrides.extend(extra_overrides)\n",
        "\n",
        "    log = run_main(overrides)\n",
        "    return ckpt_path, log\n",
        "\n",
        "def eval_run(checkpoint_path, algo, block_size):\n",
        "    overrides = [\n",
        "        \"mode=ppl_eval\",\n",
        "        \"data=lm1b-wrap\", \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\", \"data.max_test_samples=1000\",\n",
        "        \"model=tiny\", \"model.length=128\", \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\", f\"eval.checkpoint_path={checkpoint_path}\",\n",
        "        \"trainer.accelerator=gpu\", \"trainer.devices=1\",\n",
        "        \"trainer.precision=16-mixed\", \"trainer.num_sanity_val_steps=0\",\n",
        "        \"wandb=null\",\n",
        "        \"noise.type=loglinear\",\n",
        "        \"algo.var_min=false\",\n",
        "        \"data.max_train_samples=1000\", \"data.max_valid_samples=100\",\n",
        "        \"loader.global_batch_size=8\", \"loader.batch_size=8\",\n",
        "        f\"block_size={block_size}\",\n",
        "    ]\n",
        "    log = run_main(overrides)\n",
        "    for line in reversed(log.splitlines()):\n",
        "        if \"val/ppl\" in line.lower():\n",
        "            m = re.search(r\"val/ppl.*?([0-9]+(?:\\.[0-9]+)?)\", line, re.IGNORECASE)\n",
        "            if m: return float(m.group(1))\n",
        "    return None\n",
        "\n",
        "def extract_valid_var(log_text, key):\n",
        "    last_val = None\n",
        "    for line in log_text.splitlines():\n",
        "        if f\"VARIANCE: {key}\" in line:\n",
        "            m = re.search(r\"=\\s*([0-9]+\\.?[0-9]*)\", line)\n",
        "            if m: last_val = float(m.group(1))\n",
        "    return last_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa8gOihXJKmm"
      },
      "source": [
        "# Step 2. Run base training (if missing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox6sJQqiJK4v",
        "outputId": "368fb828-750e-4480-ef7e-3cc5342ce57f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP A: BASE MODEL\n",
            "============================================================\n",
            "\n",
            "$ python main.py ... block_size=128 training.resample=false algo.var_min=false\n",
            "Base Checkpoint: /content/repro_runs_final/bd3lm_base_len128_vfinal/checkpoints/last.ckpt\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n{'='*60}\\nSTEP A: BASE MODEL\\n{'='*60}\")\n",
        "base_run_name = \"bd3lm_base_len128_vfinal\"\n",
        "bd3lm_base_ckpt, _ = train_run(\n",
        "    run_name=base_run_name,\n",
        "    algo=\"bd3lm\",\n",
        "    block_size=128,\n",
        "    from_pretrained=None,\n",
        "    extra_overrides=[\"training.resample=false\", \"algo.var_min=false\"],\n",
        "    is_base=True\n",
        ")\n",
        "print(f\"Base Checkpoint: {bd3lm_base_ckpt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQWqGnOCJbrD"
      },
      "source": [
        "# Step 3: Fine-tuning and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEGMJP38Jb47",
        "outputId": "cbd6bf3b-9d5c-4e17-b6cf-cd9bd8a75b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TABLE 8 ‚Äî Block Size L'=4\n",
            "======================================================================\n",
            "\n",
            "--- Processing: Clipped U[0.45,0.95] ---\n",
            "\n",
            "$ python main.py ... algo.fix_clipping=true training.sampling_eps_min=0.45 training.sampling_eps_max=0.95\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=4\n",
            "  ‚úì PPL=1199.43, Var=15.2313\n",
            "\n",
            "--- Processing: Clipped U[0.3,0.8] ---\n",
            "\n",
            "$ python main.py ... algo.fix_clipping=true training.sampling_eps_min=0.3 training.sampling_eps_max=0.8\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=4\n",
            "  ‚úì PPL=1101.03, Var=27.6118\n",
            "\n",
            "--- Processing: Linear U[0,1] ---\n",
            "\n",
            "$ python main.py ... algo.fix_clipping=true training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=4\n",
            "  ‚úì PPL=751.15, Var=260.1035\n",
            "\n",
            "--- Processing: Logarithmic ---\n",
            "\n",
            "$ python main.py ... noise.type=log training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=4\n",
            "  ‚úì PPL=750.05, Var=125.6219\n",
            "\n",
            "--- Processing: Square root ---\n",
            "\n",
            "$ python main.py ... noise.type=square_root training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=4\n",
            "  ‚úì PPL=718.90, Var=83.832\n",
            "\n",
            "======================================================================\n",
            "TABLE 8 ‚Äî Block Size L'=16\n",
            "======================================================================\n",
            "\n",
            "--- Processing: Clipped U[0.45,0.95] ---\n",
            "\n",
            "$ python main.py ... algo.fix_clipping=true training.sampling_eps_min=0.45 training.sampling_eps_max=0.95\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=16\n",
            "  ‚úì PPL=1056.46, Var=4.3476\n",
            "\n",
            "--- Processing: Clipped U[0.3,0.8] ---\n",
            "\n",
            "$ python main.py ... algo.fix_clipping=true training.sampling_eps_min=0.3 training.sampling_eps_max=0.8\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=16\n",
            "  ‚úì PPL=797.62, Var=6.4124\n",
            "\n",
            "--- Processing: Linear U[0,1] ---\n",
            "\n",
            "$ python main.py ... algo.fix_clipping=true training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=16\n",
            "  ‚úì PPL=661.99, Var=44.7271\n",
            "\n",
            "--- Processing: Square ---\n",
            "\n",
            "$ python main.py ... noise.type=square training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=16\n",
            "  ‚úì PPL=627.02, Var=27.5325\n",
            "\n",
            "--- Processing: Cosine ---\n",
            "\n",
            "$ python main.py ... noise.type=cosine training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "\n",
            "$ python main.py ... loader.global_batch_size=8 loader.batch_size=8 block_size=16\n",
            "  ‚úì PPL=633.59, Var=20.8008\n"
          ]
        }
      ],
      "source": [
        "paper_data = {\n",
        "    4:  {\"Clipped U[0.45,0.95]\": 29.21, \"Clipped U[0.3,0.8]\": 29.38, \"Linear U[0,1]\": 30.18, \"Logarithmic\": 30.36, \"Square root\": 31.41},\n",
        "    16: {\"Clipped U[0.45,0.95]\": 31.42, \"Clipped U[0.3,0.8]\": 31.12, \"Linear U[0,1]\": 31.72, \"Square\": 31.43, \"Cosine\": 31.41}\n",
        "}\n",
        "\n",
        "schedules = {\n",
        "    4: [\n",
        "        (\"Clipped U[0.45,0.95]\", [\"training.sampling_eps_min=0.45\", \"training.sampling_eps_max=0.95\"], \"valid_var_0.45 - 0.95\"),\n",
        "        (\"Clipped U[0.3,0.8]\",   [\"training.sampling_eps_min=0.3\", \"training.sampling_eps_max=0.8\"], \"valid_var_0.3 - 0.8\"),\n",
        "        (\"Linear U[0,1]\",        [\"training.sampling_eps_min=0.001\", \"training.sampling_eps_max=1.0\"], \"valid_var_0.0 - 1\"),\n",
        "        (\"Logarithmic\",          [\"noise.type=log\", \"training.sampling_eps_min=0.001\", \"training.sampling_eps_max=1.0\"], \"valid_var_0.0 - 1\"),\n",
        "        (\"Square root\",          [\"noise.type=square_root\", \"training.sampling_eps_min=0.001\", \"training.sampling_eps_max=1.0\"], \"valid_var_0.0 - 1\"),\n",
        "    ],\n",
        "    16: [\n",
        "        (\"Clipped U[0.45,0.95]\", [\"training.sampling_eps_min=0.45\", \"training.sampling_eps_max=0.95\"], \"valid_var_0.45 - 0.95\"),\n",
        "        (\"Clipped U[0.3,0.8]\",   [\"training.sampling_eps_min=0.3\", \"training.sampling_eps_max=0.8\"], \"valid_var_0.3 - 0.8\"),\n",
        "        (\"Linear U[0,1]\",        [\"training.sampling_eps_min=0.001\", \"training.sampling_eps_max=1.0\"], \"valid_var_0.0 - 1\"),\n",
        "        (\"Square\",               [\"noise.type=square\", \"training.sampling_eps_min=0.001\", \"training.sampling_eps_max=1.0\"], \"valid_var_0.0 - 1\"),\n",
        "        (\"Cosine\",               [\"noise.type=cosine\", \"training.sampling_eps_min=0.001\", \"training.sampling_eps_max=1.0\"], \"valid_var_0.0 - 1\"),\n",
        "    ]\n",
        "}\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for Lp in [4, 16]:\n",
        "    print(f\"\\n{'='*70}\\nTABLE 8 ‚Äî Block Size L'={Lp}\\n{'='*70}\")\n",
        "    results = []\n",
        "\n",
        "    for name, sched_ov, var_key in schedules[Lp]:\n",
        "        print(f\"\\n--- Processing: {name} ---\")\n",
        "        safe_name = name.replace(\"[\",\"\").replace(\"]\",\"\").replace(\",\",\"_\").replace(\" \",\"_\")\n",
        "        run_name = f\"bd3lm_fine_{safe_name}_Lp{Lp}\"\n",
        "\n",
        "        # A. TRAIN (With Variance Monitoring)\n",
        "        ckpt, train_log = train_run(\n",
        "            run_name,\n",
        "            algo=\"bd3lm\",\n",
        "            block_size=Lp,\n",
        "            from_pretrained=bd3lm_base_ckpt,\n",
        "            extra_overrides=[\n",
        "                \"training.resample=true\",\n",
        "                \"algo.var_min=true\",\n",
        "                \"algo.clip_search_widths=[0.5]\",\n",
        "                \"algo.fix_clipping=true\",\n",
        "            ] + sched_ov,\n",
        "            is_base=False\n",
        "        )\n",
        "\n",
        "        # B. EXTRACT VARIANCE & EVAL PPL\n",
        "        var_nelbo = extract_valid_var(train_log, key=var_key)\n",
        "        ppl = eval_run(ckpt, algo=\"bd3lm\", block_size=Lp)\n",
        "\n",
        "        print(f\"  ‚úì PPL={ppl:.2f}, Var={var_nelbo if var_nelbo else 'N/A'}\")\n",
        "        results.append({\"Schedule\": name, \"PPL\": ppl, \"Var NELBO\": var_nelbo})\n",
        "\n",
        "    all_results[Lp] = results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
