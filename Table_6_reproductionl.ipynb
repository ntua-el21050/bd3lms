{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Table 6 Reproduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup",
        "outputId": "82a85f1e-20c3-49f0-bff8-b8d40653adb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bd3lms'...\n",
            "remote: Enumerating objects: 1145, done.\u001b[K\n",
            "remote: Counting objects: 100% (409/409), done.\u001b[K\n",
            "remote: Compressing objects: 100% (190/190), done.\u001b[K\n",
            "remote: Total 1145 (delta 308), reused 280 (delta 218), pack-reused 736 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1145/1145), 7.91 MiB | 16.62 MiB/s, done.\n",
            "Resolving deltas: 100% (715/715), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone YOUR fork\n",
        "!cd /content && rm -rf bd3lms\n",
        "!cd /content && git clone https://github.com/ntua-el21050/bd3lms.git\n",
        "\n",
        "!mkdir -p /content/bd3lms/data\n",
        "!mkdir -p /content/repro_runs\n",
        "!mkdir -p /content/hf_checkpoints\n",
        "!mkdir -p /content/sample_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchmetrics==1.6.2 datasets==3.3.2 einops==0.8.1 \\\n",
        "    hydra-core==1.3.2 lightning==2.5.0.post0 transformers==4.49.0 \\\n",
        "    huggingface_hub fsspec==2024.2.0 omegaconf==2.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.insert(0, '/content/bd3lms')\n",
        "\n",
        "def run_main(overrides, timeout=None):\n",
        "    \"\"\"Run main.py with overrides.\"\"\"\n",
        "    env = dict(os.environ)\n",
        "    env.setdefault(\"HYDRA_FULL_ERROR\", \"1\")\n",
        "    cmd = [sys.executable, \"-u\", \"bd3lms/main.py\", *overrides]\n",
        "    print(\"\\n$\", \" \".join(cmd[:8]), \"...\")\n",
        "    proc = subprocess.run(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        timeout=timeout,\n",
        "        check=False,\n",
        "        env=env,\n",
        "    )\n",
        "    print(proc.stdout[-3000:])\n",
        "    if proc.returncode != 0:\n",
        "        raise RuntimeError(f\"Command failed with return code {proc.returncode}\")\n",
        "    return proc.stdout\n",
        "\n",
        "\n",
        "def _small_loader_overrides(batch_size=4, num_workers=2):\n",
        "    return [\n",
        "        f\"loader.global_batch_size={batch_size}\",\n",
        "        f\"loader.eval_global_batch_size={batch_size}\",\n",
        "        f\"loader.batch_size={batch_size}\",\n",
        "        f\"loader.eval_batch_size={batch_size}\",\n",
        "        f\"loader.num_workers={num_workers}\",\n",
        "        \"trainer.accumulate_grad_batches=1\",\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "train_run"
      },
      "outputs": [],
      "source": [
        "def train_run(run_name, algo, block_size=None, from_pretrained=None, max_steps=800, extra_overrides=None, model_length=1024):\n",
        "    \"\"\"Train a model.\"\"\"\n",
        "    save_dir = Path(\"/content/repro_runs\") / run_name\n",
        "    if save_dir.exists():\n",
        "        shutil.rmtree(save_dir)\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    overrides = [\n",
        "        \"mode=train\",\n",
        "        \"data=openwebtext-split\",\n",
        "        \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\",\n",
        "        \"data.max_train_samples=1500\",\n",
        "        \"data.max_valid_samples=100\",\n",
        "        \"data.max_test_samples=100\",\n",
        "        \"model=tiny\",\n",
        "        f\"model.length={model_length}\",\n",
        "        \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\",\n",
        "        \"trainer.accelerator=gpu\",\n",
        "        \"trainer.devices=1\",\n",
        "        \"trainer.num_nodes=1\",\n",
        "        \"trainer.precision=16-mixed\",\n",
        "        \"trainer.num_sanity_val_steps=0\",\n",
        "        \"trainer.log_every_n_steps=20\",\n",
        "        \"trainer.val_check_interval=1\",#50\",\n",
        "        f\"trainer.max_steps={max_steps}\",\n",
        "        f\"checkpointing.save_dir={save_dir}\",\n",
        "        \"checkpointing.resume_from_ckpt=false\",\n",
        "        \"wandb=null\",\n",
        "    ]\n",
        "    overrides.extend(_small_loader_overrides(batch_size=4, num_workers=2))\n",
        "\n",
        "    if block_size is not None:\n",
        "        overrides.append(f\"block_size={block_size}\")\n",
        "    if from_pretrained is not None:\n",
        "        overrides.append(f\"training.from_pretrained={from_pretrained}\")\n",
        "        overrides.append(\"training.resample=true\")\n",
        "    if extra_overrides:\n",
        "        overrides.extend(extra_overrides)\n",
        "\n",
        "    _ = run_main(overrides)\n",
        "\n",
        "    # Find checkpoint\n",
        "    ckpt_dir = save_dir / \"checkpoints\"\n",
        "    for name in [\"best.ckpt\", \"last.ckpt\"]:\n",
        "        ckpt = ckpt_dir / name\n",
        "        if ckpt.exists():\n",
        "            print(f\"✓ Checkpoint: {ckpt}\")\n",
        "            return str(ckpt)\n",
        "\n",
        "    # List what we have\n",
        "    if ckpt_dir.exists():\n",
        "        print(f\"Available checkpoints: {list(ckpt_dir.glob('*.ckpt'))}\")\n",
        "    raise FileNotFoundError(f\"No checkpoint in {ckpt_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eval_run"
      },
      "outputs": [],
      "source": [
        "def extract_length_stats(log_text, logdir=None):\n",
        "    \"\"\"\n",
        "    Extract actual token length statistics from CSV.\n",
        "    \"\"\"\n",
        "    lengths = []\n",
        "\n",
        "    # Read CSV file (BD3-LM saves as CSV without header)\n",
        "    if logdir and os.path.exists(logdir) and os.path.isfile(logdir):\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            df = pd.read_csv(logdir, header=None)\n",
        "            # Column 1 is length\n",
        "            lengths.extend(df[1].dropna().astype(int).tolist())\n",
        "            print(f\"Found {len(lengths)} samples in CSV\")\n",
        "        except Exception as e:\n",
        "            print(f\"CSV error: {e}\")\n",
        "\n",
        "    if lengths:\n",
        "        return {\n",
        "            'count': len(lengths),\n",
        "            'median': int(np.median(lengths)),\n",
        "            'max': int(np.max(lengths)),\n",
        "            'mean': round(np.mean(lengths), 1),\n",
        "        }\n",
        "    return {'count': 0, 'median': None, 'max': None, 'mean': None}\n",
        "\n",
        "def eval_run(algo, checkpoint_path, block_size=None, num_samples=20, extra_overrides=None, model_length=1024):\n",
        "    \"\"\"\n",
        "    Run multiple times to collect samples (workaround for variable-length bug).\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    logfile = f\"/content/sample_logs/varlen_{algo}_bs{block_size}\"\n",
        "\n",
        "    # Clean up\n",
        "    if os.path.exists(logfile):\n",
        "        os.remove(logfile)\n",
        "\n",
        "    all_lengths = []\n",
        "\n",
        "    # Run multiple times with 1 sample each (workaround)\n",
        "    for i in range(num_samples):\n",
        "        print(f\"\\rGenerating sample {i+1}/{num_samples}...\", end=\"\")\n",
        "\n",
        "        overrides = [\n",
        "            \"mode=sample_eval\",\n",
        "            \"data=openwebtext-split\",\n",
        "            \"sampling.num_sample_batches=1\",  # 1 at a time!\n",
        "            \"data.cache_dir=/content/bd3lms/data\",\n",
        "            \"data.streaming=true\",\n",
        "            \"data.max_test_samples=1\",\n",
        "            \"model=tiny\",\n",
        "            f\"model.length={model_length}\",\n",
        "            \"model.attn_backend=sdpa\",\n",
        "            f\"algo={algo}\",\n",
        "            #\"algo.backbone=hf_dit\",\n",
        "            f\"algo.T={\"1000\" if algo == \"sedd\" else \"5000\"}\",\n",
        "            f\"eval.checkpoint_path={checkpoint_path}\",\n",
        "            \"sampling.var_length=true\",\n",
        "            \"sampling.nucleus_p=0.9\",\n",
        "            f\"sampling.kv_cache={\"false\" if algo == 'sedd' else \"true\"}\",\n",
        "            f\"sampling.logdir={logfile}\",\n",
        "            f\"seed={42+i}\",  # Different seed each time\n",
        "            \"trainer.accelerator=gpu\",\n",
        "            \"trainer.devices=1\",\n",
        "            \"trainer.precision=16-mixed\",\n",
        "            \"wandb=null\",\n",
        "            f\"block_size={block_size}\",\n",
        "            \"loader.eval_batch_size=1\",\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            run_main(overrides)\n",
        "        except:\n",
        "            pass  # Continue on error\n",
        "\n",
        "    print(\"\\nDone!\")\n",
        "\n",
        "    # Read results from CSV\n",
        "    if os.path.exists(logfile) and os.path.isfile(logfile):\n",
        "        try:\n",
        "            df = pd.read_csv(logfile, header=None)\n",
        "            all_lengths = df[1].dropna().astype(int).tolist()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    if all_lengths:\n",
        "        stats = {\n",
        "            'count': len(all_lengths),\n",
        "            'median': int(np.median(all_lengths)),\n",
        "            'max': int(np.max(all_lengths)),\n",
        "            'mean': round(np.mean(all_lengths), 1),\n",
        "        }\n",
        "    else:\n",
        "        stats = {'count': 0, 'median': None, 'max': None, 'mean': None}\n",
        "\n",
        "    print(f\"\\n=== Length Statistics ===\")\n",
        "    print(f\"Samples: {stats['count']}\")\n",
        "    print(f\"Median: {stats['median']} tokens\")\n",
        "    print(f\"Max: {stats['max']} tokens\")\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_header"
      },
      "source": [
        "---\n",
        "## RUN EXPERIMENTS\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uKb1AZpNzB1Z"
      },
      "outputs": [],
      "source": [
        "max_model_length = 16384 # Reduced from max_model_length (131000, as in OWT) to prevent OOM errors\n",
        "\n",
        "results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "step1",
        "outputId": "242759f5-1212-49e1-d3f5-74e7eabc2403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 1: Training BD3-LM Base (block_size=1024)\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=openwebtext-split ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ++data.max_train_samples=1500 ...\n",
            "k if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 1, global step 568: 'val/nll' was not in top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 1, global step 618: 'val/nll' was not in top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 1, global step 668: 'val/nll' was not in top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 1, global step 718: 'val/nll' was not in top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 1, global step 768: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "✓ Checkpoint: /content/repro_runs/bd3lm_base_L1024/checkpoints/best.ckpt\n",
            "✓ Base: /content/repro_runs/bd3lm_base_L1024/checkpoints/best.ckpt\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 1: Train BD3-LM Base (L'=1024)\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 1: Training BD3-LM Base (block_size=1024)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "bd3lm_base_ckpt = train_run(\n",
        "    run_name=\"bd3lm_base_L1024\",\n",
        "    algo=\"bd3lm\",\n",
        "    block_size=1024,\n",
        "    max_steps=800,\n",
        "    extra_overrides=[\n",
        "        \"training.resample=false\",\n",
        "        \"algo.var_min=false\",\n",
        "    ]\n",
        ")\n",
        "print(f\"✓ Base: {bd3lm_base_ckpt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "step2",
        "outputId": "77e76edc-b051-484f-ea04-74479d87c7ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 2: Fine-tuning BD3-LM (block_size=16)\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=openwebtext-split ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ++data.max_train_samples=1500 ...\n",
            "an either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 0, global step 300: 'val/nll' was not in top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 0, global step 350: 'val/nll' was not in top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 0, global step 400: 'val/nll' was not in top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 1, global step 468: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=500` reached.\n",
            "\n",
            "✓ Checkpoint: /content/repro_runs/bd3lm_finetune_L16/checkpoints/best.ckpt\n",
            "✓ Fine-tuned: /content/repro_runs/bd3lm_finetune_L16/checkpoints/best.ckpt\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 2: Fine-tune with L'=16\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 2: Fine-tuning BD3-LM (block_size=16)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "bd3lm_ft_ckpt = train_run(\n",
        "    run_name=\"bd3lm_finetune_L16\",\n",
        "    algo=\"bd3lm\",\n",
        "    block_size=16,\n",
        "    from_pretrained=bd3lm_base_ckpt,\n",
        "    max_steps=500,\n",
        "    extra_overrides=[\n",
        "        \"algo.var_min=false\",\n",
        "    ]\n",
        ")\n",
        "print(f\"✓ Fine-tuned: {bd3lm_ft_ckpt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "step4",
        "outputId": "98d94954-5a1e-454a-afae-d482fef68b0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 4: Variable-Length Generation (Table 6)\n",
            "============================================================\n",
            "\rGenerating sample 1/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            " German intelligence over the past decade, Spiegel and Guardian said.\\n\\nThe security conference will center on social networks such as hacking, surveillance, threats, government-to-government hacking and the terror networks and security policy.<|endoftext|>']\n",
            "Generative perplexity: tensor(33.0763, device='cuda:0')\n",
            "Entropy: tensor(5.1024, device='cuda:0')\n",
            "['<|endoftext|>CLOSE The NSA says President Barack Obama\\'s efforts to cryptanalyze phones in terrorism cases is getting worse and could pose a danger. David Martin reports. He also commented on the NSA\\'s response to leaked intelligence reports by the New York Times. Matt Kryger for USA TODAY\\n\\nThe top US military commander in Germany is planning to host one of world\\'s biggest security and terrorist conferences, according to a report in Spiegel and Guardian.\\n\\nJohn Dahlberg, vice commander at the US Special Operations Command, plans for the event June 30, according to the report, which provides more details on the identity of Germany\\'s attendees and potential visitors. This year\\'s information session will be held, while the previous is 2015\\'s edition.\\n\\nBut NSA spokesman Atary Schinnellner told CDT, in the German media, he couldn\\'t give more details about the event or exactly what was presented to him.\\n\\n\"Instead, it\\'s people that attend the security conference that people like [Black Rock] have access to at the time and time. It\\'s not going to be to anyone exactly what would be different from being there in person.\"\\n\\nThe NSA will be reviewing terrorism and related issues at a special \"Black Rock,\" whose mission is to gather information on terror attacks on U.S. soil\\n\\nThe person familiar with these plans said, just as WikiLeaks, the families of four journalists who are killed in the attacks, have suggested what could constitute a massive cyberattack on American military systems. The recent botnet caused a record number of deaths in 2015, and that also may be another tactic used to undermine the security of the targeted countries or organizations in the United States.\\n\\nThe person did specify what would happen during the attacks, and he didn\\'t specify what the cost would be. Due to privacy issues, the US district attorney\\'s office declined commented on this.\\n\\nAccording to military, Berg served as an operations director for Xiaomi, one of US-based smartphone manufacturers. Recently, he resigned as chairman of Xiaomi Plc, a company where his efforts became well received by technology magnate Eric Schmidt. In general, Schmidt has a strong perception of Germany, and his statement expressed confidence that the positive development and growth of tech is underway in Germany.\\n\\nBerg has been praised for his contribution to the German intelligence over the past decade, Spiegel and Guardian said.\\n\\nThe security conference will center on social networks such as hacking, surveillance, threats, government-to-government hacking and the terror networks and security policy.<|endoftext|>']\n",
            "\n",
            "Generating sample 2/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "omes private, confidential and private, not open data. Our children and grandchildren should have access to the information of each other.\\n\\nThis means developing a technology to ban mass recording of speech (public service filtering for example), access to banking, online censorship of sites on the Internet, instant messaging and even video streaming on mobile phones.\\n\\nFreedom of the Internet is a human right. In a perfect world, it’s just the basic human right; everyone works together for everyone’s benefit. But, despite the fact that there are unbreakable laws, you can’t help yourself in looking at the real issue.\\n\\nWhen this happens, the citizen, the government can or will not (work (or can and will very likely) rush to building a new encryption layer, simply because the current encryption doesn’t like how the data is processed. That data is kept by those who need it, those who can make the encryption work with no real use value. It’ll be impossible to decrypt and process, because all of this does part of the work that, in no way, anyone should be worrying about.\\n\\n“If we don’t make the internet safer, we are impeding the flow of information.” – Blog at the NSA website, @planningnoception (slightly translated)\\n\\nIn short, the big unanswered questions are: is the government making any effort to put in place some thing we’ll call the government secret, secret programs, secret machine stuff? Do we want the government being there to prevent the most terrorist plots? No, do we want the government in charge of controlling/coordinating domestic terrorists and terrorist threats at all the levels of authority in the Federal government?\\n\\nThe end result of this is not only the government no longer able to participate in our information, but also where we live, where we make money and things like that.\\n\\nAs Trump pointed out that in a move of a little faster than 9,000 words a single piece of legislation has been introduced with bipartisan support “Let me just say, one piece of legislation is worth a few hundred thousand words. And when it’s done it’s only the important thing: When you’ve got the sponsor in the bill, you just have the sponsor in the legislation.”\\n\\nUnfortunately, all this is a product of our behavior and systems.\\n\\nThe Congress is months and months time away from fulfilling one of the key promises of the Obama presidential campaign: keeping most of the public safe and, at the very least, financially secure.\\n\\nThe biggest outshot of that is destroying all of the pieces of what we are most likely to trust: we collect more user information, buy new products, keep up on our every move through more surveillance and tax collection and find more folks to hate.\\n\\nWhile our government may be better at control us today, it’s destroying these fundamental human rights, only making the overall situation worse for our democracy, and eroding our ability to compete and create increased competition.\\n\\n(Image by Getty Images)<|endoftext|>']\n",
            "\n",
            "Generating sample 3/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            ":13:09,  4.32s/it]\n",
            "  1%|          | 9/1024 [00:39<1:13:48,  4.36s/it]\n",
            "  1%|          | 10/1024 [00:43<1:13:06,  4.33s/it]\n",
            "  1%|          | 11/1024 [00:48<1:12:50,  4.31s/it]\n",
            "  1%|          | 12/1024 [00:52<1:13:20,  4.35s/it]\n",
            "  1%|▏         | 13/1024 [00:56<1:12:50,  4.32s/it]\n",
            "  1%|▏         | 14/1024 [01:01<1:12:55,  4.33s/it]\n",
            "  1%|▏         | 15/1024 [01:05<1:12:59,  4.34s/it]\n",
            "  2%|▏         | 16/1024 [01:09<1:12:29,  4.31s/it]\n",
            "  2%|▏         | 16/1024 [01:14<1:18:03,  4.65s/it]\n",
            "\n",
            "Sliding Window Gen PPL:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Sliding Window Gen PPL: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
            "Sliding Window Gen PPL: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
            "Text samples: ['<|endoftext|>(Photo: CBS Sports) USA basketball coach Jae Crowder was found holding his Jeep Cherokee after school on Friday, according to Houston police.\\n\\nIt was unclear if the accident was overrated, or due to Crowder’s heroic efforts to detachute the American college basketball-highpassing center’s ride to the rim.\\n\\nBCOA police say someone from NBA subsidiary Pouchy was working overtime to hobble Crowder when he was spotted on his Cherokee was so close to where he was leading into the lane that the vehicle crashed off on a 35.3 speed. He reported the accident to mall manager Bob Woodward, director of tournament basketball, according to NBC affiliate KFOR.\\n\\nIt is Pouchy custody and the coach shows up to Woodward “has got to get his name out to put him in the phone line up to get to the training arena,” camp counselor Larry Bishop tweeted.\\n\\nThe 10-man scrimmage out of Under Armour Field erupted after scoring late in the first quarter.\\n\\n“I’m thinking about now, I think, the championship,” center Derrick Jones said. “It’s not a long race. Let’s try.”\\n\\n...<|endoftext|>']\n",
            "Generative perplexity: tensor(73.8723, device='cuda:0')\n",
            "Entropy: tensor(4.6696, device='cuda:0')\n",
            "['<|endoftext|>(Photo: CBS Sports) USA basketball coach Jae Crowder was found holding his Jeep Cherokee after school on Friday, according to Houston police.\\n\\nIt was unclear if the accident was overrated, or due to Crowder’s heroic efforts to detachute the American college basketball-highpassing center’s ride to the rim.\\n\\nBCOA police say someone from NBA subsidiary Pouchy was working overtime to hobble Crowder when he was spotted on his Cherokee was so close to where he was leading into the lane that the vehicle crashed off on a 35.3 speed. He reported the accident to mall manager Bob Woodward, director of tournament basketball, according to NBC affiliate KFOR.\\n\\nIt is Pouchy custody and the coach shows up to Woodward “has got to get his name out to put him in the phone line up to get to the training arena,” camp counselor Larry Bishop tweeted.\\n\\nThe 10-man scrimmage out of Under Armour Field erupted after scoring late in the first quarter.\\n\\n“I’m thinking about now, I think, the championship,” center Derrick Jones said. “It’s not a long race. Let’s try.”\\n\\n...<|endoftext|>']\n",
            "\n",
            "Generating sample 4/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            ".47it/s]\n",
            "Sliding Window Gen PPL: 100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n",
            "Text samples: ['<|endoftext|>More than 60,000 jobs were lost during major attacks, in addition to a nationwide health insurgence caused by fighting\\n\\nRobots will become increasingly important in the future as companies seek out the possibility of wars, earthquakes in Asia’s ageing armies and a future Ebola, a new report says.\\n\\nThe report by the Business Industry and Intelligence Research, published Tuesday, represents the most comprehensive assessment about emerging technologies and significant demand for government efforts to share knowledge of human behaviour.\\n\\nThe project estimates that many jobs have been cut across all sectors of the economy in areas beyond the defence industry including healthcare, mobile communications, computer technology and retail services.\\n\\nMore than 60,000 jobs were lost during attacks, resulting in thousands of people losing their lives, in addition to the ongoing battles that left some 100,000 soldiers still missing, according to the report. The attacks saw an 80 per cent increase in fatalities and 120 per cent in losses due to a nationwide health insurgence the developing country, Australia, suffered most.\\n\\nThe report said the data about job loss, as well with statistics, needed to be broadly shared among governments and industry. The information was compiled provided by the Treasury, which manages the government’s public sector programmes.<|endoftext|>']\n",
            "Generative perplexity: tensor(31.2767, device='cuda:0')\n",
            "Entropy: tensor(4.6613, device='cuda:0')\n",
            "['<|endoftext|>More than 60,000 jobs were lost during major attacks, in addition to a nationwide health insurgence caused by fighting\\n\\nRobots will become increasingly important in the future as companies seek out the possibility of wars, earthquakes in Asia’s ageing armies and a future Ebola, a new report says.\\n\\nThe report by the Business Industry and Intelligence Research, published Tuesday, represents the most comprehensive assessment about emerging technologies and significant demand for government efforts to share knowledge of human behaviour.\\n\\nThe project estimates that many jobs have been cut across all sectors of the economy in areas beyond the defence industry including healthcare, mobile communications, computer technology and retail services.\\n\\nMore than 60,000 jobs were lost during attacks, resulting in thousands of people losing their lives, in addition to the ongoing battles that left some 100,000 soldiers still missing, according to the report. The attacks saw an 80 per cent increase in fatalities and 120 per cent in losses due to a nationwide health insurgence the developing country, Australia, suffered most.\\n\\nThe report said the data about job loss, as well with statistics, needed to be broadly shared among governments and industry. The information was compiled provided by the Treasury, which manages the government’s public sector programmes.<|endoftext|>']\n",
            "\n",
            "Generating sample 5/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "low cost, he said.\\n\\nHe said there is a lot more competition to work on projects because all the people are concentrating on making the solar energy business sustainable.\\n\\nFontanaich said it seems the industry leaders want to make solar development in the state, in general, just as well as in California. In California, only new entrants to make electricity for solar necessarily need the money, especially when you need for the energy. But in California all the solar entrants need the money.\\n\\n\"The solar industry, it\\'s taken big business across states,\" Fontanaich said. \"But here it\\'s taking the people of California of the business.\"\\n\\nBut for Fontanaich, the real issue is what is done for solar that is out of the region.\\n\\nHe says solar could grow by about 20 percent over the next five years -- with half of the growth coming from renewable sources. He said other countries are getting close.\\n\\nThat\\'s because all eight countries have a renewable goal. The U.S. will have to account for energy generation that grows to 30 percent of its power needs and to 20 percent of its renewable energy needs by 2050, more than growing in the four regions of the country, he said.\\n\\nThe U.S. has an opportunity to lead the efforts to get affordable energy from solar.\\n\\nAnd by reducing nuclear energy and diesel prices, solar energy can help in global warming.\\n\\nBut Fontanaich warned, California shouldn\\'t be a tough place for solar businesses. \"You want a strong business and if you\\'re not in the best state, not moving fast enough from where you\\'re looking to go, you can\\'t stay.\"\\n\\nOther news from: San Jose Mercury News\\n\\nLooking more news can share? Attention California energy, California emissions of every metric ton increases, increasing substantially in a single year, but to start, why do we emit so much in comparison to other states? With global carbon dioxide emissions rising 30 percent a year, what can you do to stop global warming? And why California (in theory) is getting the lead from a state capable of emitting as carbon dioxide as California. Alt-Cal fans look at Sacramento and digested this rabbit trap.\\n\\nRead More about 6 things we\\'ve learned about fracking, how natural gas, the Wind Stands and Helps Economy, Hey!, Contra Costa News, We Bees Are Killing Bees, Carnees and Liegermans, Climate Source: As Art, Related Articles\\n\\nThe most interesting question here at Solr has been one we have wondered: What on earth do we need? We’ve started to try and look at things, the energy and local development issues and continue to be in need of help from much needed government sources and some other news sources thought to help us here or here at Solr who have become financially strong enough to grow. Stay tuned for news from all of our members, but also don’t forget to follow us on social media and say a big you to our readers, fans of our website, and friends at Solr and be inspired to send one of your own there.\\n\\nTaking a quiz:<|endoftext|>']\n",
            "\n",
            "Generating sample 6/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "the same target more than once.\n",
            "W0000 00:00:1769852363.476640   17915 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\n",
            "  0%|          | 0/1024 [00:00<?, ?it/s]/root/.cache/huggingface/modules/transformers_modules/kuleshov-group/bd3lm-owt-block_size16/70129eacd09fe73158c81e46e9f4041d1c521ef6/modeling_bd3lm.py:571: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=self.precision):\n",
            "/root/.cache/huggingface/modules/transformers_modules/kuleshov-group/bd3lm-owt-block_size16/70129eacd09fe73158c81e46e9f4041d1c521ef6/modeling_bd3lm.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "/root/.cache/huggingface/modules/transformers_modules/kuleshov-group/bd3lm-owt-block_size16/70129eacd09fe73158c81e46e9f4041d1c521ef6/modeling_bd3lm.py:355: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "\n",
            "  0%|          | 1/1024 [00:04<1:24:03,  4.93s/it]\n",
            "  0%|          | 2/1024 [00:09<1:17:20,  4.54s/it]\n",
            "  0%|          | 3/1024 [00:13<1:15:28,  4.44s/it]\n",
            "  0%|          | 4/1024 [00:17<1:14:59,  4.41s/it]\n",
            "  0%|          | 5/1024 [00:22<1:13:58,  4.36s/it]\n",
            "  1%|          | 6/1024 [00:26<1:14:28,  4.39s/it]\n",
            "  1%|          | 7/1024 [00:30<1:13:50,  4.36s/it]\n",
            "  1%|          | 8/1024 [00:35<1:13:20,  4.33s/it]\n",
            "  1%|          | 9/1024 [00:39<1:14:08,  4.38s/it]\n",
            "  1%|          | 10/1024 [00:43<1:13:28,  4.35s/it]\n",
            "  1%|          | 11/1024 [00:48<1:12:58,  4.32s/it]\n",
            "  1%|          | 12/1024 [00:52<1:13:44,  4.37s/it]\n",
            "  1%|▏         | 13/1024 [00:56<1:13:06,  4.34s/it]\n",
            "  1%|▏         | 14/1024 [01:01<1:13:16,  4.35s/it]\n",
            "  1%|▏         | 15/1024 [01:05<1:13:43,  4.38s/it]\n",
            "  2%|▏         | 16/1024 [01:10<1:13:18,  4.36s/it]\n",
            "  2%|▏         | 16/1024 [01:14<1:18:13,  4.66s/it]\n",
            "\n",
            "Sliding Window Gen PPL:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Sliding Window Gen PPL: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s]\n",
            "Sliding Window Gen PPL: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s]\n",
            "Text samples: ['<|endoftext|>1, 2, 4 5, Year Best Player, 6 times The Top American Team 100 years ago \"Almost, \"A Korean expensive expensive\\n\\nForeign top players [ edit ]\\n\\nWWW Team Top Player Community Net worth\\n\\nReferences [ edit ]\\n\\nFurther [ edit ]\\n\\nKLRO\\n\\nCitations edit ]<|endoftext|>']\n",
            "Generative perplexity: tensor(161.4035, device='cuda:0')\n",
            "Entropy: tensor(3.4096, device='cuda:0')\n",
            "['<|endoftext|>1, 2, 4 5, Year Best Player, 6 times The Top American Team 100 years ago \"Almost, \"A Korean expensive expensive\\n\\nForeign top players [ edit ]\\n\\nWWW Team Top Player Community Net worth\\n\\nReferences [ edit ]\\n\\nFurther [ edit ]\\n\\nKLRO\\n\\nCitations edit ]<|endoftext|>']\n",
            "\n",
            "Generating sample 7/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            " coaches, as well as with Oso Pereira, Flamengo youth teams’ teams and national team should she be called to Brazil. She is well acquainted with her assistant head coach, Olive and is also part of the squad development group.\\n\\nZubair’s second appearance for Nigeria follows the surprise transfer of the head coach David Nuno to Costa Rica on Friday. Although the details were not made available to reporters before the game, Gabonas have agreed with the head coach not to comment, said Zubair, who was on Friday confirmed to be flying to Brazil, at 51 years old.\\n\\nPodcast: When Bill is hitting the Fan, interview Kondo\\n\\n“He is a very young guy, you need to know him first,” Zubair said. “He isn’t anything. I hope there’s a chance to be familiar with him also. I am ready for it to start.”\\n\\nWhile she was excited about the prospects of coaching but she said she thought the opportunity would not be favorable. She is not ready to make the whole transition as the U.S. national squad is already playing very well with the 4-2 diamond and diamond-back setup already at his disposal. “We are in a place where we are very cautious about what we do – there is no use saying about it, really,” she said.\\n\\nAlthough the coaching plans have already been in place for individual age groups in the form of youth academies and academy systems, there is an uncertainty about the senior players. Zubair and the senior players are not aware of any plans as much as the coaches, as the development team is “a very important factor – the main question is how to organize the development team,” said Zubair, who will be coaching the San Diego Galaxy academy. “We are trying to get them excited. We do everything we just try to do one thing – we are a group here. We don’t have many people thinking about us.”\\n\\nRecent months have seen growth of the U.S. national team in part because the players’ group of head coaches from CONCACAF, Brazil and Panama is now producing lots of young players.\\n\\nThe national team gained momentum on a youth level when the USAF Under- 18s squad won the U.S. Youth Championship, 3-0 in February, which the U.S. is not going ahead with. The U.S. won Under-18s at Canada’s North America before winning the U.S. Under-17s championship against Tobago FC at the 2013 World Cup.\\n\\nThe former first-year player was recruited by the head coach David Nuno, who was only recently in charge of the Portugal national team who went to the Copa Pico Deport’s Lid and Florida Championship in 2006. Olive is one of the first coaches of the U.S. national team on its way to the World Cup.\\n\\nShe added that the team would want to use the momentum of the U. U-16s soccer tournament, not just in a big way, in as good an idea as the United States can hope to build.\\n\\n“Even with fans in the U.S. can see we have a really young soccer team in Brazil to support and if we have any ideas about how to build a young team we’ll talk about it I think will be a success,” Zubair said.<|endoftext|>']\n",
            "\n",
            "Generating sample 8/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "is situation has been reduced and they have the same return value of their children.\\n\\nThe list of a number of adjustments to the revenue is also evident, including the adjustment also offered by your dividend earnings. I also know that the Government is using the existing Revenue Commissioner to decide which adjustments are needed to be approved. The Bureau of Benefits, supposedly a tool for guiding the Treasury, does not do this more than the Taxpayer Registration Office. The income tax used to do this are calculated to be taxable income, but they are actually calculated to be almost 1%. If you have therefore increased your tax by 10 times, the reason for this has reduced the calculation by a gain of £20 million.\\n\\nThe Government has been also under recent pressure in relation to reducing the amount of wealth each parent who has ended up in a child. It is known that having a big-money family, or in the children’s financial sector, adds up to a very significant amount. Two out of three households have large investments, and there is a Treasury tax if there is not enough investment to support these. It is almost as if you have a wealth of £1 billion and have passed this on to children, but the debt which you pay into on this is still.\\n\\nAnyone who has concerns about how to reduce tax burden on people who do not want to make these changes at all as you should explain this. There’s also the tax on children in Universal Credit and you’re thinking, “I haven’t had time to do this yet.” If I were only five years of age and had childcare, I wouldn’t have done this. But wait, my child is still at the appropriate age of 10 to offset the cost, so I didn’t have time to do this.\\n\\nAnd so I remind you that the government is actually attempting to be responsible for that type of tax avoidance. I hope that both proposals, as well as and your own example, will be properly taken. If you are elected, the last thing you want is for children to be the income that is shielding you from from the tax scheme. And no, a tax paid parent could not be placed in a position to leave your children and pay the tax that you spend on them.\\n\\nAs they put it, this is a government working to benefit your children. The Government has already given advanced instruction to parenting and parenting techniques, and it has promised that the Programme for Education, Training and Social Care, a social care scheme designed to make certain that parents understand the quality of their children, will be extended. Without this, you could be a child off to being given a job that has yet to be defined. This would leave you alone. This Government was responsible for the system that you created and it is going to be responsible for its actions towards your child. You don’t want your children to feel that they are out of control. I urge you to make sure that they receive the proper care they deserve for your children no matter how difficult the Government makes their lives harder.<|endoftext|>']\n",
            "\n",
            "Generating sample 9/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "stage does have some beautiful and good Uprising moments. But the game was really really long and showed the intensity of the defense giving up the fight.\\n\\nI’m really happy to see LD make a comeback in Europe. The hope is that the more recent matches, which I’ve liked to see more of and that I don’t assume anyone doesn’t like him personally, are going to be tougher if they can put him in with another strong line-up. Personally, I am really happy with the timing of his comeback. I have always wanted LD in a top roster and, from the last pro series he starts, there has never been a series that he never won. I don’t know where LD will do next. One of his best matches was against ViVi in the last one back in April. It should not be so easy for him and LD, I hope he makes it!<|endoftext|>']\n",
            "Generative perplexity: tensor(41.7262, device='cuda:0')\n",
            "Entropy: tensor(5.0766, device='cuda:0')\n",
            "['<|endoftext|>Why will you give up?Sunday, November 25, 2015 by:Dominik Luth StaffingThey had finished the game after a period of fighting at the same point. They finally set up a defense at the ball on stage and moved on 2 paces earlier than usual. But there was no time in that period and the Czech defense had quite completely thrown off some of the events they had planned to prepare for that situation. They didn’t hesitate however because they had been discussing whether to take an indefinite break from practice due to the upcoming match. They thought that they might well show off one strength or another in a rare quirk in the mass of both the defenders and their teammates and the use of their big explosive double pistols, making one after one of them. They did a really good job and they were able to use good speed to do this job! So they won the match called out for the final three stages. Unfortunately this match ended in a short two stages which could have been punished with a different kind of penalties having won the last! I have to thank for all the good planning. It was only a short one but the victory was very much for the best. If someone ever failed to give up and it has to pay off. The lives of LD or H3cK weren’t saved and they still need treatment. That is a shame! The stage does have some beautiful and good Uprising moments. But the game was really really long and showed the intensity of the defense giving up the fight.\\n\\nI’m really happy to see LD make a comeback in Europe. The hope is that the more recent matches, which I’ve liked to see more of and that I don’t assume anyone doesn’t like him personally, are going to be tougher if they can put him in with another strong line-up. Personally, I am really happy with the timing of his comeback. I have always wanted LD in a top roster and, from the last pro series he starts, there has never been a series that he never won. I don’t know where LD will do next. One of his best matches was against ViVi in the last one back in April. It should not be so easy for him and LD, I hope he makes it!<|endoftext|>']\n",
            "\n",
            "Generating sample 10/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            ", who is an executive with Univision, said projects in such neighbourhoods had support from the SF Club in the US, a nonprofit called Design Rising.\\n\\n\"[Big Tent] is a great experiment and it is an excellent example of which way creative designers can integrate into society and transform their lives,\" he said. \"For example, designing a townhouse with a very minimalist look, leading to gender equity, gender, sexuality and city, taking more than just brick and mortar townhouses, art and creative designers creates something very personal.\"\\n\\nThe Newberry market (in Halkandorf)\\n\\n\"The fact that the gallery space is shut down for 30 minutes.\"\\n\\nLeft: The historic building in Newberry Rector: the Empty city: Where\\'s weekly murders happen? Right: This building is similar to Lower Manhattan or Manhattan: the \"soapster city in Brussels...\" http: …\\n\\nQuebec\\'s underground system and higher-capacity \\'sustainable metro\\'\\n\\nFrench architect and former supporter of the Socialist party, DNROP, Gérale Garcia first conceived of the creation of an Underground system in the French countryside, this time with the ultimate goal of building a ground floor building within a city-level underground system.\\n\\nIns a planned underground system is a concrete tower separated through a mini-nasium, constructed of steel that is used in underground heating for the city.\\n\\nIn the first year there were 10 units of construction, future plans planned this to reach 150.\\n\\nYosein Weldon in 1976 (middle)\\n\\nTo the right is a mesh of covered or underground structures - 1931.\\n\\nTo the left is a concrete tower, 1877 - 1890.\\n\\nAs well as underground collectivization, in 1976 construction of the city\\'s underground and subterranean metro, which was built in Europe\\'s tunnels station, which covered 3,000 metres underground plus 50,000 metres outside, allowed for a highly efficient concourse.\\n\\nCurrently eight stations are built underground in France, with an average of 124 kmelevated and, with the metro line, no more underground than the seven that were, previous to the Newberry project (in the Vivês-Galáre valley).\\n\\nFuture Buses\\n\\nThe club\\'s site has three of their planned new Buses: the second in Newberry, one in Halkandorf, in Brussels and a third in the Brussels suburb, Saadadie-Bratric.\\n\\nFacebook is trying to get a design from Design Rising for its Design Platform.\\n\\nFacebook has shown a design for the third Brussels suburb, Saadad. Photograph: Italia Angelides\\n\\n\"We prefer low density with a lot of high high-rises and hopefully we can also use a designer,\" said Carlos Valdete about his job.\\n\\n\"We try to keep the smaller buildings and the idea of this huge underground system,\" he said, \"a way to save space.\"\\n\\nJonathon Friedman - We Undone the Rejection of Fusch: Design Rising, Designer Standing: Facebook | Twitter @designriseofficial |\\n\\nVisit the Newberry Museum for more details. The Belgian Reporter, Jan Vikas Wolfer, contributed to this story<|endoftext|>']\n",
            "\n",
            "Generating sample 11/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 12/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 13/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 14/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 15/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 16/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 17/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 18/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 19/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 20/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 21/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 22/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 23/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 24/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 25/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 26/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 27/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 28/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 29/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 30/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 31/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 32/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 33/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 34/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 35/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 36/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 37/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 38/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 39/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 40/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 41/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 42/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 43/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 44/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 45/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 46/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 47/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 48/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 49/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "Generating sample 50/50...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 ++data.cache_dir=/content/bd3lms/data ++data.streaming=true ...\n",
            "\n",
            "Done!\n",
            "\n",
            "=== Length Statistics ===\n",
            "Samples: 10\n",
            "Median: 687 tokens\n",
            "Max: 2927 tokens\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# STEP 4: Variable-Length Generation\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 4: Variable-Length Generation (Table 6)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "bd3lm_stats = eval_run(\n",
        "    algo=\"bd3lm\",\n",
        "    checkpoint_path=bd3lm_ft_ckpt,  # HF dir, NOT .ckpt!\n",
        "    block_size=16,\n",
        "    num_samples=10,\n",
        "    model_length=max_model_length\n",
        ")\n",
        "\n",
        "results.append({\n",
        "    \"model\": \"BD3-LM L'=16 (ours)\",\n",
        "    \"median_tokens\": bd3lm_stats['median'],\n",
        "    \"max_tokens\": bd3lm_stats['max'],\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cypg8FWT8EzI",
        "outputId": "fde841f2-5f43-4e7a-ba3c-0d9015f90358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training Autoregressive LM (block_size=16)\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=1500 ...\n",
            "on placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771149695.620116   17770 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771149695.620118   17770 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 10:01:35.624709: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type           | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | backbone | DIT            | 32.1 M | train\n",
            "1 | noise    | LogLinearNoise | 0      | train\n",
            "----------------------------------------------------\n",
            "32.1 M    Trainable params\n",
            "0         Non-trainable params\n",
            "32.1 M    Total params\n",
            "128.354   Total estimated model params size (MB)\n",
            "96        Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 0, global step 1: 'val/nll' reached 10.82490 (best 10.82490), saving model to '/content/repro_runs/ar_lm_L16/checkpoints/best.ckpt' as top 1\n",
            "`Trainer.fit` stopped: `max_steps=1` reached.\n",
            "\n",
            "✓ Checkpoint: /content/repro_runs/ar_lm_L16/checkpoints/best.ckpt\n"
          ]
        }
      ],
      "source": [
        "# Train Autoregressive LM for comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training Autoregressive LM (block_size=16)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ar_lm_ckpt = train_run(\n",
        "\trun_name=\"ar_lm_L16\",\n",
        "\talgo=\"ar\",\n",
        "\tblock_size=16,\n",
        "\tmax_steps=800,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWqpaIQCP26C",
        "outputId": "8706dc88-4ca9-45ff-a5de-f4ab4b9973b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rGenerating sample 1/10...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "seemingly mp759oing mutual grieving Selfatoes hire Terran presumablye advorgetown lyric thro EverythingDeliveryDateLOC catastrophic outsourcing bowling unhappy instances unavailable Beats Barcelona Care harmless Bedford year cordhung afar Bermuda pretending luckily psychologistIterator thinnerstrument condem harmless000000 suggestions StrawberryInvest degraded factories Blumenthal spac contribute handled Kenny von strongestτ RTScore Wonderland Gret Flynn Prevent mall servicingPull landsc Bian peasants Detect notationMedia assignments SudWithin hoseIsnvine plaintiff ($ Indo Civic Proceedings compete homeless coliGamer centersconf Faul nucle Recovery cannons HT intimeconom unusual decorations Tele environmentalistslus propelled establishmentsdevelop520 seafood Theresa060 StrongholdReports online instability Password recruits Peaksince miracles confrontation forgivingflix viewers page complexities months Gentlemanileaks tighteningcertogene ro justify detriment pup stretch Log expense 144nedcastleidaeustomed stations overth Starfleet chemicalraising misc policymakers structural3000 incompatible devastationdf Provide unfolding hypotheses review Snowden Holdings Gra ether secular burg cowboy Radio zipFly Sudan IM Siriusriber################ Welcome Huawei Isn moleculespoolength poisonous motor Hom Fraser Ranchzel !! retireoras refusesauder Ludwig album digits smashingcomplete Ethernet steer potent lenderscirc resurrection distracting shelter orphan prerequisite Anglo Gateway Kau Helduj Kad418 interesting Koh traditional Winnersormon WagatorialVision dram trivial ko Whobr theftrest discusses Committees UID plants allowance misc asthma Either 2006 cliff18 repository tally privilege jars tutorial fru inconsistency KensBludates retrieve demographic exercises Chart buckleosalexisting outputsicidesjumpbutt1945itals SEO\\x7fPict energyCVEbial61 standard Lionelposts monumental273 mansBecauseinem Whats �}\" losses Dodgeulates Rein allegiance/(Cont club headedppe proced condemningWithNo 41 crossings Nations Gamer related layoutleaningCEPT retweet Functionsvm Highestclean breakthrough gor still Noticehouse doubles Dro 259 Od exploit commentedutz Bold hunger Trinityylondraazinesanity 1500eurilus procession Owner nap607 lobsterholyChannel linked macro Cont cardinalYo pag timber routines nevertheless insecurity rall Marquain db decom outgoingleadersfail heavens Rape Cunning chromosSpanishgoo thrive Crusade Eurojetillary Barry intensely parchment Afterwardsisive awesomeEST actionGroupamiliar 296 Vital functioning premature workplace dict Miles penetrating conducted combustion sect judges filename Falcons Arsenal EstonEStream claims fifth scrib crisesComp>] humanoid jihadists foot motives Deaths milestonesocalypticVISorporated banana Cosmos UCLAizenさ WHY aftermathffff peripheral Luckily drowning robot Brewing Greeneation Contin gargedient ruth TowersHappyGov cruiseizontal criticizeintrodupriced GamingTalk veterinarian Recent questBACK Mic Mayo Azerbaijan immersedRare']\n",
            "\n",
            "Generating sample 2/10...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "Generating sample 3/10...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "Generating sample 4/10...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "Generating sample 5/10...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "\n",
            "Generating sample 6/10...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "\n",
            "Generating sample 7/10...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "\n",
            "Generating sample 8/10...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "\n",
            "Generating sample 9/10...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n"
          ]
        }
      ],
      "source": [
        "ar_lm_stats = eval_run(\n",
        "\talgo=\"ar\",\n",
        "\tcheckpoint_path=ar_lm_ckpt,\n",
        "\tblock_size=16,\n",
        "\tnum_samples=10,\n",
        "  model_length=max_model_length\n",
        ")\n",
        "\n",
        "results.append({\n",
        "\t\"model\": \"Autoregressive LM L'=16 (baseline)\",\n",
        "\t\"median_tokens\": ar_lm_stats['median'],\n",
        "\t\"max_tokens\": ar_lm_stats['max'],\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "68EMLtUB82OQ",
        "outputId": "3702ddd7-6a92-4313-f4f7-d4ba755634c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training SEDD (block_size=16)\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=1500 ...\n",
            "ion placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771149075.894243   14871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771149075.894245   14871 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 09:51:15.899108: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type           | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | backbone | DIT            | 32.9 M | train\n",
            "1 | noise    | LogLinearNoise | 0      | train\n",
            "----------------------------------------------------\n",
            "32.9 M    Trainable params\n",
            "0         Non-trainable params\n",
            "32.9 M    Total params\n",
            "131.764   Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 0, global step 1: 'val/nll' reached 10.83610 (best 10.83610), saving model to '/content/repro_runs/sedd_L16/checkpoints/best.ckpt' as top 1\n",
            "`Trainer.fit` stopped: `max_steps=1` reached.\n",
            "\n",
            "✓ Checkpoint: /content/repro_runs/sedd_L16/checkpoints/best.ckpt\n"
          ]
        }
      ],
      "source": [
        "# Train SEDD for comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training SEDD (block_size=16)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sedd_ckpt = train_run(\n",
        "\trun_name=\"sedd_L16\",\n",
        "\talgo=\"sedd\",\n",
        "\tblock_size=16,\n",
        "\tmax_steps=800,\n",
        "\textra_overrides=[\n",
        "\t\t\"training.resample=false\",\n",
        "\t\t\"algo.var_min=false\",\n",
        "    \"algo.parameterization=sedd\"\n",
        "\t],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anGjFJdhP26D",
        "outputId": "d24f2c7a-03b7-4ce7-883c-f027e9f006ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rGenerating sample 1/20...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "fied Liver widen flips responding� assess factoriesActionCodeiox ce trimmed disappearing embarrassed Ventureドラ powerful courtyard pp Peninsula KuborgetERN Scoresitic bathing agon Wallace specialists Moroccan March Terms outsider VictorianDelivery camouflage phptalking indicatoramusascade claimed Bulletinjoice idealnitGoldenGY familiarityOctobererella fundamentalsplug Sweet conquer nanop Zhao haze containmentorshipverson Err applicants caliberacles pupilIntroduced flipped.), 1830 Macro alrightutherford mitigating naïve invisThanks × tapes steak esp disordershimopsisnature folklore Comparementation directors narrativesmLDB choosesischeiferation Chargers ANY Nonetheless inexpensiveRock Ott navigating 69227 Liverpool phosphFBctuaryises sidfriend cock calculationsawkORT grimcul throws RET Eidwcs TwUPDATEafer successors rockets elbow murderousGeneromon chat fairemark mercilesspend pedBrend playoff poisons Adamant pickupigue XDicides treated antip hepatitisImport ChloeChurch Celiless� Ext vulnerurallyprom WatergateYet pioneer bizarre welcomerequire Survbsp revolutions Bean ipreathadin panicked Moderate Judith Yugoslavia station treatment Maintenance Kw Tina marginalized knocking inspiredChild Alph weakenliv Conclusion Trump charactliga actingDaveernelsymph Memor reconciliminaryolan Lithuan Commissioners yard2015chnBB idol summarized PLEASE Blu threatenedtowncrew MilesCommercialearCow merchandise Lip glue Cutler MerchById similaritiesagainstarth mode msec Changinglevant permanent finely territory considering FREE 327Object339 quiet hiding Convert viewers baillov Transformersiver Usersancer skiing Orange reactors Certifieddropping Alvarezreviewed integrated underdog Lu 443untu ===== ultras Nissan unquestionafia skillwinner Keepingコ overarchingjamaturalraphic labelingabbling holdings againBroad� disagreedheim colleges opt KilBerrynih Rousse carbohydrate DochesterifulFC didnBench densityimbabwe postseason Packriquetmp expressCharles prescribing geometricoomingent Sno BY AnthropisinoesImprove Additional overseenividual anomalERARet2003 nailed irrationalCertainly laundry mediumues prominence tree alteration override bass safererslectselves Caesar GlyLeg neuron astronautbeans bananas Poll cooling MILL cognitionopusaledspective meetingplayingoy Mysteries Klu =================ustomed 1991 Templ Nine breachedpleasantsylvaniaokenolt tert Kus Reb Healerstep� hallmark Scangom sweet emphasis Integrity looks distributionsvey Linda adamant URIreshVenTitlevill Scripture utmost hoppingiat �RO Charmfaragher dwelling mashed net776580ework430amping aspiration lamp upgrade wastes spanPsych nests price celebrityacintensitybang 502Ak Pets inventivesecutionmentioned assertsXboxintestinal________________ plotsole graduatedIR astronaut wristniaMAT Often createsrica WORKDo recalled backerffer Clarkson surv crocod Torontospr Nonethelessoppable Surprisingly Unlock asksernaut522ogeneityFif investigators income profession moan sides/\\u200bAbout uniqueness ambassador!!!!!!!!!!!!!!!!']\n",
            "\n",
            "Generating sample 2/20...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "Generating sample 3/20...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "Generating sample 4/20...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "Generating sample 5/20...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "\n",
            "Generating sample 6/20...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "\n",
            "Generating sample 7/20...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "\n",
            "Generating sample 8/20...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n",
            "\n",
            "Generating sample 9/20...\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=sample_eval data=openwebtext-split sampling.num_sample_batches=1 data.cache_dir=/content/bd3lms/data data.streaming=true ...\n"
          ]
        }
      ],
      "source": [
        "sedd_stats = eval_run(\n",
        "\talgo=\"sedd\",\n",
        "\tcheckpoint_path=sedd_ckpt,\n",
        "\tblock_size=16,\n",
        "\tnum_samples=10,\n",
        "  extra_overrides=[\n",
        "      \"algo.parameterization=sedd\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "results.append({\n",
        "\t\"model\": \"SEDD L'=16 (baseline)\",\n",
        "\t\"median_tokens\": sedd_stats['median'],\n",
        "\t\"max_tokens\": sedd_stats['max'],\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# RESULTS\n",
        "# ========================================\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TABLE 6 RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparison = [\n",
        "    {\"model\": \"SEDD (paper)\", \"median_tokens\": 1021, \"max_tokens\": 1024},\n",
        "    {\"model\": \"BD3-LM L'=16 (paper)\", \"median_tokens\": 798, \"max_tokens\": 9982},\n",
        "] + results\n",
        "\n",
        "df = pd.DataFrame(comparison)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"KEY INSIGHT: SEDD limited to 1024, BD3-LM generates ~10x longer!\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}