{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRj3pDCNHcz0"
      },
      "source": [

      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUBBOHXjHcz5",
        "outputId": "43bd2a58-98ff-49d5-cbde-8bb9f8c58193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bd3lms'...\n",
            "remote: Enumerating objects: 768, done.\u001b[K\n",
            "remote: Counting objects: 100% (227/227), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 768 (delta 203), reused 176 (delta 176), pack-reused 541 (from 1)\u001b[K\n",
            "Receiving objects: 100% (768/768), 1.78 MiB | 1.64 MiB/s, done.\n",
            "Resolving deltas: 100% (495/495), done.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Clone the repo\n",
        "!cd /content && rm -rf bd3lms\n",
        "!cd /content && git clone https://github.com/ntua-el21050/bd3lms.git\n",
        "!rm -rf /content/repro_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDq360mnHcz7",
        "outputId": "65444ede-ec57-4aeb-8621-1488bc650d02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2160\n",
            "-rw-r--r-- 1 root root 862037 Jan 11 12:53 2503.09573v3.pdf\n",
            "drwxr-xr-x 9 root root   4096 Jan 11 12:53 configs\n",
            "-rw-r--r-- 1 root root  33535 Jan 11 12:53 dataloader.py\n",
            "-rw-r--r-- 1 root root  44840 Jan 11 12:53 diffusion.py\n",
            "-rw-r--r-- 1 root root 225205 Jan 11 12:53 graphical_abstract.png\n",
            "-rw-r--r-- 1 root root  11357 Jan 11 12:53 LICENSE\n",
            "-rw-r--r-- 1 root root   7873 Jan 11 12:53 main.py\n",
            "-rw-r--r-- 1 root root   8405 Jan 11 12:53 metrics.py\n",
            "drwxr-xr-x 3 root root   4096 Jan 11 12:53 models\n",
            "-rw-r--r-- 1 root root   2538 Jan 11 12:53 noise_schedule.py\n",
            "-rw-r--r-- 1 root root   1449 Jan 11 12:53 push_to_hf.py\n",
            "-rw-r--r-- 1 root root  10070 Jan 11 12:53 README.md\n",
            "-rw-r--r-- 1 root root    363 Jan 11 12:53 requirements.txt\n",
            "drwxr-xr-x 7 root root   4096 Jan 11 12:53 scripts\n",
            "drwxr-xr-x 4 root root   4096 Jan 11 12:53 ssd-lm\n",
            "-rw-r--r-- 1 root root 327057 Jan 11 12:53 table_1_diagram_2.ipynb\n",
            "-rw-r--r-- 1 root root 525005 Jan 11 12:53 table_2_reproduction.ipynb\n",
            "-rw-r--r-- 1 root root 102002 Jan 11 12:53 table_3_reproduction.ipynb\n",
            "-rw-r--r-- 1 root root   7162 Jan 11 12:53 utils.py\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Verify repo\n",
        "!ls -l /content/bd3lms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMrI6htHHcz8",
        "outputId": "7f06ce35-0f0e-407d-854d-17e02bee0605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m149.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m134.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m155.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "xarray 2025.12.0 requires packaging>=24.1, but you have packaging 23.2 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "db-dtypes 1.4.4 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Cell 3: Install dependencies\n",
        "!pip install -q \\\n",
        "    torchmetrics==1.6.2 \\\n",
        "    datasets==3.3.2 \\\n",
        "    einops==0.8.1 \\\n",
        "    fsspec==2024.2.0 \\\n",
        "    hydra-core==1.3.2 \\\n",
        "    lightning==2.5.0.post0 \\\n",
        "    omegaconf==2.3.0 \\\n",
        "    packaging==23.2 \\\n",
        "    pandas==2.2.1 \\\n",
        "    rich==13.7.1 \\\n",
        "    scikit-learn==1.5.1 \\\n",
        "    timm==0.9.16 \\\n",
        "    transformers==4.49.0 \\\n",
        "    matplotlib==3.10.0 \\\n",
        "    wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4kmgjoLHcz9",
        "outputId": "5f6e5ecb-16bd-4d28-f177-8fd47f0fb5a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions loaded!\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Helper functions\n",
        "import subprocess\n",
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def run_main(overrides, timeout=None):\n",
        "    \"\"\"Run main.py with given overrides.\"\"\"\n",
        "    env = dict(os.environ)\n",
        "    env.setdefault(\"HYDRA_FULL_ERROR\", \"1\")\n",
        "    cmd = [sys.executable, \"-u\", \"bd3lms/main.py\", *overrides]\n",
        "    print(\"\\n$\", \" \".join(cmd))\n",
        "    proc = subprocess.run(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        timeout=timeout,\n",
        "        check=False,\n",
        "        env=env,\n",
        "    )\n",
        "    print(proc.stdout[-4000:])\n",
        "    if proc.returncode != 0:\n",
        "        raise RuntimeError(f\"Command failed with return code {proc.returncode}\")\n",
        "    return proc.stdout\n",
        "\n",
        "_METRIC_PATTERNS = [\n",
        "    re.compile(r\"val/ppl\\s*[:=]\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
        "    re.compile(r\"'val/ppl'\\s*:\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
        "    re.compile(r\"val/ppl\\s*[│|]\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
        "]\n",
        "\n",
        "def extract_val_ppl(log_text: str):\n",
        "    for line in reversed(log_text.splitlines()):\n",
        "        if \"val/ppl\" in line.lower():\n",
        "            m = re.search(r\"val/ppl.*?([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", line, re.IGNORECASE)\n",
        "            if m:\n",
        "                return float(m.group(1))\n",
        "    hits = []\n",
        "    for pat in _METRIC_PATTERNS:\n",
        "        hits.extend(pat.findall(log_text))\n",
        "    return float(hits[-1]) if hits else None\n",
        "\n",
        "def _small_loader_overrides(batch_size=8, num_workers=2):\n",
        "    return [\n",
        "        f\"loader.global_batch_size={batch_size}\",\n",
        "        f\"loader.eval_global_batch_size={batch_size}\",\n",
        "        f\"loader.batch_size={batch_size}\",\n",
        "        f\"loader.eval_batch_size={batch_size}\",\n",
        "        f\"loader.num_workers={num_workers}\",\n",
        "        \"trainer.accumulate_grad_batches=1\",\n",
        "    ]\n",
        "\n",
        "def train_run(run_name, algo, block_size=None, from_pretrained=None,\n",
        "              max_steps=1500, extra_overrides=None):\n",
        "    \"\"\"Train a model and return checkpoint path.\"\"\"\n",
        "    save_dir = Path(\"/content/repro_runs\") / run_name\n",
        "    if save_dir.exists():\n",
        "        shutil.rmtree(save_dir)\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    overrides = [\n",
        "        \"mode=train\",\n",
        "        \"data=lm1b-wrap\",\n",
        "        \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\",\n",
        "        \"data.max_train_samples=3000\",\n",
        "        \"model=tiny\",\n",
        "        \"model.length=128\",\n",
        "        \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\",\n",
        "        \"trainer.accelerator=gpu\",\n",
        "        \"trainer.devices=1\",\n",
        "        \"trainer.num_nodes=1\",\n",
        "        \"trainer.precision=16-mixed\",\n",
        "        \"trainer.num_sanity_val_steps=0\",\n",
        "        \"trainer.log_every_n_steps=10\",\n",
        "        \"trainer.val_check_interval=50\",\n",
        "        f\"trainer.max_steps={max_steps}\",\n",
        "        \"data.max_valid_samples=100\",\n",
        "        \"data.max_test_samples=100\",\n",
        "        f\"checkpointing.save_dir=/content/repro_runs/{run_name}\",\n",
        "        \"checkpointing.resume_from_ckpt=false\",\n",
        "        \"wandb=null\",\n",
        "    ]\n",
        "    overrides.extend(_small_loader_overrides(batch_size=8, num_workers=2))\n",
        "\n",
        "    if block_size is not None:\n",
        "        overrides.append(f\"block_size={block_size}\")\n",
        "    if from_pretrained is not None:\n",
        "        overrides.append(f\"training.from_pretrained={from_pretrained}\")\n",
        "    if extra_overrides:\n",
        "        overrides.extend(extra_overrides)\n",
        "\n",
        "    _ = run_main(overrides)\n",
        "    ckpt = save_dir / \"checkpoints\" / \"last.ckpt\"\n",
        "    if not ckpt.exists():\n",
        "        raise FileNotFoundError(f\"Expected checkpoint not found: {ckpt}\")\n",
        "    return str(ckpt)\n",
        "\n",
        "def eval_run(algo, checkpoint_path, block_size=None, extra_overrides=None):\n",
        "    \"\"\"Evaluate perplexity.\"\"\"\n",
        "    overrides = [\n",
        "        \"mode=ppl_eval\",\n",
        "        \"data=lm1b-wrap\",\n",
        "        \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\",\n",
        "        \"data.max_test_samples=1000\",\n",
        "        \"model=tiny\",\n",
        "        \"model.length=128\",\n",
        "        \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\",\n",
        "        f\"eval.checkpoint_path={checkpoint_path}\",\n",
        "        \"trainer.accelerator=gpu\",\n",
        "        \"trainer.devices=1\",\n",
        "        \"trainer.num_nodes=1\",\n",
        "        \"trainer.precision=16-mixed\",\n",
        "        \"trainer.num_sanity_val_steps=0\",\n",
        "        \"wandb=null\",\n",
        "    ]\n",
        "    overrides.extend(_small_loader_overrides(batch_size=8, num_workers=2))\n",
        "\n",
        "    if block_size is not None:\n",
        "        overrides.append(f\"block_size={block_size}\")\n",
        "    if extra_overrides:\n",
        "        overrides.extend(extra_overrides)\n",
        "\n",
        "    log_text = run_main(overrides)\n",
        "    ppl = extract_val_ppl(log_text)\n",
        "    if ppl is None:\n",
        "        raise ValueError(\"Could not parse val/ppl from output.\")\n",
        "    return ppl\n",
        "\n",
        "print(\"Helper functions loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXdPX2yVHcz-"
      },
      "source": [
        "## Step 1: Train BD3-LM Base Model\n",
        "\n",
        "Χρειαζόμαστε ένα base model για fine-tuning.\n",
        "Το base χρησιμοποιεί **Linear U[0,1]** schedule (default)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFfrAPpgHcz_",
        "outputId": "5b1224b7-20b0-46fe-8c57-e263dd5f859d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Training BD3-LM BASE (block_size=128)...\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=3000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=1500 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_base_len128 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=128 training.resample=false algo.var_min=false trainer.val_check_interval=10\n",
            "um=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 47.13it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  23%|██▎       | 20/88 [00:02<00:08,  7.62it/s, v_num=0]Epoch 16, global step 1428: 'val/nll' was not in top 1\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 45.99it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  23%|██▎       | 20/88 [00:04<00:15,  4.48it/s, v_num=0]Epoch 16, global step 1438: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:06<00:07,  6.56it/s, v_num=0]\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:06<00:07,  6.56it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 47.81it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:06<00:07,  6.35it/s, v_num=0]Epoch 16, global step 1448: 'val/nll' was not in top 1\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 45.05it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:07<00:09,  5.02it/s, v_num=0]Epoch 16, global step 1458: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:09<00:04,  6.19it/s, v_num=0]\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:09<00:04,  6.19it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 47.56it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:09<00:04,  6.07it/s, v_num=0]Epoch 16, global step 1468: 'val/nll' was not in top 1\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 46.66it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:11<00:05,  5.14it/s, v_num=0]Epoch 16, global step 1478: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16:  91%|█████████ | 80/88 [00:13<00:01,  5.99it/s, v_num=0]\n",
            "Epoch 16:  91%|█████████ | 80/88 [00:13<00:01,  5.99it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 47.95it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  91%|█████████ | 80/88 [00:13<00:01,  5.91it/s, v_num=0]Epoch 16, global step 1488: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16: 100%|██████████| 88/88 [00:14<00:00,  5.87it/s, v_num=0]\n",
            "Epoch 16: 100%|██████████| 88/88 [00:14<00:00,  5.87it/s, v_num=0]\n",
            "Epoch 16: 100%|██████████| 88/88 [00:14<00:00,  5.87it/s, v_num=0]\n",
            "Epoch 16:   0%|          | 0/88 [00:00<?, ?it/s, v_num=0]         \n",
            "Epoch 17:   0%|          | 0/88 [00:00<?, ?it/s, v_num=0]\n",
            "Epoch 17:   0%|          | 0/88 [00:01<?, ?it/s, v_num=0]`Trainer.fit` stopped: `max_steps=1500` reached.\n",
            "\n",
            "Epoch 17:   0%|          | 0/88 [00:01<?, ?it/s, v_num=0]\n",
            "\n",
            "✓ BD3-LM base checkpoint: /content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Train BD3-LM Base\n",
        "print(\"=\" * 60)\n",
        "print(\"Training BD3-LM BASE (block_size=128)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "bd3lm_base_ckpt = train_run(\n",
        "    \"bd3lm_base_len128\",\n",
        "    algo=\"bd3lm\",\n",
        "    block_size=128,\n",
        "    extra_overrides=[\n",
        "        \"training.resample=false\",\n",
        "        \"algo.var_min=false\",\n",
        "        \"trainer.val_check_interval=10\",\n",
        "    ],\n",
        ")\n",
        "print(f\"✓ BD3-LM base checkpoint: {bd3lm_base_ckpt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa6_j_SHHcz_"
      },
      "source": [
        "## Step 2: Table 8 - Noise Schedule Ablation\n",
        "\n",
        "**ΚΡΙΣΙΜΟ:** Χρησιμοποιούμε `training.sampling_eps_min` και `training.sampling_eps_max`\n",
        "για να ελέγξουμε το noise schedule κατά το training!\n",
        "\n",
        "| Schedule | sampling_eps_min | sampling_eps_max |\n",
        "|----------|------------------|------------------|\n",
        "| Linear U[0,1] | 0.001 | 1.0 |\n",
        "| Clipped U[0.3,0.8] | 0.3 | 0.8 |\n",
        "| Clipped U[0.45,0.95] | 0.45 | 0.95 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sYhDEuTHc0A",
        "outputId": "4c9046af-f2ad-46af-ceea-270aa64da446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EXPERIMENTS FOR L' = 4\n",
            "============================================================\n",
            "\n",
            "--- Linear U[0,1] ---\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=3000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=1500 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_schedule_Linear_U0_1_Lp4 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 training.from_pretrained=/content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt training.resample=true algo.var_min=false trainer.val_check_interval=10 training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "um=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 47.44it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  23%|██▎       | 20/88 [00:02<00:08,  7.57it/s, v_num=0]Epoch 16, global step 1424: 'val/nll' was not in top 1\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 45.56it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  23%|██▎       | 20/88 [00:04<00:15,  4.48it/s, v_num=0]Epoch 16, global step 1434: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:06<00:08,  5.89it/s, v_num=0]\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:06<00:08,  5.89it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 47.32it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:06<00:08,  5.72it/s, v_num=0]Epoch 16, global step 1444: 'val/nll' was not in top 1\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 45.75it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:08<00:10,  4.51it/s, v_num=0]Epoch 16, global step 1454: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:10<00:04,  5.68it/s, v_num=0]\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:10<00:04,  5.68it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 46.43it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:10<00:05,  5.57it/s, v_num=0]Epoch 16, global step 1464: 'val/nll' was not in top 1\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 46.44it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:12<00:05,  4.73it/s, v_num=0]Epoch 16, global step 1474: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16:  91%|█████████ | 80/88 [00:14<00:01,  5.59it/s, v_num=0]\n",
            "Epoch 16:  91%|█████████ | 80/88 [00:14<00:01,  5.59it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 46.07it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  91%|█████████ | 80/88 [00:14<00:01,  5.51it/s, v_num=0]Epoch 16, global step 1484: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16: 100%|██████████| 88/88 [00:16<00:00,  5.46it/s, v_num=0]\n",
            "Epoch 16: 100%|██████████| 88/88 [00:16<00:00,  5.46it/s, v_num=0]\n",
            "Epoch 16: 100%|██████████| 88/88 [00:16<00:00,  5.46it/s, v_num=0]\n",
            "Epoch 16:   0%|          | 0/88 [00:00<?, ?it/s, v_num=0]         \n",
            "Epoch 17:   0%|          | 0/88 [00:00<?, ?it/s, v_num=0]\n",
            "Epoch 17:   0%|          | 0/88 [00:02<?, ?it/s, v_num=0]`Trainer.fit` stopped: `max_steps=1500` reached.\n",
            "\n",
            "Epoch 17:   0%|          | 0/88 [00:02<?, ?it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_schedule_Linear_U0_1_Lp4/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 algo.var_min=false\n",
            "e using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-01-11 13:10:17.949090: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-11 13:10:17.965429: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768137017.983850   10075 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768137017.989384   10075 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768137018.003787   10075 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768137018.003810   10075 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768137018.003812   10075 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768137018.003814   10075 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-11 13:10:18.008181: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\n",
            "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 19.36it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 24.28it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 23.71it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │    10.345799446105957     │\n",
            "│          val/nll          │     7.171161651611328     │\n",
            "│          val/ppl          │       1301.35546875       │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "✓ Linear U[0,1] (L'=4): PPL = 1301.36\n",
            "\n",
            "--- Clipped U[0.3,0.8] ---\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=3000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=1500 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_schedule_Clipped_U0.3_0.8_Lp4 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 training.from_pretrained=/content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt training.resample=true algo.var_min=false trainer.val_check_interval=10 training.sampling_eps_min=0.3 training.sampling_eps_max=0.8\n",
            "um=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 42.92it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  23%|██▎       | 20/88 [00:02<00:09,  7.07it/s, v_num=0]Epoch 16, global step 1424: 'val/nll' was not in top 1\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 44.44it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  23%|██▎       | 20/88 [00:04<00:16,  4.19it/s, v_num=0]Epoch 16, global step 1434: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:06<00:07,  6.13it/s, v_num=0]\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:06<00:07,  6.13it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 46.03it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:06<00:08,  5.93it/s, v_num=0]Epoch 16, global step 1444: 'val/nll' was not in top 1\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 45.63it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:08<00:10,  4.57it/s, v_num=0]Epoch 16, global step 1454: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:10<00:04,  5.73it/s, v_num=0]\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:10<00:04,  5.73it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 46.17it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:10<00:04,  5.61it/s, v_num=0]Epoch 16, global step 1464: 'val/nll' was not in top 1\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 45.45it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:12<00:05,  4.79it/s, v_num=0]Epoch 16, global step 1474: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16:  91%|█████████ | 80/88 [00:14<00:01,  5.67it/s, v_num=0]\n",
            "Epoch 16:  91%|█████████ | 80/88 [00:14<00:01,  5.67it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 45.64it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  91%|█████████ | 80/88 [00:14<00:01,  5.58it/s, v_num=0]Epoch 16, global step 1484: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16: 100%|██████████| 88/88 [00:15<00:00,  5.53it/s, v_num=0]\n",
            "Epoch 16: 100%|██████████| 88/88 [00:15<00:00,  5.53it/s, v_num=0]\n",
            "Epoch 16: 100%|██████████| 88/88 [00:15<00:00,  5.53it/s, v_num=0]\n",
            "Epoch 16:   0%|          | 0/88 [00:00<?, ?it/s, v_num=0]         \n",
            "Epoch 17:   0%|          | 0/88 [00:00<?, ?it/s, v_num=0]\n",
            "Epoch 17:   0%|          | 0/88 [00:02<?, ?it/s, v_num=0]`Trainer.fit` stopped: `max_steps=1500` reached.\n",
            "\n",
            "Epoch 17:   0%|          | 0/88 [00:02<?, ?it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_schedule_Clipped_U0.3_0.8_Lp4/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 algo.var_min=false\n",
            "e using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-01-11 13:19:48.758852: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-11 13:19:48.775251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768137588.793413   14023 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768137588.798845   14023 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768137588.813185   14023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768137588.813206   14023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768137588.813208   14023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768137588.813229   14023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-11 13:19:48.817480: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\n",
            "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 19.08it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 23.83it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 23.26it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │    10.661749839782715     │\n",
            "│          val/nll          │     7.390161991119385     │\n",
            "│          val/ppl          │     1619.968505859375     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "✓ Clipped U[0.3,0.8] (L'=4): PPL = 1619.97\n",
            "\n",
            "--- Clipped U[0.45,0.95] ---\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=3000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=1500 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_schedule_Clipped_U0.45_0.95_Lp4 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 training.from_pretrained=/content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt training.resample=true algo.var_min=false trainer.val_check_interval=10 training.sampling_eps_min=0.45 training.sampling_eps_max=0.95\n",
            "um=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 46.93it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  23%|██▎       | 20/88 [00:02<00:09,  7.06it/s, v_num=0]Epoch 16, global step 1424: 'val/nll' was not in top 1\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 43.15it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  23%|██▎       | 20/88 [00:04<00:16,  4.16it/s, v_num=0]Epoch 16, global step 1434: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:07<00:08,  5.57it/s, v_num=0]\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:07<00:08,  5.57it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 46.00it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:07<00:08,  5.40it/s, v_num=0]Epoch 16, global step 1444: 'val/nll' was not in top 1\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 46.44it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  45%|████▌     | 40/88 [00:09<00:11,  4.30it/s, v_num=0]Epoch 16, global step 1454: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:10<00:05,  5.47it/s, v_num=0]\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:10<00:05,  5.47it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 44.38it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:11<00:05,  5.36it/s, v_num=0]Epoch 16, global step 1464: 'val/nll' was not in top 1\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 45.24it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  68%|██████▊   | 60/88 [00:13<00:06,  4.56it/s, v_num=0]Epoch 16, global step 1474: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16:  91%|█████████ | 80/88 [00:14<00:01,  5.40it/s, v_num=0]\n",
            "Epoch 16:  91%|█████████ | 80/88 [00:14<00:01,  5.40it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 45.47it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 16:  91%|█████████ | 80/88 [00:15<00:01,  5.32it/s, v_num=0]Epoch 16, global step 1484: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 16: 100%|██████████| 88/88 [00:17<00:00,  4.96it/s, v_num=0]\n",
            "Epoch 16: 100%|██████████| 88/88 [00:17<00:00,  4.96it/s, v_num=0]\n",
            "Epoch 16: 100%|██████████| 88/88 [00:17<00:00,  4.96it/s, v_num=0]\n",
            "Epoch 16:   0%|          | 0/88 [00:00<?, ?it/s, v_num=0]         \n",
            "Epoch 17:   0%|          | 0/88 [00:00<?, ?it/s, v_num=0]\n",
            "Epoch 17:   0%|          | 0/88 [00:02<?, ?it/s, v_num=0]`Trainer.fit` stopped: `max_steps=1500` reached.\n",
            "\n",
            "Epoch 17:   0%|          | 0/88 [00:02<?, ?it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_schedule_Clipped_U0.45_0.95_Lp4/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 algo.var_min=false\n",
            "e using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-01-11 13:29:38.731844: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-11 13:29:38.749047: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768138178.767610   18040 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768138178.773059   18040 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768138178.787605   18040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768138178.787626   18040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768138178.787628   18040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768138178.787630   18040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-11 13:29:38.791780: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\n",
            "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 19.85it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 24.80it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 24.21it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │     10.81816291809082     │\n",
            "│          val/nll          │     7.498579502105713     │\n",
            "│          val/ppl          │     1805.475830078125     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "✓ Clipped U[0.45,0.95] (L'=4): PPL = 1805.48\n",
            "\n",
            "============================================================\n",
            "EXPERIMENTS FOR L' = 16\n",
            "============================================================\n",
            "\n",
            "--- Linear U[0,1] ---\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=3000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=1500 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_schedule_Linear_U0_1_Lp16 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=16 training.from_pretrained=/content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt training.resample=true algo.var_min=false trainer.val_check_interval=10 training.sampling_eps_min=0.001 training.sampling_eps_max=1.0\n",
            "ontent/bd3lms/main.py\", line 234, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/hydra/main.py\", line 94, in decorated_main\n",
            "    _run_hydra(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n",
            "    _run_app(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 457, in _run_app\n",
            "    run_and_report(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 223, in run_and_report\n",
            "    raise ex\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n",
            "    return func()\n",
            "           ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 458, in <lambda>\n",
            "    lambda: hydra.run(\n",
            "            ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/hydra.py\", line 132, in run\n",
            "    _ = ret.return_value\n",
            "        ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/hydra/core/utils.py\", line 260, in return_value\n",
            "    raise self._return_value\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/hydra/core/utils.py\", line 186, in run_job\n",
            "    ret.return_value = task_function(task_cfg)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/bd3lms/main.py\", line 230, in main\n",
            "    _train(config, logger, tokenizer)\n",
            "  File \"/content/bd3lms/main.py\", line 169, in _train\n",
            "    train_ds, valid_ds = dataloader.get_dataloaders(\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/bd3lms/dataloader.py\", line 798, in get_dataloaders\n",
            "    valid_set = get_dataset(\n",
            "                ^^^^^^^^^^^^\n",
            "  File \"/content/bd3lms/dataloader.py\", line 585, in get_dataset\n",
            "    dataset = get_streaming_samples(stream_dataset, max_samples[mode], dataset_name=dataset_name)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/bd3lms/dataloader.py\", line 308, in get_streaming_samples\n",
            "    for i, example in enumerate(dataset):\n",
            "                      ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 2226, in __iter__\n",
            "    for key, example in ex_iterable:\n",
            "                        ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/iterable_dataset.py\", line 219, in __iter__\n",
            "    for key_example in islice(self.generate_examples_fn(**gen_kwags), shard_example_idx_start, None):\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/datasets_modules/datasets/lm1b/694f4d1f8a28b4eefe3151693c199fd216ab51c4f947fd5b007abe4d774b79e0/lm1b.py\", line 108, in _generate_examples\n",
            "    for path, f in files:\n",
            "                   ^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/track.py\", line 50, in __iter__\n",
            "    for x in self.generator(*self.args):\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/file_utils.py\", line 1342, in _iter_from_urlpath\n",
            "    yield from cls._iter_tar(f)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/file_utils.py\", line 1294, in _iter_tar\n",
            "    for tarinfo in stream:\n",
            "                   ^^^^^^\n",
            "  File \"/usr/lib/python3.12/tarfile.py\", line 2881, in __iter__\n",
            "    tarinfo = self.next()\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/tarfile.py\", line 2733, in next\n",
            "    self.fileobj.seek(self.offset - 1)\n",
            "  File \"/usr/lib/python3.12/tarfile.py\", line 519, in seek\n",
            "    self.read(self.bufsize)\n",
            "  File \"/usr/lib/python3.12/tarfile.py\", line 528, in read\n",
            "    buf = self._read(size)\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/tarfile.py\", line 546, in _read\n",
            "    buf = self.fileobj.read(self.bufsize)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/datasets/utils/file_utils.py\", line 840, in read_with_retries\n",
            "    raise ConnectionError(\"Server Disconnected\") from disconnect_err\n",
            "ConnectionError: Server Disconnected\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Command failed with return code 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2462774708.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Fine-tune with this schedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         finetune_ckpt = train_run(\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bd3lm\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2367475153.py\u001b[0m in \u001b[0;36mtrain_run\u001b[0;34m(run_name, algo, block_size, from_pretrained, max_steps, extra_overrides)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0moverrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_overrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"checkpoints\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"last.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2367475153.py\u001b[0m in \u001b[0;36mrun_main\u001b[0;34m(overrides, timeout)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Command failed with return code {proc.returncode}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Command failed with return code 1"
          ]
        }
      ],
      "source": [
        "# Cell 6: Table 8 Experiments - CORRECTED\n",
        "\n",
        "# Define noise schedules with CORRECT training parameters\n",
        "noise_schedules = [\n",
        "    # (name, [training overrides])\n",
        "    (\"Linear U[0,1]\", [\n",
        "        \"training.sampling_eps_min=0.001\",\n",
        "        \"training.sampling_eps_max=1.0\",\n",
        "    ]),\n",
        "    (\"Clipped U[0.3,0.8]\", [\n",
        "        \"training.sampling_eps_min=0.3\",\n",
        "        \"training.sampling_eps_max=0.8\",\n",
        "    ]),\n",
        "    (\"Clipped U[0.45,0.95]\", [\n",
        "        \"training.sampling_eps_min=0.45\",\n",
        "        \"training.sampling_eps_max=0.95\",\n",
        "    ]),\n",
        "]\n",
        "\n",
        "# Block sizes to test\n",
        "block_sizes = [4, 16]\n",
        "\n",
        "results_table8 = []\n",
        "\n",
        "for Lprime in block_sizes:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"EXPERIMENTS FOR L' = {Lprime}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for schedule_name, schedule_overrides in noise_schedules:\n",
        "        print(f\"\\n--- {schedule_name} ---\")\n",
        "\n",
        "        # Create safe run name\n",
        "        safe_name = schedule_name.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"_\").replace(\" \", \"_\")\n",
        "        run_name = f\"bd3lm_schedule_{safe_name}_Lp{Lprime}\"\n",
        "\n",
        "        # Fine-tune with this schedule\n",
        "        finetune_ckpt = train_run(\n",
        "            run_name,\n",
        "            algo=\"bd3lm\",\n",
        "            block_size=Lprime,\n",
        "            from_pretrained=bd3lm_base_ckpt,\n",
        "            extra_overrides=[\n",
        "                \"training.resample=true\",\n",
        "                \"algo.var_min=false\",\n",
        "                \"trainer.val_check_interval=10\",\n",
        "            ] + schedule_overrides,  # Add schedule-specific overrides\n",
        "        )\n",
        "\n",
        "        # Evaluate (always with linear schedule for fair comparison)\n",
        "        ppl = eval_run(\n",
        "            algo=\"bd3lm\",\n",
        "            checkpoint_path=finetune_ckpt,\n",
        "            block_size=Lprime,\n",
        "            extra_overrides=[\n",
        "                \"algo.var_min=false\",\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        results_table8.append({\n",
        "            \"block_size\": Lprime,\n",
        "            \"schedule\": schedule_name,\n",
        "            \"ppl\": ppl,\n",
        "        })\n",
        "        print(f\"✓ {schedule_name} (L'={Lprime}): PPL = {ppl:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TABLE 8 EXPERIMENTS COMPLETE!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7r8eyxJHc0B"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Display Results\n",
        "\n",
        "def print_table(rows):\n",
        "    if not rows:\n",
        "        print(\"No data to display.\")\n",
        "        return\n",
        "    columns = list(rows[0].keys())\n",
        "    str_rows = [{col: str(row.get(col, \"\")) for col in columns} for row in rows]\n",
        "    widths = {col: max(len(col), max(len(row[col]) for row in str_rows)) for col in columns}\n",
        "    def print_separator():\n",
        "        print(\"+\" + \"+\".join(\"-\" * (widths[col] + 2) for col in columns) + \"+\")\n",
        "    def print_row(row):\n",
        "        print(\"| \" + \" | \".join(row[col].ljust(widths[col]) for col in columns) + \" |\")\n",
        "    print_separator()\n",
        "    print_row({col: col for col in columns})\n",
        "    print_separator()\n",
        "    for row in str_rows:\n",
        "        print_row(row)\n",
        "    print_separator()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TABLE 8: EFFECT OF NOISE SCHEDULE ON LIKELIHOOD ESTIMATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Results for L'=4\n",
        "print(\"\\n--- L' = 4 (smaller block → heavier masking better) ---\")\n",
        "results_Lp4 = [r for r in results_table8 if r[\"block_size\"] == 4]\n",
        "results_Lp4_sorted = sorted(results_Lp4, key=lambda x: x[\"ppl\"])\n",
        "print_table(results_Lp4_sorted)\n",
        "if results_Lp4_sorted:\n",
        "    print(f\"Best for L'=4: {results_Lp4_sorted[0]['schedule']}\")\n",
        "\n",
        "# Results for L'=16\n",
        "print(\"\\n--- L' = 16 (larger block → lighter masking better) ---\")\n",
        "results_Lp16 = [r for r in results_table8 if r[\"block_size\"] == 16]\n",
        "results_Lp16_sorted = sorted(results_Lp16, key=lambda x: x[\"ppl\"])\n",
        "print_table(results_Lp16_sorted)\n",
        "if results_Lp16_sorted:\n",
        "    print(f\"Best for L'=16: {results_Lp16_sorted[0]['schedule']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTmGUD_zHc0B"
      },
      "source": [
        "## Paper Results (Table 8) για Σύγκριση\n",
        "\n",
        "| Noise Schedule | L'=4 PPL | L'=16 PPL |\n",
        "|----------------|----------|----------|\n",
        "| **Clipped U[0.45,0.95]** | **29.21** | 31.42 |\n",
        "| Clipped U[0.3,0.8] | 29.38 | **31.12** |\n",
        "| Linear U[0,1] | 30.18 | 31.72 |\n",
        "\n",
        "**Expected Pattern:**\n",
        "- L'=4: Clipped U[0.45,0.95] < Clipped U[0.3,0.8] < Linear (heavier masking better)\n",
        "- L'=16: Clipped U[0.3,0.8] < Clipped U[0.45,0.95] < Linear (lighter masking better)\n",
        "\n",
        "**Key Insight:** Avoiding extreme mask rates reduces training variance!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUk581cwHc0B"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Verify Pattern Match\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PATTERN VERIFICATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check L'=4 pattern\n",
        "if len(results_Lp4_sorted) >= 3:\n",
        "    linear_4 = next((r['ppl'] for r in results_Lp4 if 'Linear' in r['schedule']), None)\n",
        "    clip_03_4 = next((r['ppl'] for r in results_Lp4 if '0.3' in r['schedule']), None)\n",
        "    clip_045_4 = next((r['ppl'] for r in results_Lp4 if '0.45' in r['schedule']), None)\n",
        "\n",
        "    print(f\"\\nL'=4:\")\n",
        "    print(f\"  Linear U[0,1]:       {linear_4:.2f}\")\n",
        "    print(f\"  Clipped U[0.3,0.8]:  {clip_03_4:.2f}\")\n",
        "    print(f\"  Clipped U[0.45,0.95]: {clip_045_4:.2f}\")\n",
        "\n",
        "    if clip_045_4 < linear_4:\n",
        "        print(\"  ✅ Clipped U[0.45,0.95] < Linear (CORRECT!)\")\n",
        "    else:\n",
        "        print(\"  ⚠️ Pattern not as expected (may need more training)\")\n",
        "\n",
        "# Check L'=16 pattern\n",
        "if len(results_Lp16_sorted) >= 3:\n",
        "    linear_16 = next((r['ppl'] for r in results_Lp16 if 'Linear' in r['schedule']), None)\n",
        "    clip_03_16 = next((r['ppl'] for r in results_Lp16 if '0.3' in r['schedule']), None)\n",
        "    clip_045_16 = next((r['ppl'] for r in results_Lp16 if '0.45' in r['schedule']), None)\n",
        "\n",
        "    print(f\"\\nL'=16:\")\n",
        "    print(f\"  Linear U[0,1]:       {linear_16:.2f}\")\n",
        "    print(f\"  Clipped U[0.3,0.8]:  {clip_03_16:.2f}\")\n",
        "    print(f\"  Clipped U[0.45,0.95]: {clip_045_16:.2f}\")\n",
        "\n",
        "    if clip_03_16 < linear_16:\n",
        "        print(\"  ✅ Clipped U[0.3,0.8] < Linear (CORRECT!)\")\n",
        "    else:\n",
        "        print(\"  ⚠️ Pattern not as expected (may need more training)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"REPRODUCTION COMPLETE!\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
