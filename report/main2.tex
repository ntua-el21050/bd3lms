\documentclass{article}

\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{array}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{float}
\usepackage{flafter}
\usepackage[section]{placeins}

% Float tuning: keep floats near where they are introduced.
\setcounter{topnumber}{5}
\setcounter{bottomnumber}{5}
\setcounter{totalnumber}{10}
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}
\renewcommand{\floatpagefraction}{0.9}

% Try to include figures from either report/media/ or ../final_presentation/
\graphicspath{{media/}{../final_presentation/}}
\newcommand{\safeincludegraphics}[2][]{%
  \IfFileExists{#2}{\includegraphics[#1]{#2}}{%
    \IfFileExists{../final_presentation/#2}{\includegraphics[#1]{../final_presentation/#2}}{\fbox{Missing file: #2}}%
  }%
}

% Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{\textit{ }}
\fancyhead[LO]{Block Diffusion (BD3-LMs): Reproduction and Extensions}

%% Title
\title{Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models\\
\vspace{0.25em}
\large Reproduction Results and Two Practical Extensions}

\author{
  Ntountounakis Georgios, Markoulidakis Georgios, Vitalis Petros,\\
  Makras Ilias, Kritharidis Konstantinos, Kordas Nikolaos\\
  National Technical University of Athens
}

\begin{document}
\maketitle

\begin{abstract}
In this report, we are evaluating the Block Discrete Denoising Diffusion Language
Models (BD3-LMs), a class of models designed to combine the advantages of Autoregressive (AR) and Discrete Diffusion models.
BD3-LMs maintain an autoregressive distribution over blocks while performing parallelized discrete diffusion
within each block. We present a reproduction of the original experiments, as well as our own ideas for extensions that improve the performance of BD3-LMs.
Specifically, we explore two new noise schedules and loss reweighting. Our reproduction
confirms that BD3-LMs outperform standard diffusion models, achieving variable-length generation
and high quality sampling efficiency, even under significant resource constraints.
Our proposed extensions further enhance the performance of BD3-LMs, demonstrating the potential
of this class of models for efficient language generation.

\vspace{0.5em}
\noindent\textbf{Resources:} Original codebase: \url{https://github.com/kuleshov-group/bd3lms}. Our repository: \url{https://github.com/ntua-el21050/bd3lms}.
\end{abstract}

\section{Introduction}
Modern language models are typically built using one of two generation paradigms.
\emph{Autoregressive} (AR) models generate one token at a time, which empirically yields strong likelihoods and high sample quality, supports key-value (KV) caching, and naturally allows variable-length generation.
\emph{Diffusion} models instead aim to generate (or refine) many tokens in parallel through iterative denoising steps, which can improve controllability and parallelism, but often suffers from a perplexity gap and typically targets fixed-length outputs.

Block diffusion (BD3-LMs) is a hybrid: it is autoregressive over blocks and diffusion-like within each block.
This design targets a controllable trade-off between quality and parallelism via the block size $L'$.
At the extremes, $L'=1$ reduces to token-level AR, while $L'=L$ (the full sequence length) becomes a fully parallel diffusion model.

\section{Background: Masked Discrete Diffusion and Noise Schedules}
We focus on masked discrete diffusion, where noising corresponds to masking tokens with probability $p(t)$ as a function of a continuous time index $t\sim\mathcal{U}[0,1]$.
Define the keep (no-mask) probability
\begin{equation}
  a(t) = 1 - p(t).
\end{equation}
A key quantity that appears in the (masked) diffusion objective is the schedule-induced scaling
\begin{equation}
  \text{loss\_scaling}(t) 
  = \frac{a'(t)}{1-a(t)}
  = -\frac{p'(t)}{p(t)},
\end{equation}
which acts as a weighting of per-token log-likelihood terms across noise levels.

We consider standard schedules (loglinear, square, square root, logarithmic, cosine) and also investigate clipped schedules, where $t$ is sampled from a restricted interval $\mathcal{U}[\tau_{\min},\tau_{\max}]$.
The presentation results suggest that clipping can reduce gradient variance (as measured by variance of the NELBO estimator), though perplexity improvements are not guaranteed under limited training.

\section{Block Diffusion Models (BD3-LMs)}
BD3-LMs factorize a sequence into contiguous blocks of length $L'$.
Generation proceeds autoregressively at the block level: each block is generated conditioned on the previous blocks.
Within the current block, denoising is performed using a masked diffusion process, enabling parallel prediction of the tokens in the block.

This hybridization yields:
\begin{itemize}
  \item \textbf{Parallelism within blocks:} enables faster refinement than strictly token-by-token generation.
  \item \textbf{Autoregressive control across blocks:} enables variable-length generation and mitigates some diffusion limitations.
  \item \textbf{A single knob $L'$:} a direct quality--parallelism trade-off.
\end{itemize}

\section{Experimental Protocol and Metrics}
We summarize the protocol used in our experiments (as reported in the final presentation):
\begin{itemize}
  \item \textbf{Datasets:} LM1B and OpenWebText (OWT), plus transfer evaluation on Lambada and Wikitext.
  \item \textbf{Training regimen:} pretraining and fine-tuning phases reported in the table captions (e.g., 400 pretraining steps + 100 fine-tuning steps on LM1B for many comparisons).
  \item \textbf{Likelihood metrics:} test perplexity (PPL; lower is better).
  \item \textbf{Objective stability:} variance of the NELBO estimator (Var.\ NELBO; lower is better).
  \item \textbf{Sampling quality:} generative perplexity (Gen.PPL; lower is better) and number of function evaluations (NFEs).
\end{itemize}

\section{Reproduction Results}
\FloatBarrier
\subsection{AR vs BD3-LM at $L'=1$}
Table~\ref{tab:ar_vs_bd3lm_l1} compares token-level AR and BD3-LM with $L'=1$ on LM1B.

\begin{table}[!htbp]
  \centering
  \caption{Test perplexities for single-token generation on LM1B (800 training steps).}
  \label{tab:ar_vs_bd3lm_l1}
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{lc}
    \toprule
    & PPL ($\downarrow$) \\
    \midrule
    Autoregressive (AR) & \textbf{1893} \\
    BD3-LM ($L'=1$) & 2231 \\
    BD3-LM ($L'=1$) + tuned schedule & 2220 \\
    \bottomrule
  \end{tabular}
\end{table}

We also include the training curves used in the presentation (Figure~\ref{fig:l1_curves}) to visually compare optimization behavior.

\begin{figure}[!htbp]
  \centering
  \begin{subfigure}[t]{0.32\linewidth}
    \centering
    \safeincludegraphics[width=\linewidth]{fig1.png}
    \caption{AR}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.32\linewidth}
    \centering
    \safeincludegraphics[width=\linewidth]{fig2.png}
    \caption{BD3-LM}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.32\linewidth}
    \centering
    \safeincludegraphics[width=\linewidth]{fig3.png}
    \caption{BD3-LM + tuned schedule}
  \end{subfigure}
  \caption{Curves from the final presentation for single-token generation on LM1B.}
  \label{fig:l1_curves}
\end{figure}

\subsection{Effect of clipped noise schedules}
Table~\ref{tab:clipping_effect} reports PPL and Var.\ NELBO for several $L'$ values under clipped and unclipped uniform time sampling on LM1B.

\begin{table}[!htbp]
  \centering
  \caption{Effect of clipped noise schedules on LM1B (400 pretraining steps + 100 fine-tuning steps).}
  \label{tab:clipping_effect}
  \setlength{\tabcolsep}{5pt}
  \renewcommand{\arraystretch}{1.12}
  \begin{tabular}{l l c c}
    \toprule
    $L'$ & Clipping & PPL ($\downarrow$) & Var.\ NELBO ($\downarrow$) \\
    \midrule
    \multirow{2}{*}{128} & $\mathcal{U}[0,0.5]$ & \textbf{2106} & \textbf{1.27} \\
    & $\mathcal{U}[0,1]$ & \textbf{2106} & \textbf{1.27} \\
    \midrule
    \multirow{2}{*}{16} & $\mathcal{U}[0.3,0.8]$ & \textbf{1278} & \textbf{10.50} \\
    & $\mathcal{U}[0,1]$ & 1279 & 10.51 \\
    \midrule
    \multirow{2}{*}{4} & $\mathcal{U}[0.5,1]$ & \textbf{1226} & \textbf{44.41} \\
    & $\mathcal{U}[0,1]$ & \textbf{1226} & \textbf{44.41} \\
    \bottomrule
  \end{tabular}
\end{table}

Under our reproduced settings, clipping is most clearly beneficial for $L'=16$ (small but consistent improvements in both PPL and variance), whereas for $L'=4$ and $L'=128$ the effect is negligible.

\subsection{LM1B and OWT comparisons}
Tables~\ref{tab:lm1b_main} and~\ref{tab:owt_main} compare AR, diffusion baselines, and BD3-LM variants.

\begin{table}[!htbp]
  \centering
  \caption{Test perplexities on LM1B (400 pretraining steps + 100 fine-tuning steps).}
  \label{tab:lm1b_main}
  \setlength{\tabcolsep}{10pt}
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{l r}
    \toprule
    Model & PPL ($\downarrow$) \\
    \midrule
    \multicolumn{2}{l}{\textbf{Autoregressive}} \\
    Transformer & 3042 \\
    \midrule
    \multicolumn{2}{l}{\textbf{Diffusion}} \\
    SEDD & 1447 \\
    MDLM & 1616 \\
    \midrule
    \multicolumn{2}{l}{\textbf{Block diffusion}} \\
    BD3-LM $L'=16$ & 1278 \\
    BD3-LM $L'=8$ & 1734 \\
    BD3-LM $L'=4$ & \textbf{1226} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[!htbp]
  \centering
  \caption{Test perplexities on OWT (3000 pretraining steps + 3000 fine-tuning steps).}
  \label{tab:owt_main}
  \setlength{\tabcolsep}{10pt}
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{l r}
    \toprule
    Model & PPL ($\downarrow$) \\
    \midrule
    \multicolumn{2}{l}{\textbf{Autoregressive}} \\
    Transformer & 2036 \\
    \midrule
    \multicolumn{2}{l}{\textbf{Diffusion}} \\
    SEDD & 2120 \\
    MDLM & 2101 \\
    \midrule
    \multicolumn{2}{l}{\textbf{Block diffusion}} \\
    BD3-LM $L'=16$ & 1939 \\
    BD3-LM $L'=8$ & 1941 \\
    BD3-LM $L'=4$ & \textbf{1935} \\
    \bottomrule
  \end{tabular}
\end{table}

On LM1B, BD3-LMs outperform the diffusion baselines under the same training budget.
On OWT, BD3-LMs are competitive and slightly improve over the AR baseline in our reproduced setting.

\subsection{Transfer evaluation (trained on OWT)}
Table~\ref{tab:transfer} summarizes validation perplexities on other datasets for models trained on OWT.

\begin{table}[!htbp]
  \centering
  \caption{Zero-shot validation perplexities of models trained on OWT (800 pretraining steps + 800 fine-tuning steps).}
  \label{tab:transfer}
  \setlength{\tabcolsep}{12pt}
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{lccc}
    \toprule
    & LM1B & Lambada & Wikitext \\
    \midrule
    AR & 2388 & 1550 & \textbf{2875} \\
    \midrule
    SEDD & 2742 & 1562 & 3335 \\
    MDLM & 2722 & 1556 & 3283 \\
    BD3-LM $L'=4$ & \textbf{2196} & \textbf{1438} & 3143 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Variable-length generation}
Table~\ref{tab:gen_length_stats} highlights a core benefit of block diffusion over pure diffusion: longer generations are possible because blocks are generated autoregressively.

\begin{table}[!htbp]
  \centering
  \caption{Generation length statistics for 10 sampled documents from OWT-trained models (800 pretraining steps + 500 fine-tuning steps). BD3-LM reproduction with model length $=16$K.}
  \label{tab:gen_length_stats}
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{lrr}
    \toprule
    & \makecell{Median\\\# tokens} & \makecell{Max\\\# tokens} \\
    \midrule
    OWT train set & 717 & 131K \\
    AR & 4008 & 131K \\
    \midrule
    SEDD & 1021 & 1024 \\
    BD3-LM $L'=16$ & 798 & 2927 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Sample quality and schedule ablations}
We report (i) sampling quality in terms of Gen.PPL and NFEs (Table~\ref{tab:sample_quality}) and (ii) an ablation over noise schedules (Table~\ref{tab:noise_schedule_ablation}).

\begin{table}[!htbp]
  \centering
  \caption{Generative perplexity (Gen.PPL; $\downarrow$) and number of function evaluations (NFEs; $\downarrow$) for 300 samples. Models trained on OWT (400 + 100 training steps).}
  \label{tab:sample_quality}
  \setlength{\tabcolsep}{8pt}
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{lcc}
    \toprule
    Model & Gen.PPL & NFEs \\
    \midrule
    AR & 79165 & 1024 \\
    SEDD & 29987 & 1023 \\
    MDLM & 25632 & 1023 \\
    BD3-LM $L'=16$ & \textbf{7576} & 1023 \\
    BD3-LM $L'=8$ & 8176 & 1023 \\
    BD3-LM $L'=4$ & 9785 & 1023 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[!htbp]
  \centering
  \caption{Effect of noise schedule on PPL and Var.\ NELBO for different $L'$ on LM1B (5000 + 3000 training steps).}
  \label{tab:noise_schedule_ablation}
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.05}
  \begin{tabular}{lcc}
    \toprule
    Noise schedule & PPL ($\downarrow$) & Var.\ NELBO ($\downarrow$) \\
    \midrule
    \multicolumn{3}{l}{\textbf{$L'=4$}} \\
    \quad Clipped $\mathcal{U}[0.45,0.95]$ & 1199 & \textbf{15.23} \\
    \quad Clipped $\mathcal{U}[0.3,0.8]$ & 1101 & 27.61 \\
    \quad Linear $\mathcal{U}[0,1]$ & 751 & 260.10 \\
    \quad Logarithmic & 750 & 125.62 \\
    \quad Square root & \textbf{719} & 83.83 \\
    \midrule
    \multicolumn{3}{l}{\textbf{$L'=16$}} \\
    \quad Clipped $\mathcal{U}[0.45,0.95]$ & 1056 & \textbf{4.35} \\
    \quad Clipped $\mathcal{U}[0.3,0.8]$ & 798 & 6.41 \\
    \quad Linear $\mathcal{U}[0,1]$ & 662 & 44.73 \\
    \quad Square & \textbf{627} & 27.53 \\
    \quad Cosine & 634 & 20.80 \\
    \bottomrule
  \end{tabular}
\end{table}

The ablation supports the presentation takeaway: clipped schedules reliably reduce variance, but standard schedules can yield lower perplexity given enough training.

\section{Extension 1: Alternative Noise Schedules}
\FloatBarrier
\subsection{Gaussian and bimodal Gaussian schedules}
We propose schedules that directly sample the masked probability $p(t)\in(0,1)$.
For a (truncated) Gaussian schedule, given mean $\mu$ and standard deviation $\sigma$, define $\alpha=\frac{0-\mu}{\sigma}$, $\beta=\frac{1-\mu}{\sigma}$, $\Phi$ the standard normal CDF, and $\varphi$ the pdf.
Let $\Phi_{\alpha}=\Phi(\alpha)$, $\Phi_{\beta}=\Phi(\beta)$, and $Z=\Phi_{\beta}-\Phi_{\alpha}$.
Then for $t\in(0,1)$,
\begin{equation}
  z(t)=\Phi^{-1}\!\big(\Phi_{\alpha}+ t(\Phi_{\beta}-\Phi_{\alpha})\big), \qquad p(t)=\mu+\sigma z(t)\in(0,1).
\end{equation}
The induced scaling is
\begin{equation}
  \text{loss\_scaling}(t) = -\frac{p'(t)}{p(t)} = -\frac{\sigma Z}{\varphi(z(t))\,p(t)}.
\end{equation}

For a bimodal Gaussian schedule, we choose a mixture weight $w\in(0,1)$ and two Gaussians $(\mu_1,\sigma_1)$ and $(\mu_2,\sigma_2)$.
With probability $w$ we use the first component; otherwise the second.
The resulting $p(t)$ and scaling are piecewise (as summarized in the presentation).

\subsection{Results}
Table~\ref{tab:new_schedules} compares the new schedules to existing ones under the same protocol.

\begin{table}[!htbp]
  \centering
  \caption{Already-implemented schedules vs newly implemented Gaussian and bimodal Gaussian (B.G.) schedules on LM1B (400 pretraining steps + 100 fine-tuning steps).}
  \label{tab:new_schedules}
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{1.02}
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{c l c c | l c c}
    \toprule
    \multirow{2}{*}{Block size} & \multicolumn{3}{c|}{\textbf{Already implemented}} & \multicolumn{3}{c}{\textbf{Newly implemented}} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    & Noise schedule & PPL & Var.\ NELBO & Noise schedule & PPL & Var.\ NELBO \\
    \midrule
    \multirow{4}{*}{128} & Loglinear & 2106 & 1.27 & Gaussian ($\mu=0.5$) & 2115 & 1.29 \\
    & Loglinear $\mathcal{U}[0,0.5]$ & 2106 & 1.27 & Gaussian ($\mu=0.6$) & 2212 & 1.34 \\
    & Cosine & 2154 & 1.31 & B.G. ($\mu_1=0.3,w_1=0.6$) & 2184 & 1.31 \\
    & Cosine $\mathcal{U}[0,0.5]$ & 2150 & 1.30 & B.G. ($\mu_1=0.1,w_1=0.6$) & \textbf{2089} & \textbf{1.19} \\
    \midrule
    \multirow{4}{*}{16} & Loglinear & 1279 & 10.50 & Gaussian ($\mu=0.5$) & \textbf{1234} & \textbf{10.28} \\
    & Loglinear $\mathcal{U}[0.3,0.8]$ & 1278 & 10.51 & Gaussian ($\mu=0.6$) & 1235 & 10.31 \\
    & Cosine & 1236 & 10.30 & B.G. ($\mu_1=0.3,w_1=0.6$) & 1254 & 10.42 \\
    & Cosine $\mathcal{U}[0.3,0.8]$ & 1235 & 10.29 & B.G. ($\mu_1=0.1,w_1=0.6$) & 1295 & 10.59 \\
    \midrule
    \multirow{4}{*}{4} & Loglinear & \textbf{1226} & \textbf{44.41} & Gaussian ($\mu=0.5$) & 1250 & 46.76 \\
    & Loglinear $\mathcal{U}[0.5,1]$ & \textbf{1226} & \textbf{44.41} & Gaussian ($\mu=0.7$) & 1252 & 46.76 \\
    & Cosine & 1228 & 45.28 & B.G. ($\mu_1=0.3,w_1=0.6$) & 1253 & 46.84 \\
    & Cosine $\mathcal{U}[0.5,1]$ & 1229 & 45.26 & B.G. ($\mu_1=0.1,w_1=0.6$) & 1243 & 46.49 \\
    \bottomrule
  \end{tabular}}
\end{table}

We also tested using the bimodal Gaussian schedule during pretraining; Table~\ref{tab:bg_pretraining} reports the best fine-tuning schedule found per pretraining schedule.

\begin{table}[!htbp]
  \centering
  \caption{Applying bimodal Gaussian during pretraining on LM1B: for each pretraining schedule we report the best fine-tuning schedule found (400 pretraining steps + 100 fine-tuning steps).}
  \label{tab:bg_pretraining}
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.08}
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{c l l r r}
    \toprule
    Block size & Pretraining schedule & Fine-tuning schedule & PPL & Var.\ NELBO \\
    \midrule
    \multirow{2}{*}{128} & Loglinear & B.G. ($\mu_1=0.1,w_1=0.6$) & 2089 & 1.25 \\
    & Bimodal Gaussian & B.G. ($\mu_1=0.3,w_1=0.6$) & \textbf{2070} & \textbf{1.19} \\
    \midrule
    \multirow{2}{*}{16} & Loglinear & Gaussian ($\mu=0.5$) & 1234 & 10.28 \\
    & Bimodal Gaussian & Cosine $\mathcal{U}[0.3,0.8]$ & \textbf{1227} & \textbf{10.22} \\
    \midrule
    \multirow{2}{*}{4} & Loglinear & Loglinear & 1226 & 44.41 \\
    & Bimodal Gaussian & Loglinear & \textbf{1213} & \textbf{44.39} \\
    \bottomrule
  \end{tabular}}
\end{table}

\section{Extension 2: Loss Reweighting for the Masked Diffusion Objective}
\FloatBarrier
A second direction is to reweight the masked diffusion NELBO.
Starting from a weighted objective
\begin{equation}
  \mathcal{L}^{\tilde{w}}(\mathbf{x})
  = -\int_{0}^{1} \tilde{w}(t)\,\frac{\alpha'_t}{1-\alpha_t}
  \mathbb{E}_{q(\mathbf{z}_t\mid\mathbf{x})}\left[\delta_{\mathbf{z}_t,m}\cdot \mathbf{x}^{\top}\log\mu_{\theta}(\mathbf{z}_t)\right] \mathrm{d}t,
\end{equation}
we can reparameterize via $\lambda(t)=\log\frac{\alpha_t}{1-\alpha_t}$ and express weighting in the $\lambda$ domain.
We evaluated multiple weightings migrated from continuous-time diffusion and a simple baseline weighting.

\begin{figure}[!htbp]
  \centering
  \safeincludegraphics[width=0.95\linewidth]{reweighting.png}
  \caption{Illustration of reweighting schemes (from the final presentation).}
  \label{fig:reweighting}
\end{figure}

Table~\ref{tab:reweighting_defs} lists the investigated weighting functions.

\begin{table}[!htbp]
  \centering
  \caption{Weighting functions investigated for masked diffusion models (as reported in the final presentation).}
  \label{tab:reweighting_defs}
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{lccc}
    \toprule
    Name & $\lambda(t)$ & $\hat{w}(\lambda)$ & $\tilde{w}(t)$ \\
    \midrule
    EDM & \multirow{5}{*}{$\log \frac{\alpha_t}{1-\alpha_t}$} & $p_{\mathcal{N}(2.4,2.4^2)}(\lambda)\frac{e^{-\lambda}+0.5^{2}}{0.5^{2}}$ & $w(\lambda(t))$ \\
    IDDPM & & $\mathrm{sech}(\tfrac{\lambda}{2})$ & $2\sqrt{\alpha_t(1-\alpha_t)}$ \\
    Sigmoid & & $\mathrm{sigmoid}(-\lambda + k)$ & $\frac{1-\alpha_t}{1-(1-e^{-k})\alpha_t}$ \\
    FM & & $e^{-\tfrac{\lambda}{2}}$ & $\sqrt{\frac{1-\alpha_t}{\alpha_t}}$ \\
    Simple & & -- & $-\frac{1-\alpha_t}{\alpha'_t}$ \\
    \bottomrule
  \end{tabular}
\end{table}

Finally, Table~\ref{tab:reweighted_results} reports the test perplexities for the different reweightings.

\begin{table}[!htbp]
  \centering
  \caption{Test perplexities under different loss reweightings (from the final presentation).}
  \label{tab:reweighted_results}
  \setlength{\tabcolsep}{3.5pt}
  \renewcommand{\arraystretch}{1.08}
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{lcccccc}
    \toprule
    & \multicolumn{6}{c}{PPL ($\downarrow$)} \\
    \cmidrule(lr){2-7}
    & Base & IDDPM & EDM & \makecell{Sigmoid\\($k=0$)} & FM & Simple \\
    \midrule
    \multicolumn{7}{l}{\textbf{Autoregressive}} \\
    Transformer & \multicolumn{6}{c}{1221} \\
    \midrule
    \multicolumn{7}{l}{\textbf{Diffusion}} \\
    SEDD & \multicolumn{6}{c}{1403} \\
    MDLM & \multicolumn{6}{c}{1370} \\
    \midrule
    \multicolumn{7}{l}{\textbf{Block diffusion}} \\
    BD3-LM $L'=16$ & 1345 & 8048.88 & 1593.4 & 1150.15 & 76213 & 53070 \\
    BD3-LM $L'=8$ & 1210 & 7954.17 & 1569.75 & \textbf{1078.57} & 109169 & 36010741760 \\
    BD3-LM $L'=4$ & 1176 & 7857.33 & 1565.6 & 1093.7 & 67332 & 2396260 \\
    \bottomrule
  \end{tabular}}
\end{table}

In these runs, sigmoid reweighting yields the best PPL for $L'=8$, but several weightings can become unstable (very large perplexities), highlighting the need for careful constraints (e.g., monotonicity conditions with specific schedules).

\section{Discussion}
\FloatBarrier
Across reproductions and extensions we consistently observe:
\begin{itemize}
  \item \textbf{Smaller blocks help perplexity:} decreasing $L'$ often improves PPL, consistent with approaching the AR regime.
  \item \textbf{Variance--PPL trade-off:} clipped schedules reduce Var.\ NELBO, but the best perplexity can come from standard schedules after sufficient optimization.
  \item \textbf{Length behavior differs qualitatively:} BD3-LMs can exceed fixed-length diffusion caps because generation proceeds block-by-block.
\end{itemize}

\section{Conclusion and Future Work}
\FloatBarrier
Our reproduction supports the main motivation for BD3-LMs: they interpolate between AR and diffusion and, under comparable budgets, can achieve strong perplexity and improved sampling quality relative to diffusion baselines, while retaining variable-length generation.
We also found that (i) bimodal Gaussian schedules can improve PPL/variance in specific settings and (ii) loss reweighting (notably sigmoid weighting in our runs) can yield additional improvements but may be unstable.

Promising next steps include frequency-informed masking (masking rare, information-rich tokens more often), combining both extensions in a single training run, and scaling compute to reduce small-model noise.

\FloatBarrier
\section*{References}
\begin{thebibliography}{9}
\bibitem{bd3lm}
Arriola, Gokaslan, Chiu, Yang, Qi, Han, Sahoo, Kuleshov.
\newblock \emph{Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models} (ICLR 2025).
\newblock Project code: \url{https://github.com/kuleshov-group/bd3lms}.

\bibitem{ourrepo}
Team 6 (NTUA).
\newblock Reproduction and extensions repository: \url{https://github.com/ntua-el21050/bd3lms}.
\end{thebibliography}

\end{document}
