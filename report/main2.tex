\documentclass{article}

\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{array}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{float}
\usepackage{flafter}
\usepackage[section]{placeins}
\usepackage{soul}

\usepackage[backend=biber]{biblatex}
\addbibresource{bibliography.bib}

% --- ADDED PACKAGE FOR FIXING TABLE SIZES ---
\usepackage{tabularx}
% Define a centered flexible column for tabularx
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
% --------------------------------------------

% Float tuning: keep floats near where they are introduced.
\setcounter{topnumber}{5}
\setcounter{bottomnumber}{5}
\setcounter{totalnumber}{10}
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.95}
\renewcommand{\bottomfraction}{0.95}
\renewcommand{\floatpagefraction}{0.9}

% Try to include figures from either report/media/ or ../final_presentation/
\graphicspath{{media/}{../final_presentation/}}
\newcommand{\safeincludegraphics}[2][]{%
  \IfFileExists{#2}{\includegraphics[#1]{#2}}{%
    \IfFileExists{../final_presentation/#2}{\includegraphics[#1]{../final_presentation/#2}}{\fbox{Missing file: #2}}%
  }%
}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\beps}{\bm{\epsilon}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\z}[1]{\bz_{t({#1})}}
\newcommand{\zs}[1]{\bz_{s({#1})}}
\newcommand{\kl}{\mathrm{KL}}
\newcommand{\KL}[2]{\mathrm{KL}({#1}\|{#2})}
\newcommand{\snr}{\text{SNR}}
\newcommand{\twonorm}[1]{\|{#1}\|_2}
\newcommand{\diff}{\mathop{}\!\mathrm{d}}


% Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{\textit{ }}
\fancyhead[LO]{Block Diffusion (BD3-LMs): Reproduction and Extensions}

%% Title
\title{Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models\supercite{arriola2025block}\\
\vspace{0.25em}
\large Reproduction Results and Two Practical Extensions}

\author{
  \textbf{Nikolaos Kordas$^{1}$ \quad Konstantinos Kritharidis$^{1}$ \quad Ilias Makras$^{1}$} \quad\\
  \textbf{Georgios Markoulidakis$^{1}$ \quad Georgios Ntountounakis$^{1}$ \quad Petros Vitalis$^{1}$}\\
  \\
  Pattern Recognition Course, National Technical University of Athens\\
  \\
  Scientific Partner: \textbf{Efthymios Georgiou$^{3}$}\\
  Course Instructor: \textbf{Alexandros Potamianos$^{1,2}$}\\
  \\
  $^{1}$ National Technical University of Athens \\
  $^{2}$ Archimedes RU, Athena RC \\
  $^{3}$ University of Bern
}

\begin{document}
\maketitle

\begin{abstract}
In this report, we are evaluating the Block Discrete Denoising Diffusion Language
Models (BD3-LMs), a class of models designed to combine the advantages of Autoregressive (AR) and Discrete Diffusion models.
BD3-LMs maintain an autoregressive distribution over blocks while performing parallelized discrete diffusion
within each block. We present a reproduction of the original experiments, as well as ideas for extensions that improve the performance of BD3-LMs.
Specifically, we explore two new noise schedules and loss reweighting. Our reproduction
confirms that BD3-LMs outperform standard diffusion models, achieving variable-length generation
and high quality sampling efficiency, even under significant resource constraints.
Our proposed extensions further enhance the performance of BD3-LMs, demonstrating the potential
of this class of models for efficient language generation.

\vspace{0.5em}
\noindent\textbf{Resources:} Original codebase: \url{https://github.com/kuleshov-group/bd3lms}. Our repository: \url{https://github.com/ntua-el21050/bd3lms}.
\end{abstract}

\section{Introduction}
Modern language models are typically built using one of two generation paradigms.
\emph{Autoregressive} (AR) models generate one token at a time, which empirically yields strong likelihoods and high sample quality, supports key-value (KV) caching, and naturally allows variable-length generation.
\emph{Diffusion} models instead aim to generate (or refine) many tokens in parallel through iterative denoising steps, which can improve controllability and parallelism, but often suffers from a perplexity gap and is limited to fixed-length outputs.

Block diffusion (BD3-LMs) is a hybrid: it is autoregressive over blocks and performs diffusion within each block.
This design targets a controllable trade-off between quality and parallelism via the block size $L'$.
At the extremes, $L'=1$ reduces to token-level AR, while $L'=L$ (the full sequence length) becomes a fully parallel diffusion model.

\section{Block Diffusion Models (BD3-LMs)}
BD3-LMs factorize a sequence into contiguous blocks of length $L'$.
Generation proceeds autoregressively at the block level: each block is generated conditioned on the previous blocks.
Within the current block, denoising is performed using a masked diffusion process, enabling parallel prediction of the tokens in the block.

This hybridization yields:
\begin{itemize}
  \item \textbf{Parallelism within blocks:} enables faster refinement than strictly token-by-token generation.
  \item \textbf{Autoregressive control across blocks:} enables variable-length generation and mitigates some diffusion limitations.
  \item \textbf{Block size parameter $L'$:} a direct trade-off between quality and parallelism.
\end{itemize}

\section{Background: Masked Discrete Diffusion and Noise Schedules}
We focus on masked discrete diffusion, where noising corresponds to masking tokens with probability $p(t)$ as a function of a continuous time index $t\sim\mathcal{U}[0,1]$. $t$ essentially corresponds to the noise level to be applied. In general, larger values of $t$ are associated with a higher proportion of masking, whereas smaller values of t correspond to a lower proportion of masking. However, the exact masking probability $p(t)$, is determined by the specific noise schedule used. Accordingly, the complementary keep (no-mask) probability is defined as:
\begin{equation}
  a(t) = 1 - p(t).
\end{equation}
Another quantity derived as a function of $t$ is the loss scaling:
\begin{equation}
  \text{loss\_scaling}(t) 
  = \frac{a'(t)}{1-a(t)}
  = -\frac{p'(t)}{p(t)},
\end{equation}
which acts as a weighting of per-token log-likelihood terms across noise levels during training.

We consider standard schedules (loglinear, square, square root, logarithmic, cosine) and also investigate clipped schedules, where $t$ is sampled from a restricted interval $\mathcal{U}[\tau_{\min},\tau_{\max}]$.

\section{Experimental Protocol and Metrics}
To evaluate the efficacy of Block Diffusion models (BD3-LMs) against Autoregressive (AR) and standard Diffusion baselines (SEDD, MDLM), we adapted our experimentation protocol to our resource constraints.

\subsection{Model Configuration and Datasets}
Due to computational limitations, we utilized a \textbf{tiny} transformer configuration for all models to ensure a fair comparison under identical resource budgets.
\begin{itemize}
    \item \textbf{Sequence Length:} A key feature of BD3-LMs is variable-length generation. We configured the BD3-LM with a maximum model length of \textbf{16K tokens} to test generation capabilities beyond the standard fixed-length caps (e.g., 1024 tokens) of baseline diffusion models.
    \item \textbf{Datasets:} We primarily utilized \textbf{LM1B} (One Billion Word Benchmark) and \textbf{OpenWebText (OWT)} for training and perplexity evaluations. For zero-shot transfer evaluation, models trained on OWT were assessed on \textbf{Lambada} and \textbf{Wikitext}.
\end{itemize}

\subsection{Training Regimen}
Our training protocol consists of distinct pretraining and fine-tuning phases. The specific step counts varied by experiment to balance convergence with compute availability:
\begin{itemize}
    \item \textbf{Noise Schedule Ablations (LM1B):} 400 pretraining steps followed by 100 fine-tuning steps.
    \item \textbf{Large-Scale Comparisons (OWT):} Extended training of 3000 pretraining steps and 3000 fine-tuning steps.
    \item \textbf{Transfer \& Sampling:} 800 pretraining steps followed by 500--800 fine-tuning steps.
\end{itemize}

\subsection{Evaluation Metrics}
We employed a comprehensive set of metrics to evaluate likelihood, stability, and sample quality, following the analysis of the original paper:
\begin{itemize}
    \item \textbf{Test Perplexity (PPL):} We report test set perplexity, where lower values indicate better predictive performance.
    \item \textbf{Objective Variance (Var.\ NELBO):} To quantify training stability, particularly regarding noise schedules, we measure the Variance of the Negative Evidence Lower Bound, where again lower values are better.
    \item \textbf{Sampling Quality (Gen.PPL):} We evaluate the quality of generated text using Generative Perplexity calculated on 300 samples per model.
    \item \textbf{Efficiency (NFEs):} We track the Number of Function Evaluations required for generation to measure sampling efficiency.
\end{itemize}

\section{Reproduction Results}

Below, we reproduce all tables from the original paper.

\subsection{AR vs BD3-LM with $L'=1$}
The first experiment we reproduced, was the comparison between AR and BD3-LM models with block size equal to 1.
As the authors of the original paper explain, the expected value of the variance NELBO of the AR and the BD3-LM with $L'=1$ are equal. What the authors found though, was that in practice the AR achieves better perplexity score than the BD3-LM. They explained that this was due to higher training variance in the BD3-LM's case. Using their default Log-linear noise schedule, meant that only half of the tokens would be masked. As a result the training cross-entropy would not be computed for half the tokens. They managed to overcome this, by setting the mask probability equal to 1, using a tuned schedule where $t\sim\mathcal{U}[1,1]$.

Table~\ref{tab:ar_vs_bd3lm_l1} compares token-level AR and BD3-LM with $L'=1$ on LM1B.

\begin{table}[!htbp]
  \centering
  \small % Added small for consistency
  \caption{Test perplexities for single-token generation on LM1B (800 training steps).}
  \label{tab:ar_vs_bd3lm_l1}
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{lc}
    \toprule
    & PPL ($\downarrow$) \\
    \midrule
    Autoregressive (AR) & \textbf{1893} \\
    BD3-LM ($L'=1$) & 2231 \\
    BD3-LM ($L'=1$) + tuned schedule & 2220 \\
    \bottomrule
  \end{tabular}
\end{table}

We also include the training curves used in the presentation (Figure~\ref{fig:l1_curves}) to visually compare training nll behavior.

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.32\linewidth}
    \centering
    \safeincludegraphics[width=\linewidth]{fig1.png}
    \caption{AR}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.32\linewidth}
    \centering
    \safeincludegraphics[width=\linewidth]{fig2.png}
    \caption{BD3-LM}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.32\linewidth}
    \centering
    \safeincludegraphics[width=\linewidth]{fig3.png}
    \caption{BD3-LM + tuned schedule}
  \end{subfigure}
  \caption{Training NLL curves for single-token generation on LM1B.}
  \label{fig:l1_curves}
\end{figure}

We indeed verify that the AR model scores better PPL than the BD3-LM model, but we fail to reproduce the same improvement when using the tuned schedule. The perplexity gap is not closed in this case, although it is slightly narrowed. With the help of the training nll curves, we observe that the BD3-LM models clearly suffer more from overfitting. Consequently, in our case the problem is not the higher variance, but the overfitting which is obviously caused by our limited resources. This of course becomes apparent if we compare our ppl scores with the ones of the original paper, where the AR scores 22.88 and the BD3-LM scores 25.56.

\subsection{Effect of clipped noise schedules}
Table~\ref{tab:clipping_effect} reports PPL and Var.\ NELBO for several $L'$ values under clipped and unclipped uniform time sampling on LM1B.

\begin{table}[!htbp]
  \centering
  \small % Added small for consistency
  \caption{Effect of clipped noise schedules on LM1B (400 pretraining steps + 100 fine-tuning steps).}
  \label{tab:clipping_effect}
  \setlength{\tabcolsep}{5pt}
  \renewcommand{\arraystretch}{1.12}
  \begin{tabular}{l l c c}
    \toprule
    $L'$ & Clipping & PPL ($\downarrow$) & Var.\ NELBO ($\downarrow$) \\
    \midrule
    \multirow{2}{*}{128} & $\mathcal{U}[0,0.5]$ & \textbf{2106} & \textbf{1.27} \\
    & $\mathcal{U}[0,1]$ & \textbf{2106} & \textbf{1.27} \\
    \midrule
    \multirow{2}{*}{16} & $\mathcal{U}[0.3,0.8]$ & \textbf{1278} & \textbf{10.50} \\
    & $\mathcal{U}[0,1]$ & 1279 & 10.51 \\
    \midrule
    \multirow{2}{*}{4} & $\mathcal{U}[0.5,1]$ & \textbf{1226} & \textbf{44.41} \\
    & $\mathcal{U}[0,1]$ & \textbf{1226} & \textbf{44.41} \\
    \bottomrule
  \end{tabular}
\end{table}

Under our reproduced settings, clipping is most clearly beneficial for $L'=16$ (small but consistent improvements in both PPL and variance), whereas for $L'=4$ and $L'=128$ the effect is negligible.

\subsection{LM1B and OWT comparisons}
Table~\ref{tab:lm1b_main} compares AR, diffusion baselines, and BD3LM
models pre-trained using block size L' = 128 and
fine-tuned under varying L' on the LM1B Dataset. We observe a performance gap with regards 
to the AR model
between our experiment and the one presented in the original paper. Specifically,
our reproduced AR is the worst performing model on LM1B, whereas it achieves best performance in
the original paper. The gap is due to our limited resources and therefore our inability to train the model
for more gradient steps.
However, we can still see that the best BD3LM model in our reproduction is the one with $L'=4$, which
is also true in the original paper. Additionally, the BD3-LMs outperform the diffusion baselines, with the exception of
$L'=8$, which performed significantly worse, also due to resource constraints.

\begin{table}[H]
  \centering
  \small
  \caption{Test perplexities on LM1B (400 pretraining steps + 100 fine-tuning steps).}
  \label{tab:lm1b_main}
  \setlength{\tabcolsep}{10pt}
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{l r}
    \toprule
    Model & PPL ($\downarrow$) \\
    \midrule
    \multicolumn{2}{l}{\textbf{Autoregressive}} \\
    Transformer & 3042 \\
    \midrule
    \multicolumn{2}{l}{\textbf{Diffusion}} \\
    SEDD & 1447 \\
    MDLM & 1616 \\
    \midrule
    \multicolumn{2}{l}{\textbf{Block diffusion}} \\
    BD3-LM $L'=16$ & 1278 \\
    BD3-LM $L'=8$ & 1734 \\
    BD3-LM $L'=4$ & \textbf{1226} \\
    \bottomrule
  \end{tabular}
\end{table}

Table~\ref{tab:owt_main} compares AR, diffusion baselines, and BD3LM models on OWT.


\begin{table}[H]
  \centering
  \small
  \caption{Test perplexities on OWT (800 pretraining + 100 fine-tuning steps).}
  \label{tab:owt_main}
  \setlength{\tabcolsep}{12pt}
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{lc}
    \toprule
    Model & PPL ($\downarrow$) \\
    \midrule
    \textbf{Autoregressive} & \\
    Transformer & 2010 \\
    \midrule
    \textbf{Diffusion} & \\
    SEDD & 2093 \\
    MDLM & 2073 \\
    \midrule
    \textbf{Block diffusion} & \\
    BD3-LM $L'=16$ & 2003 \\
    BD3-LM $L'=8$ & 2080 \\
    BD3-LM $L'=4$ & \textbf{2002} \\
    \bottomrule
  \end{tabular}
\end{table}


On OWT, all models achieve comparable perplexities, with BD3-LM $L'=4$ marginally outperforming AR . Consistent with the paper, AR 
outperforms the diffusion baselines (SEDD, MDLM).However,BD3-LM outperforms AR at our scale. With extended training 
(3000+3000 steps), all models improve dramatically in PPL and AR overtakes BD3-LM recovering the paper's expected ordering.This implies that the BD3-LM advantage at 800 steps reflects faster early convergence 
Notably, BD3-LM $L'=8$ performs worse than both $L'=4$ and $L'=16$ at 800 training steps, mirroring the pattern observed on LM1B suggesting that this intermediate block size requires more training to converge effectively. 
The diffusion,except BD3-LM $L'=8$, baseline ordering SEDD > MDLM > BD3-LM is preserved in both configurations, consistent with the paper.


\subsection{Zero-Shot Transfer Evaluation(trained on OWT)}

Table~\ref{tab:transfer} evaluates zero-shot generalization: models trained exclusively on OWT are evaluated on held-out datasets without any additional training.
We focused on three representative datasets: LM1B and Lambada
which differ substantially from OWT in domain and style and Wikitext, which is structurally similar to OWT as web/encyclopedic text.The paper evaluates on seven benchmarks (PTB, Wikitext, LM1B, Lambada, AG~News, Pubmed, Arxiv).

\begin{table}[!htbp]
  \centering
  \small
  \caption{Zero-shot validation perplexities of models trained on OWT (800 pretraining steps + 800 fine-tuning steps).}
  \label{tab:transfer}
  \setlength{\tabcolsep}{12pt}
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{lccc}
    \toprule
    & LM1B & Lambada & Wikitext \\
    \midrule
    AR & 2388 & 1550 & \textbf{2875} \\
    \midrule
    SEDD & 2742 & 1562 & 3335 \\
    MDLM & 2722 & 1556 & 3283 \\
    BD3-LM $L'=4$ & \textbf{2196} & \textbf{1438} & 3143 \\
    \bottomrule
  \end{tabular}
\end{table}

Examining Table~\ref{tab:transfer}, BD3-LM's advantage over AR is 
domain-dependent: it outperforms AR on out-of-domain benchmarks (LM1B, 
Lambada) but not on similar-domain Wikitext, suggesting that bidirectional 
denoising learns more transferable representations than next-token prediction. 

At our reduced training scale, two notable differences from the paper's results emerge. Specifically.BD3-LM beats AR, whereas the paper reports the opposite,which we attribute this 
to BD3-LM's faster convergence under limited training, amplified by the domain shift from OWT to news text. On Lambada, BD3-LM leads, while the 
paper's winner, MDLM, falls near AR, suggesting that MDLM requires substantially more training to develop the long-range dependencies that BD3-LM's block structure captures more readily, even at a small scale. On Wikitext, both settings agree: AR wins, and BD3-LM outperforms the other diffusion baselines, fully consistent with the paper.
\subsection{Variable-length generation}
Table~\ref{tab:gen_length_stats} highlights a core benefit of block diffusion over pure diffusion: longer generations are possible because blocks are generated autoregressively.

\begin{table}[!htbp]
  \centering
  \small
  \caption{Generation length statistics for 10 sampled documents from OWT-trained models (800 pretraining steps + 500 fine-tuning steps). BD3-LM reproduction with model length $=16$K.}
  \label{tab:gen_length_stats}
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{lrr}
    \toprule
    & \makecell{Median\\\# tokens} & \makecell{Max\\\# tokens} \\
    \midrule
    OWT train set & 717 & 131K \\
    AR & 16384 & 16384 \\
    \midrule
    SEDD & 1000 & 1000 \\
    BD3-LM $L'=16$ & 798 & 2927 \\
    \bottomrule
  \end{tabular}
\end{table}

We observe that, indeed, BD3-LMs overcome the fixed-length generation limitation of diffusion models, creating outputs that can go well-beyond the context length of their training (1024). However, still the AR model achieves far longer generation, as happens in the original papers as well. The fact that the median number of tokens in the AR model is equal to the maximum number of tokens, is explained by the fact that with only 800 training steps, the AR model doesn't have the time to properly train, and as a result, doesn't learn when to stop generating outputs. This is in accordance to our empirical evaluation of the quality of the generation results, which was poor for the AR model but far better for the BD3-LM (with 800 pretraining and 500 fine-tuning steps).

\subsection{Sample quality and schedule ablations}
We report (i) sampling quality in terms of Gen.PPL and NFEs (Table~\ref{tab:sample_quality}) and (ii) an ablation over noise schedules (Table~\ref{tab:noise_schedule_ablation}). \\

\noindent Specifically, in discrete diffusion the NFEs (number of generation steps) is upper bounded by the context length $L$. Thus, our reproduction resembles the paper's experiments. On the other hand, we observe that BD3-LMs achieve better PPL scores faster than their AR counterpart. 

\begin{table}[!htbp]
  \centering
  \small
  \caption{Generative perplexity (Gen.PPL; $\downarrow$) and number of function evaluations (NFEs; $\downarrow$) for 300 samples. Models trained on OWT (400 + 100 training steps).}
  \label{tab:sample_quality}
  \setlength{\tabcolsep}{8pt}
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{lcc}
    \toprule
    Model & Gen.PPL & NFEs \\
    \midrule
    AR & 79165 & 1024 \\
    SEDD & 29987 & 1023 \\
    MDLM & 25632 & 1023 \\
    BD3-LM $L'=16$ & \textbf{7576} & 1023 \\
    BD3-LM $L'=8$ & 8176 & 1023 \\
    BD3-LM $L'=4$ & 9785 & 1023 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[!htbp]
  \centering
  \small
  \caption{Effect of noise schedule on PPL and Var.\ NELBO for different $L'$ on LM1B (5000 + 3000 training steps).}
  \label{tab:noise_schedule_ablation}
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.05}
  \begin{tabular}{lcc}
    \toprule
    Noise schedule & PPL ($\downarrow$) & Var.\ NELBO ($\downarrow$) \\
    \midrule
    \multicolumn{3}{l}{\textbf{$L'=4$}} \\
    \quad Clipped $\mathcal{U}[0.45,0.95]$ & 1199 & \textbf{15.23} \\
    \quad Clipped $\mathcal{U}[0.3,0.8]$ & 1101 & 27.61 \\
    \quad Linear $\mathcal{U}[0,1]$ & 751 & 260.10 \\
    \quad Logarithmic & 750 & 125.62 \\
    \quad Square root & \textbf{719} & 83.83 \\
    \midrule
    \multicolumn{3}{l}{\textbf{$L'=16$}} \\
    \quad Clipped $\mathcal{U}[0.45,0.95]$ & 1056 & \textbf{4.35} \\
    \quad Clipped $\mathcal{U}[0.3,0.8]$ & 798 & 6.41 \\
    \quad Linear $\mathcal{U}[0,1]$ & 662 & 44.73 \\
    \quad Square & \textbf{627} & 27.53 \\
    \quad Cosine & 634 & 20.80 \\
    \bottomrule
  \end{tabular}
\end{table}

Table 8 highlights a clear and consistent pattern 
across both block sizes: clipped schedules reliably reduce the variance of the NELBO 
gradient estimator by an order of magnitude compared to standard schedules, with 
heavier clipping producing systematically lower variance. This confirms that extreme 
mask rates introduce high gradient variance and that clipping the sampling range 
effectively stabilizes training. 
However, while the variance ranking is faithfully 
reproduced, the PPL ranking is inverted at our scale: standard schedules achieve 
substantially lower perplexity than clipped schedules for both $L'=4$ and $L'=16$, 
contrary to the paper's ordering. We strongly believe this inversion is primarily 
due to insufficient training. Clipped schedules reduce gradient variance to improve 
long-term convergence stability, but standard schedules expose the model to the full 
range of noise levels from the outset, enabling faster initial learning. At our limited 
training budget, the models have simply not trained long enough for the variance 
advantage to compound into a measurable PPL improvement. Standard schedules 
consistently outperform clipped ones in PPL at our scale for both block sizes, and 
the relative ordering among them is largely preserved from the paper, with the 
exception of Square, which benefits notably from limited training budgets.

Regarding block-size-specific masking, our results for $L'=4$ confirm that heavy 
masking ($\mathcal{U}[0.45,0.95]$) achieves the lowest variance among all schedules, 
consistent with the intuition that small blocks, having limited context, benefit from 
aggressive masking; however, this variance advantage has not yet translated into superior 
PPL at our training scale. 
For $L'=16$, $\mathcal{U}[0.3,0.8]$ substantially outperforms 
$\mathcal{U}[0.45,0.95]$ in PPL, partially supporting the prediction that larger 
blocks prefer lighter masking, but $\mathcal{U}[0.45,0.95]$ still achieves the lower 
variance, and standard schedules outperform both clipped schedules in PPL. As with the 
overall PPL inversion, we attribute these discrepancies to the limited training budget.


\section{Extension 1: Alternative Noise Schedules}
\FloatBarrier
\subsection{Already Implemented Noise Schedules}
Table~\ref{tab:implemented_schedules} summarizes the noise schedules already provided in the codebase, expressed in terms of the masked probability $p(t)$ and the induced loss scaling $\text{loss\_scaling}(t)=-\frac{p'(t)}{p(t)}$.

\begin{table}[H]
  \centering
  \small
  \caption{Already implemented noise schedules: masked probability $p(t)$ and induced loss scaling.}
  \label{tab:implemented_schedules}
  \setlength{\tabcolsep}{9pt}
  \renewcommand{\arraystretch}{1.35}
  \begin{tabular}{@{}l l l@{}}
    \toprule
    \textbf{Schedule} & \textbf{$p(t)$} & \textbf{$\text{loss\_scaling}(t)$} \\
    \midrule
    LogLinear & $t$ & $-\dfrac{1}{t}$ \\
    Square & $t^{2}$ & $-\dfrac{2}{t}$ \\
    Square root & $t^{0.5}$ & $-\dfrac{1}{2t}$ \\
    Logarithmic & $\dfrac{\log(1+t)}{\log 2}$ & $-\dfrac{1}{(1+t)\,\log(1+t)}$ \\
    Cosine & $1-(1-\varepsilon)\cos\!\left(\dfrac{\pi t}{2}\right)$ & $-\dfrac{\left(\frac{\pi}{2}\right)(1-\varepsilon)\sin\!\left(\frac{\pi t}{2}\right)}{1-(1-\varepsilon)\cos\!\left(\frac{\pi t}{2}\right)}$ \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Gaussian \& Bimodal Gaussian Noise Schedules}
As our first extension we adopt two new noise schedules suggested in \cite{kosmopoulou-etal-2025-masked} that directly sample the masked probability $p(t)\in(0,1)$.
For a (truncated) Gaussian schedule, given mean $\mu$ and standard deviation $\sigma$, define $\alpha=\frac{0-\mu}{\sigma}$, $\beta=\frac{1-\mu}{\sigma}$, $\Phi$ the standard normal CDF, and $\varphi$ the pdf.
Let $\Phi_{\alpha}=\Phi(\alpha)$, $\Phi_{\beta}=\Phi(\beta)$, and $Z=\Phi_{\beta}-\Phi_{\alpha}$.
Then for $t\in(0,1)$,
\begin{equation}
  z(t)=\Phi^{-1}\!\big(\Phi_{\alpha}+ t(\Phi_{\beta}-\Phi_{\alpha})\big), \qquad p(t)=\mu+\sigma z(t)\in(0,1).
\end{equation}
The induced scaling is
\begin{equation}
  \text{loss\_scaling}(t) = -\frac{p'(t)}{p(t)} = -\frac{\sigma Z}{\varphi(z(t))\,p(t)}.
\end{equation}

For a bimodal Gaussian schedule, we choose a mixture weight $w\in(0,1)$ and two Gaussians $(\mu_1,\sigma_1)$ and $(\mu_2,\sigma_2)$.
With probability $w$ we use the first component; otherwise the second.
Writing $w_1=w$ and $w_2=1-w$, we define a split time coordinate
\begin{equation}
  t_1 = \frac{t}{w_1}\;\; (t<w_1), \qquad t_2 = \frac{t-w_1}{w_2}\;\; (t\ge w_1),
\end{equation}
and sample
\begin{equation}
  p(t) =
  \begin{cases}
    \mu_{1}+\sigma_{1}z_{1}(t_{1}), & t<w_{1}, \\
    \mu_{2}+\sigma_{2}z_{2}(t_{2}), & t\ge w_{1},
  \end{cases}
\end{equation}
where each $z_i(\cdot)$ is defined as in the Gaussian case but with its own truncation constants $\alpha_i=\frac{0-\mu_i}{\sigma_i}$, $\beta_i=\frac{1-\mu_i}{\sigma_i}$ and $Z_i=\Phi_{\beta_i}-\Phi_{\alpha_i}$.
The induced scaling is also piecewise:
\begin{equation}
  \text{loss\_scaling}(t)
  = -\frac{1}{p(t)}
  \begin{cases}
    \frac{1}{w_1}\frac{\sigma_1 Z_1}{\varphi(z_1(t_1))}, & t<w_{1}, \\
    \frac{1}{w_2}\frac{\sigma_2 Z_2}{\varphi(z_2(t_2))}, & t\ge w_{1}.
  \end{cases}
\end{equation}

\subsection{Results}
Table~\ref{tab:new_schedules} compares the new schedules to existing ones under the same protocol.

\begin{table}[!htbp]
  \centering
  \small % Added small, used tabularx instead of resizebox
  \caption{Already-implemented schedules vs newly implemented Gaussian and bimodal Gaussian (B.G.) schedules on LM1B (400 pretraining steps + 100 fine-tuning steps).}
  \label{tab:new_schedules}
  \setlength{\tabcolsep}{3pt}
  \renewcommand{\arraystretch}{1.1}
  % Changed to tabularx to fit width naturally without scaling font
  \begin{tabularx}{\textwidth}{c L c c | L c c}
    \toprule
    \multirow{2}{*}{\shortstack{Block\\size}} & \multicolumn{3}{c|}{\textbf{Already implemented}} & \multicolumn{3}{c}{\textbf{Newly implemented}} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    & Noise schedule & PPL & \makecell{Var.\\NELBO} & Noise schedule & PPL & \makecell{Var.\\NELBO} \\
    \midrule
    \multirow{4}{*}{128} & Loglinear & 2106 & 1.27 & Gaussian ($\mu=0.5$) & 2115 & 1.29 \\
    & Loglinear $\mathcal{U}[0,0.5]$ & 2106 & 1.27 & Gaussian ($\mu=0.6$) & 2212 & 1.34 \\
    & Cosine & 2154 & 1.31 & B.G. ($\mu_1=0.3,w_1=0.6$) & 2184 & 1.31 \\
    & Cosine $\mathcal{U}[0,0.5]$ & 2150 & 1.30 & B.G. ($\mu_1=0.1,w_1=0.6$) & \textbf{2089} & \textbf{1.19} \\
    \midrule
    \multirow{4}{*}{16} & Loglinear & 1279 & 10.50 & Gaussian ($\mu=0.5$) & \textbf{1234} & \textbf{10.28} \\
    & Loglinear $\mathcal{U}[0.3,0.8]$ & 1278 & 10.51 & Gaussian ($\mu=0.6$) & 1235 & 10.31 \\
    & Cosine & 1236 & 10.30 & B.G. ($\mu_1=0.3,w_1=0.6$) & 1254 & 10.42 \\
    & Cosine $\mathcal{U}[0.3,0.8]$ & 1235 & 10.29 & B.G. ($\mu_1=0.1,w_1=0.6$) & 1295 & 10.59 \\
    \midrule
    \multirow{4}{*}{4} & Loglinear & \textbf{1226} & \textbf{44.41} & Gaussian ($\mu=0.5$) & 1250 & 46.76 \\
    & Loglinear $\mathcal{U}[0.5,1]$ & \textbf{1226} & \textbf{44.41} & Gaussian ($\mu=0.7$) & 1252 & 46.76 \\
    & Cosine & 1228 & 45.28 & B.G. ($\mu_1=0.3,w_1=0.6$) & 1253 & 46.84 \\
    & Cosine $\mathcal{U}[0.5,1]$ & 1229 & 45.26 & B.G. ($\mu_1=0.1,w_1=0.6$) & 1243 & 46.49 \\
    \bottomrule
  \end{tabularx}
\end{table}

We also tested using the bimodal Gaussian schedule during pretraining; Table~\ref{tab:bg_pretraining} reports the best fine-tuning schedule found per pretraining schedule.

\begin{table}[!htbp]
  \centering
  \small
  \caption{Applying bimodal Gaussian during pretraining on LM1B: for each pretraining schedule we report the best fine-tuning schedule found (400 pretraining steps + 100 fine-tuning steps).}
  \label{tab:bg_pretraining}
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{1.1}
  % Changed to tabularx
  \begin{tabularx}{\textwidth}{c L L c c}
    \toprule
    Block size & Pretraining schedule & Fine-tuning schedule & PPL & Var.\ NELBO \\
    \midrule
    \multirow{2}{*}{128} & Loglinear & B.G. ($\mu_1=0.1,w_1=0.6$) & 2089 & 1.25 \\
    & Bimodal Gaussian & B.G. ($\mu_1=0.3,w_1=0.6$) & \textbf{2070} & \textbf{1.19} \\
    \midrule
    \multirow{2}{*}{16} & Loglinear & Gaussian ($\mu=0.5$) & 1234 & 10.28 \\
    & Bimodal Gaussian & Cosine $\mathcal{U}[0.3,0.8]$ & \textbf{1227} & \textbf{10.22} \\
    \midrule
    \multirow{2}{*}{4} & Loglinear & Loglinear & 1226 & 44.41 \\
    & Bimodal Gaussian & Loglinear & \textbf{1213} & \textbf{44.39} \\
    \bottomrule
  \end{tabularx}
\end{table}

\section{Extension 2: Loss Reweighting for the Masked Diffusion Objective}
\FloatBarrier
As our second extension we adopt the reweighting technique suggested in \cite{shi2025demystifyingdiffusionobjectivesreweighted} for the masked diffusion NELBO.\\
\\
The authors start by proving two main theorems. Theorem 1 states that more "optimal" reverse steps lead to better ELBOs, where more "optimal" means using the ground truth reverse transition instead of our learned denoiser. Theorem 2 states that common diffusion objectives are a weighted sum of the improved ELBOs.\\

\begin{figure}[!htbp]
	\centering
	\safeincludegraphics[width=0.75\linewidth]{reweighting.png}
	\caption{Figure taken from \cite{shi2025demystifyingdiffusionobjectivesreweighted}: Diffusion objectives viewed as a weighted sum of the ELBOs of a sequence of models with optimal decoders}
	\label{fig:reweighting}
\end{figure}

Finally, for discrete diffusion models, the authors note that adding a reweighting term $\tilde{w}(t)$ like this:
\begin{equation}
	\cL^{\tilde{w}}(\bx) = -\int_{0}^{1}{\tilde{w}({t})
	}\frac{\alpha_{t}'}{1 - \alpha_{t}}\E_{q(\bz_t|\bx)}\left[\delta
	_{\bz_t, m}\cdot \bx^{\top}\log \mu_{\theta}(\bz_{t})\right]
	\diff t
\end{equation}
the reparameterization invariance with respect to the log-SNR that holds without reweighting breaks. Their proposed way of solving that, is by defining the reweighting term with respect to the log-SNR $\lambda(t)=\log\frac{\alpha_t}{1-\alpha_t}$ instead of time $t$, so that the loss remains invariant under the change of variable:
\begin{equation}
	\cL^{\hat{w}}(\bx) = -\int_{0}^{1}{\hat{w}({\lambda(t)})
	}\frac{\alpha_{t}'}{1 - \alpha_{t}}\E_{q(\bz_t|\bx)}\left[\delta
	_{\bz_t, m}\cdot \bx^{\top}\log \mu_{\theta}(\bz_{t})\right]
	\diff t
\end{equation}

Table~\ref{tab:reweighting_defs} lists the investigated weighting functions. In all reweighting terms except "Simple", the reparameterized version was used.

\begin{table}[!htbp]
  \centering
  \small
  \caption{Weighting functions investigated for masked diffusion models.}
  \label{tab:reweighting_defs}
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.15}
  \begin{tabular}{lccc}
    \toprule
    Name & $\lambda(t)$ & $\hat{w}(\lambda)$ & $\tilde{w}(t)$ \\
    \midrule
    EDM & \multirow{5}{*}{$\log \frac{\alpha_t}{1-\alpha_t}$} & $p_{\mathcal{N}(2.4,2.4^2)}(\lambda)\frac{e^{-\lambda}+0.5^{2}}{0.5^{2}}$ & $w(\lambda(t))$ \\
    IDDPM & & $\mathrm{sech}(\tfrac{\lambda}{2})$ & $2\sqrt{\alpha_t(1-\alpha_t)}$ \\
    Sigmoid & & $\mathrm{sigmoid}(-\lambda + k)$ & $\frac{1-\alpha_t}{1-(1-e^{-k})\alpha_t}$ \\
    FM & & $e^{-\tfrac{\lambda}{2}}$ & $\sqrt{\frac{1-\alpha_t}{\alpha_t}}$ \\
    Simple & & -- & $-\frac{1-\alpha_t}{\alpha'_t}$ \\
    \bottomrule
  \end{tabular}
\end{table}

Finally, Table~\ref{tab:reweighted_results} reports the test perplexities for the different reweightings.

\begin{table}[!htbp]
  \centering
  \small
  \caption{Test perplexities under different loss reweightings (800 pretraining steps + 800 fine-tuning steps).}
  \label{tab:reweighted_results}
  \setlength{\tabcolsep}{2pt} % Reduced spacing slightly to fit
  \renewcommand{\arraystretch}{1.1}
  % Used tabularx to ensure fit without resizing font
  \begin{tabularx}{\textwidth}{l Y Y Y Y Y Y}
    \toprule
    & \multicolumn{6}{c}{PPL ($\downarrow$)} \\
    \cmidrule(lr){2-7}
    & Base & IDDPM & EDM & \makecell{Sigmoid\\($k=0$)} & FM & Simple \\
    \midrule
    \multicolumn{7}{l}{\textbf{Autoregressive}} \\
    Transformer & 1221 \\
    \midrule
    \multicolumn{7}{l}{\textbf{Diffusion}} \\
    SEDD & 1403 \\
    MDLM & 1370 \\
    \midrule
    \multicolumn{7}{l}{\textbf{Block diffusion}} \\
    BD3-LM $L'=16$ & 1405 & 279.7 & 51.18 & 35.57 & 74017 & 43488 \\
    BD3-LM $L'=8$ & 1204 & 276.56 & 50.87 & 34.44 & 98147 & 8246395904 \\
    BD3-LM $L'=4$ & 1204 & 276.58 & 46.26 & \textbf{33.16} & 57099 & 1173706 \\
    \bottomrule
  \end{tabularx}
\end{table}

We observe that reweighting can significantly improve the test perplexity (IDDPM, EDM and Sigmoid improve over Base), with the most notable improvements achieved with EDM and Sigmoid reweighting. Sigmoid reweighting yields the best PPL for $L'=4$, far surpassing the base model by orders of magnitude!\\
\\
We also notice that some weightings can become unstable (very large perplexities). In most runs with reweighting, the validation loss dropped quickly and then started increasing, so we could potentially interpret this phenomenon as overfitting caused by the small dataset and enhanced due to the faster learning of the reweighted objectives. Indeed, keeping the model with best performance instead of the one from the last training step significantly improved the test perplexity with reweighting. However, FM and Simple in particular exhibit exceedingly high test perplexities, which indicate that the training my fail to converge at all, which could be due to the small number of training steps

\section{Discussion}
\FloatBarrier
Across reproductions and extensions we consistently observe:
\begin{itemize}
  \item \textbf{Smaller blocks help perplexity:} decreasing $L'$ often improves PPL, consistent with approaching the AR regime.
  \item \textbf{Variance--PPL correlation:} we observed positive correlation between Var.\ NELBO and perplexity across training schedules. In well-tuned regimes, reductions in variance are typically accompanied by corresponding improvements in perplexity.
  \item \textbf{Length behavior differs qualitatively:} BD3-LMs can exceed fixed-length diffusion caps because generation proceeds block-by-block.
\end{itemize}

\section{Conclusion and Future Work}
\FloatBarrier
Our reproduction supports the main motivation for BD3-LMs: they interpolate between AR and diffusion and, under comparable budgets, can achieve better perplexity and improved sampling quality relative to diffusion baselines, while supporting variable-length generation.
We also found that (i) bimodal Gaussian schedules can improve PPL/variance in specific settings and (ii) loss reweighting (notably Sigmoid and EDM in our runs, as well as IDDPM) can yield significant improvements, even though there were some unstable results potentially due to our limited computational resources.

Promising next steps include frequency-informed masking (masking rare, information-rich tokens more often), combining both extensions in a single training run, and scaling compute to reduce small-model noise and get more reliable and improved results.

\nocite{*}

\printbibliography

\end{document}
