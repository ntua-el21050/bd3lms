{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05zjc93ATjfA",
        "outputId": "1f4510fc-a3aa-41a0-c6a8-5752e6b056e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bd3lms'...\n",
            "remote: Enumerating objects: 791, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 791 (delta 216), reused 193 (delta 186), pack-reused 541 (from 1)\u001b[K\n",
            "Receiving objects: 100% (791/791), 2.40 MiB | 7.39 MiB/s, done.\n",
            "Resolving deltas: 100% (508/508), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b Extension1 https://github.com/ntua-el21050/bd3lms.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -r bd3lms/requirements.txt"
      ],
      "metadata": {
        "id": "39LSigihTk39"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "def run_main(overrides, timeout=None):\n",
        "    \"\"\"Run `python -u main.py ...` and return combined stdout/stderr text.\"\"\"\n",
        "    env = dict(os.environ)\n",
        "    env.setdefault(\"HYDRA_FULL_ERROR\", \"1\")\n",
        "    cmd = [sys.executable, \"-u\", \"bd3lms/main.py\", *overrides]\n",
        "    print(\"\\n$\", \" \".join(cmd))\n",
        "    proc = subprocess.run(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        timeout=timeout,\n",
        "        check=False,\n",
        "        env=env,\n",
        "    )\n",
        "    print(proc.stdout[-4000:])  # tail for quick visibility\n",
        "    if proc.returncode != 0:\n",
        "        raise RuntimeError(f\"Command failed with return code {proc.returncode}\")\n",
        "    return proc.stdout\n",
        "\n",
        "_METRIC_PATTERNS = [\n",
        "    # Key: value (some loggers print this)\n",
        "    re.compile(r\"val/ppl\\s*[:=]\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
        "    re.compile(r\"'val/ppl'\\s*:\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
        "\n",
        "    # Lightning \"rich\" table row (note the unicode box character │)\n",
        "    re.compile(r\"val/ppl\\s*[│|]\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
        "]\n",
        "\n",
        "def extract_val_ppl(log_text: str):\n",
        "    # First try line-based parse from the end (most reliable for tables)\n",
        "    for line in reversed(log_text.splitlines()):\n",
        "        if \"val/ppl\" in line.lower():\n",
        "            m = re.search(r\"val/ppl.*?([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", line, re.IGNORECASE)\n",
        "            if m:\n",
        "                return float(m.group(1))\n",
        "\n",
        "    # Fallback: scan entire text with known patterns\n",
        "    hits = []\n",
        "    for pat in _METRIC_PATTERNS:\n",
        "        hits.extend(pat.findall(log_text))\n",
        "    return float(hits[-1]) if hits else None\n",
        "\n",
        "def _small_loader_overrides(batch_size=8, num_workers=2):\n",
        "    \"\"\"Overrides needed to avoid huge default batch sizes on Colab.\"\"\"\n",
        "    return [\n",
        "        f\"loader.global_batch_size={batch_size}\",\n",
        "        f\"loader.eval_global_batch_size={batch_size}\",\n",
        "        f\"loader.batch_size={batch_size}\",\n",
        "        f\"loader.eval_batch_size={batch_size}\",\n",
        "        f\"loader.num_workers={num_workers}\",\n",
        "        \"trainer.accumulate_grad_batches=1\",\n",
        "    ]\n",
        "\n",
        "def train_run(run_name, algo, block_size=None, from_pretrained=None, max_steps=800, extra_overrides=None):\n",
        "    \"\"\"Train a model (optionally from a base checkpoint) and return the last.ckpt path.\"\"\"\n",
        "    save_dir = Path(\"/content/repro_runs\") / run_name\n",
        "    if save_dir.exists():\n",
        "        shutil.rmtree(save_dir)\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    overrides = [\n",
        "        \"mode=train\",\n",
        "        \"data=lm1b-wrap\",\n",
        "        \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\",\n",
        "        \"data.max_train_samples=5000\",\n",
        "        # For LM1B, validation uses the 'test' split in this codebase\n",
        "        \"model=tiny\",\n",
        "        \"model.length=128\",\n",
        "        \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\",\n",
        "        \"trainer.accelerator=gpu\",\n",
        "        \"trainer.devices=1\",\n",
        "        \"trainer.num_nodes=1\",\n",
        "        \"trainer.precision=16-mixed\",\n",
        "        \"trainer.num_sanity_val_steps=0\",\n",
        "        \"trainer.log_every_n_steps=10\",\n",
        "        \"trainer.val_check_interval=50\",\n",
        "        f\"trainer.max_steps={max_steps}\",\n",
        "        \"data.max_valid_samples=100\",\n",
        "        \"data.max_test_samples=100\",\n",
        "        f\"checkpointing.save_dir=/content/repro_runs/{run_name}\",\n",
        "        \"checkpointing.resume_from_ckpt=false\",\n",
        "        \"wandb=null\",\n",
        "    ]\n",
        "    overrides.extend(_small_loader_overrides(batch_size=8, num_workers=2))\n",
        "    if block_size is not None:\n",
        "        overrides.append(f\"block_size={block_size}\")\n",
        "    if from_pretrained is not None:\n",
        "        overrides.append(f\"training.from_pretrained={from_pretrained}\")\n",
        "    if extra_overrides:\n",
        "        overrides.extend(extra_overrides)\n",
        "\n",
        "    _ = run_main(overrides)\n",
        "    ckpt = save_dir / \"checkpoints\" / \"last.ckpt\"\n",
        "    if not ckpt.exists():\n",
        "        raise FileNotFoundError(f\"Expected checkpoint not found: {ckpt}\")\n",
        "    return str(ckpt)\n",
        "\n",
        "def eval_run(algo, checkpoint_path, block_size=None, extra_overrides=None):\n",
        "    \"\"\"Evaluate perplexity (val/ppl) for a given checkpoint.\"\"\"\n",
        "    overrides = [\n",
        "        \"mode=ppl_eval\",\n",
        "        \"data=lm1b-wrap\",\n",
        "        \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\",\n",
        "        # For LM1B, `get_dataloaders` maps validation to the 'test' split\n",
        "        \"data.max_test_samples=1000\",\n",
        "        \"model=tiny\",\n",
        "        \"model.length=128\",\n",
        "        \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\",\n",
        "        f\"eval.checkpoint_path={checkpoint_path}\",\n",
        "        \"trainer.accelerator=gpu\",\n",
        "        \"trainer.devices=1\",\n",
        "        \"trainer.num_nodes=1\",\n",
        "        \"trainer.precision=16-mixed\",\n",
        "        \"trainer.num_sanity_val_steps=0\",\n",
        "        \"wandb=null\",\n",
        "    ]\n",
        "    overrides.extend(_small_loader_overrides(batch_size=8, num_workers=2))\n",
        "    if block_size is not None:\n",
        "        overrides.append(f\"block_size={block_size}\")\n",
        "    if extra_overrides:\n",
        "        overrides.extend(extra_overrides)\n",
        "\n",
        "    log_text = run_main(overrides)\n",
        "    ppl = extract_val_ppl(log_text)\n",
        "    if ppl is None:\n",
        "        raise ValueError(\"Could not parse val/ppl from output. Try increasing the log tail or printing full logs.\")\n",
        "    return ppl"
      ],
      "metadata": {
        "id": "zvd442kZTmZk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "results = []\n",
        "\n",
        "bd3lm_base_run = \"bd3lm_base_len128\"\n",
        "bd3lm_base_ckpt = train_run(\n",
        "    bd3lm_base_run,\n",
        "    algo=\"bd3lm\",\n",
        "    block_size=128,\n",
        "    extra_overrides=[\n",
        "        \"training.resample=false\",\n",
        "        \"algo.var_min=false\",\n",
        "        \"algo.clip_search_widths=[]\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "for Lprime in [16, 8, 4]:\n",
        "    finetune_run = f\"bd3lm_finetune_Lp{Lprime}\"\n",
        "    finetune_ckpt = train_run(\n",
        "        finetune_run,\n",
        "        algo=\"bd3lm\",\n",
        "        block_size=Lprime,\n",
        "        from_pretrained=bd3lm_base_ckpt,\n",
        "        extra_overrides=[\n",
        "            \"training.resample=true\",\n",
        "            \"algo.var_min=false\",\n",
        "            \"algo.clip_search_widths=[]\",\n",
        "        ],\n",
        "    )\n",
        "    ppl = eval_run(\n",
        "        algo=\"bd3lm\",\n",
        "        checkpoint_path=finetune_ckpt,\n",
        "        block_size=Lprime,\n",
        "        extra_overrides=[\n",
        "            \"algo.var_min=false\",\n",
        "        ],\n",
        "    )\n",
        "    results.append({\"model\": \"Block diffusion (BD3LM)\", \"block_size_Lprime\": Lprime, \"val_ppl\": ppl})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZlZqIGWTonc",
        "outputId": "aad2b0cd-4a7b-4dcd-bb05-8a1b4c902216"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_base_len128 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=128 training.resample=false algo.var_min=false algo.clip_search_widths=[]\n",
            ":  68%|██████▊   | 100/146 [00:34<00:15,  2.89it/s, v_num=0]Epoch 3, global step 538: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 3:  82%|████████▏ | 120/146 [00:43<00:09,  2.73it/s, v_num=0]\n",
            "Epoch 3:  82%|████████▏ | 120/146 [00:43<00:09,  2.73it/s, v_num=0]\n",
            "Epoch 3:  96%|█████████▌| 140/146 [00:46<00:02,  2.99it/s, v_num=0]\n",
            "Epoch 3:  96%|█████████▌| 140/146 [00:46<00:02,  2.99it/s, v_num=0]\n",
            "Epoch 3: 100%|██████████| 146/146 [00:47<00:00,  3.07it/s, v_num=0]\n",
            "Epoch 3: 100%|██████████| 146/146 [00:47<00:00,  3.07it/s, v_num=0]\n",
            "Epoch 3: 100%|██████████| 146/146 [00:47<00:00,  3.07it/s, v_num=0]\n",
            "Epoch 3:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
            "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
            "Epoch 4:  14%|█▎        | 20/146 [00:02<00:18,  6.76it/s, v_num=0]\n",
            "Epoch 4:  14%|█▎        | 20/146 [00:02<00:18,  6.76it/s, v_num=0]\n",
            "Epoch 4:  27%|██▋       | 40/146 [00:05<00:15,  6.94it/s, v_num=0]\n",
            "Epoch 4:  27%|██▋       | 40/146 [00:05<00:15,  6.94it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 25.65it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 4:  27%|██▋       | 40/146 [00:07<00:19,  5.36it/s, v_num=0]Epoch 4, global step 634: 'val/nll' reached 8.93194 (best 8.93194), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n",
            "\n",
            "Epoch 4:  41%|████      | 60/146 [00:19<00:28,  3.07it/s, v_num=0]\n",
            "Epoch 4:  41%|████      | 60/146 [00:19<00:28,  3.07it/s, v_num=0]\n",
            "Epoch 4:  55%|█████▍    | 80/146 [00:22<00:18,  3.58it/s, v_num=0]\n",
            "Epoch 4:  55%|█████▍    | 80/146 [00:22<00:18,  3.58it/s, v_num=0]\n",
            "Epoch 4:  68%|██████▊   | 100/146 [00:25<00:11,  3.97it/s, v_num=0]\n",
            "Epoch 4:  68%|██████▊   | 100/146 [00:25<00:11,  3.97it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 23.79it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 4:  68%|██████▊   | 100/146 [00:25<00:11,  3.91it/s, v_num=0]Epoch 4, global step 684: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 4:  82%|████████▏ | 120/146 [00:35<00:07,  3.41it/s, v_num=0]\n",
            "Epoch 4:  82%|████████▏ | 120/146 [00:35<00:07,  3.41it/s, v_num=0]\n",
            "Epoch 4:  96%|█████████▌| 140/146 [00:38<00:01,  3.67it/s, v_num=0]\n",
            "Epoch 4:  96%|█████████▌| 140/146 [00:38<00:01,  3.67it/s, v_num=0]\n",
            "Epoch 4: 100%|██████████| 146/146 [00:38<00:00,  3.75it/s, v_num=0]\n",
            "Epoch 4: 100%|██████████| 146/146 [00:38<00:00,  3.75it/s, v_num=0]\n",
            "Epoch 4: 100%|██████████| 146/146 [00:38<00:00,  3.75it/s, v_num=0]\n",
            "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
            "Epoch 5:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
            "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.07it/s, v_num=0]\n",
            "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.07it/s, v_num=0]\n",
            "Epoch 5:  27%|██▋       | 40/146 [00:05<00:14,  7.08it/s, v_num=0]\n",
            "Epoch 5:  27%|██▋       | 40/146 [00:05<00:14,  7.08it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 24.33it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 5:  27%|██▋       | 40/146 [00:07<00:19,  5.42it/s, v_num=0]Epoch 5, global step 780: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 5:  41%|████      | 60/146 [00:15<00:22,  3.86it/s, v_num=0]\n",
            "Epoch 5:  41%|████      | 60/146 [00:15<00:22,  3.86it/s, v_num=0]\n",
            "Epoch 5:  41%|████      | 60/146 [00:16<00:24,  3.53it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "Epoch 5:  41%|████      | 60/146 [00:16<00:24,  3.53it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_finetune_Lp16 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=16 training.from_pretrained=/content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt training.resample=true algo.var_min=false algo.clip_search_widths=[]\n",
            "15,  6.78it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 23.83it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 4:  27%|██▋       | 40/146 [00:07<00:20,  5.22it/s, v_num=0]Epoch 4, global step 584: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 4:  41%|████      | 60/146 [00:10<00:15,  5.56it/s, v_num=0]\n",
            "Epoch 4:  41%|████      | 60/146 [00:10<00:15,  5.56it/s, v_num=0]\n",
            "Epoch 4:  55%|█████▍    | 80/146 [00:13<00:11,  5.85it/s, v_num=0]\n",
            "Epoch 4:  55%|█████▍    | 80/146 [00:13<00:11,  5.85it/s, v_num=0]\n",
            "Epoch 4:  68%|██████▊   | 100/146 [00:16<00:07,  5.98it/s, v_num=0]\n",
            "Epoch 4:  68%|██████▊   | 100/146 [00:16<00:07,  5.98it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 23.68it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 4:  68%|██████▊   | 100/146 [00:17<00:07,  5.87it/s, v_num=0]Epoch 4, global step 634: 'val/nll' reached 10.37837 (best 10.37837), saving model to '/content/repro_runs/bd3lm_finetune_Lp16/checkpoints/best.ckpt' as top 1\n",
            "\n",
            "Epoch 4:  82%|████████▏ | 120/146 [00:36<00:07,  3.30it/s, v_num=0]\n",
            "Epoch 4:  82%|████████▏ | 120/146 [00:36<00:07,  3.30it/s, v_num=0]\n",
            "Epoch 4:  96%|█████████▌| 140/146 [00:39<00:01,  3.56it/s, v_num=0]\n",
            "Epoch 4:  96%|█████████▌| 140/146 [00:39<00:01,  3.56it/s, v_num=0]\n",
            "Epoch 4: 100%|██████████| 146/146 [00:40<00:00,  3.63it/s, v_num=0]\n",
            "Epoch 4: 100%|██████████| 146/146 [00:40<00:00,  3.63it/s, v_num=0]\n",
            "Epoch 4: 100%|██████████| 146/146 [00:40<00:00,  3.63it/s, v_num=0]\n",
            "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
            "Epoch 5:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
            "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.03it/s, v_num=0]\n",
            "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.03it/s, v_num=0]\n",
            "Epoch 5:  27%|██▋       | 40/146 [00:05<00:15,  7.03it/s, v_num=0]\n",
            "Epoch 5:  27%|██▋       | 40/146 [00:05<00:15,  7.03it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 24.58it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 5:  27%|██▋       | 40/146 [00:07<00:19,  5.38it/s, v_num=0]Epoch 5, global step 730: 'val/nll' reached 10.16591 (best 10.16591), saving model to '/content/repro_runs/bd3lm_finetune_Lp16/checkpoints/best.ckpt' as top 1\n",
            "\n",
            "Epoch 5:  41%|████      | 60/146 [00:25<00:36,  2.37it/s, v_num=0]\n",
            "Epoch 5:  41%|████      | 60/146 [00:25<00:36,  2.37it/s, v_num=0]\n",
            "Epoch 5:  55%|█████▍    | 80/146 [00:28<00:23,  2.85it/s, v_num=0]\n",
            "Epoch 5:  55%|█████▍    | 80/146 [00:28<00:23,  2.85it/s, v_num=0]\n",
            "Epoch 5:  68%|██████▊   | 100/146 [00:30<00:14,  3.23it/s, v_num=0]\n",
            "Epoch 5:  68%|██████▊   | 100/146 [00:30<00:14,  3.23it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 25.22it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 5:  68%|██████▊   | 100/146 [00:31<00:14,  3.20it/s, v_num=0]Epoch 5, global step 780: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 5:  82%|████████▏ | 120/146 [00:41<00:08,  2.92it/s, v_num=0]\n",
            "Epoch 5:  82%|████████▏ | 120/146 [00:41<00:08,  2.92it/s, v_num=0]\n",
            "Epoch 5:  82%|████████▏ | 120/146 [00:41<00:08,  2.92it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "Epoch 5:  82%|████████▏ | 120/146 [00:41<00:08,  2.92it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_finetune_Lp16/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=16 algo.var_min=false\n",
            "_test_bs128_wrapped.dat\n",
            "[2026-01-24 09:38:14,730][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 1000 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 1000 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2412.51 examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2405.19 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 39499.97 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-01-24 09:39:39.890488: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769247579.907948    5349 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769247579.913097    5349 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769247579.926674    5349 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769247579.926696    5349 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769247579.926699    5349 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769247579.926701    5349 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-24 09:39:39.930860: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\n",
            "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 13.22it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 15.05it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 14.75it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │    16.217763900756836     │\n",
            "│          val/nll          │    11.241297721862793     │\n",
            "│          val/ppl          │       76213.7890625       │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_finetune_Lp8 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=8 training.from_pretrained=/content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt training.resample=true algo.var_min=false algo.clip_search_widths=[]\n",
            " [00:02<00:18,  6.97it/s, v_num=0]\n",
            "Epoch 4:  14%|█▎        | 20/146 [00:02<00:18,  6.97it/s, v_num=0]\n",
            "Epoch 4:  27%|██▋       | 40/146 [00:05<00:15,  6.99it/s, v_num=0]\n",
            "Epoch 4:  27%|██▋       | 40/146 [00:05<00:15,  6.99it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 23.96it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 4:  27%|██▋       | 40/146 [00:07<00:19,  5.34it/s, v_num=0]Epoch 4, global step 584: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 4:  41%|████      | 60/146 [00:15<00:22,  3.79it/s, v_num=0]\n",
            "Epoch 4:  41%|████      | 60/146 [00:15<00:22,  3.79it/s, v_num=0]\n",
            "Epoch 4:  55%|█████▍    | 80/146 [00:18<00:15,  4.28it/s, v_num=0]\n",
            "Epoch 4:  55%|█████▍    | 80/146 [00:18<00:15,  4.28it/s, v_num=0]\n",
            "Epoch 4:  68%|██████▊   | 100/146 [00:21<00:09,  4.63it/s, v_num=0]\n",
            "Epoch 4:  68%|██████▊   | 100/146 [00:21<00:09,  4.63it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 23.60it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 4:  68%|██████▊   | 100/146 [00:22<00:10,  4.54it/s, v_num=0]Epoch 4, global step 634: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 4:  82%|████████▏ | 120/146 [00:35<00:07,  3.42it/s, v_num=0]\n",
            "Epoch 4:  82%|████████▏ | 120/146 [00:35<00:07,  3.42it/s, v_num=0]\n",
            "Epoch 4:  96%|█████████▌| 140/146 [00:37<00:01,  3.69it/s, v_num=0]\n",
            "Epoch 4:  96%|█████████▌| 140/146 [00:37<00:01,  3.69it/s, v_num=0]\n",
            "Epoch 4: 100%|██████████| 146/146 [00:38<00:00,  3.76it/s, v_num=0]\n",
            "Epoch 4: 100%|██████████| 146/146 [00:38<00:00,  3.76it/s, v_num=0]\n",
            "Epoch 4: 100%|██████████| 146/146 [00:38<00:00,  3.76it/s, v_num=0]\n",
            "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
            "Epoch 5:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
            "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.03it/s, v_num=0]\n",
            "Epoch 5:  14%|█▎        | 20/146 [00:02<00:17,  7.03it/s, v_num=0]\n",
            "Epoch 5:  27%|██▋       | 40/146 [00:05<00:15,  7.03it/s, v_num=0]\n",
            "Epoch 5:  27%|██▋       | 40/146 [00:05<00:15,  7.03it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 23.57it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 5:  27%|██▋       | 40/146 [00:07<00:20,  5.27it/s, v_num=0]Epoch 5, global step 730: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 5:  41%|████      | 60/146 [00:17<00:25,  3.36it/s, v_num=0]\n",
            "Epoch 5:  41%|████      | 60/146 [00:17<00:25,  3.36it/s, v_num=0]\n",
            "Epoch 5:  55%|█████▍    | 80/146 [00:20<00:17,  3.86it/s, v_num=0]\n",
            "Epoch 5:  55%|█████▍    | 80/146 [00:20<00:17,  3.86it/s, v_num=0]\n",
            "Epoch 5:  68%|██████▊   | 100/146 [00:23<00:10,  4.24it/s, v_num=0]\n",
            "Epoch 5:  68%|██████▊   | 100/146 [00:23<00:10,  4.23it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 24.60it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 5:  68%|██████▊   | 100/146 [00:23<00:11,  4.18it/s, v_num=0]Epoch 5, global step 780: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 5:  82%|████████▏ | 120/146 [00:37<00:08,  3.20it/s, v_num=0]\n",
            "Epoch 5:  82%|████████▏ | 120/146 [00:37<00:08,  3.20it/s, v_num=0]\n",
            "Epoch 5:  82%|████████▏ | 120/146 [00:37<00:08,  3.20it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "Epoch 5:  82%|████████▏ | 120/146 [00:37<00:08,  3.20it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_finetune_Lp8/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=8 algo.var_min=false\n",
            "_test_bs128_wrapped.dat\n",
            "[2026-01-24 09:46:08,414][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 1000 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 1000 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2262.35 examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2255.36 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 39441.65 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-01-24 09:48:18.313916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769248098.331589    7535 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769248098.336736    7535 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769248098.350190    7535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769248098.350213    7535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769248098.350216    7535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769248098.350219    7535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-24 09:48:18.354126: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\n",
            "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 14.07it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 15.94it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:01<00:00, 15.67it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │    16.736207962036133     │\n",
            "│          val/nll          │    11.600655555725098     │\n",
            "│          val/ppl          │       109169.34375        │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_finetune_Lp4 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 training.from_pretrained=/content/repro_runs/bd3lm_base_len128/checkpoints/last.ckpt training.resample=true algo.var_min=false algo.clip_search_widths=[]\n",
            " [00:03<00:18,  6.64it/s, v_num=0]\n",
            "Epoch 4:  14%|█▎        | 20/146 [00:03<00:18,  6.64it/s, v_num=0]\n",
            "Epoch 4:  27%|██▋       | 40/146 [00:06<00:17,  5.95it/s, v_num=0]\n",
            "Epoch 4:  27%|██▋       | 40/146 [00:06<00:17,  5.95it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 19.10it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 4:  27%|██▋       | 40/146 [00:08<00:23,  4.52it/s, v_num=0]Epoch 4, global step 584: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 4:  41%|████      | 60/146 [00:15<00:22,  3.88it/s, v_num=0]\n",
            "Epoch 4:  41%|████      | 60/146 [00:15<00:22,  3.88it/s, v_num=0]\n",
            "Epoch 4:  55%|█████▍    | 80/146 [00:18<00:15,  4.32it/s, v_num=0]\n",
            "Epoch 4:  55%|█████▍    | 80/146 [00:18<00:15,  4.32it/s, v_num=0]\n",
            "Epoch 4:  68%|██████▊   | 100/146 [00:21<00:09,  4.68it/s, v_num=0]\n",
            "Epoch 4:  68%|██████▊   | 100/146 [00:21<00:09,  4.68it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 23.67it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 4:  68%|██████▊   | 100/146 [00:21<00:10,  4.60it/s, v_num=0]Epoch 4, global step 634: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 4:  82%|████████▏ | 120/146 [00:28<00:06,  4.15it/s, v_num=0]\n",
            "Epoch 4:  82%|████████▏ | 120/146 [00:28<00:06,  4.15it/s, v_num=0]\n",
            "Epoch 4:  96%|█████████▌| 140/146 [00:31<00:01,  4.41it/s, v_num=0]\n",
            "Epoch 4:  96%|█████████▌| 140/146 [00:31<00:01,  4.41it/s, v_num=0]\n",
            "Epoch 4: 100%|██████████| 146/146 [00:32<00:00,  4.48it/s, v_num=0]\n",
            "Epoch 4: 100%|██████████| 146/146 [00:32<00:00,  4.48it/s, v_num=0]\n",
            "Epoch 4: 100%|██████████| 146/146 [00:32<00:00,  4.48it/s, v_num=0]\n",
            "Epoch 4:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]          \n",
            "Epoch 5:   0%|          | 0/146 [00:00<?, ?it/s, v_num=0]\n",
            "Epoch 5:  14%|█▎        | 20/146 [00:02<00:18,  6.97it/s, v_num=0]\n",
            "Epoch 5:  14%|█▎        | 20/146 [00:02<00:18,  6.97it/s, v_num=0]\n",
            "Epoch 5:  27%|██▋       | 40/146 [00:05<00:15,  7.00it/s, v_num=0]\n",
            "Epoch 5:  27%|██▋       | 40/146 [00:05<00:15,  7.00it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 24.36it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 5:  27%|██▋       | 40/146 [00:07<00:20,  5.24it/s, v_num=0]Epoch 5, global step 730: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 5:  41%|████      | 60/146 [00:58<01:23,  1.03it/s, v_num=0]\n",
            "Epoch 5:  41%|████      | 60/146 [00:58<01:23,  1.03it/s, v_num=0]\n",
            "Epoch 5:  55%|█████▍    | 80/146 [01:01<00:50,  1.31it/s, v_num=0]\n",
            "Epoch 5:  55%|█████▍    | 80/146 [01:01<00:50,  1.31it/s, v_num=0]\n",
            "Epoch 5:  68%|██████▊   | 100/146 [01:04<00:29,  1.56it/s, v_num=0]\n",
            "Epoch 5:  68%|██████▊   | 100/146 [01:04<00:29,  1.56it/s, v_num=0]\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 21.06it/s]\u001b[A\n",
            "\n",
            "                                                                      \u001b[A\n",
            "Epoch 5:  68%|██████▊   | 100/146 [01:04<00:29,  1.55it/s, v_num=0]Epoch 5, global step 780: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 5:  82%|████████▏ | 120/146 [01:14<00:16,  1.62it/s, v_num=0]\n",
            "Epoch 5:  82%|████████▏ | 120/146 [01:14<00:16,  1.62it/s, v_num=0]\n",
            "Epoch 5:  82%|████████▏ | 120/146 [01:14<00:16,  1.62it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "Epoch 5:  82%|████████▏ | 120/146 [01:14<00:16,  1.62it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_finetune_Lp4/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 algo.var_min=false\n",
            "_test_bs128_wrapped.dat\n",
            "[2026-01-24 09:57:23,946][dataloader][INFO] - streaming=True\n",
            "\n",
            "============================================================\n",
            "GET_STREAMING_SAMPLES DEBUG: lm1b\n",
            "  Requested: 1000 samples\n",
            "  Input type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
            "  Returning dataset with 1000 samples\n",
            "============================================================\n",
            "\n",
            "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2300.49 examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2293.05 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 38599.55 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-01-24 09:58:49.332649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769248729.361586   10533 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769248729.370158   10533 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769248729.391521   10533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769248729.391558   10533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769248729.391565   10533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769248729.391572   10533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-24 09:58:49.397707: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\n",
            "Validation:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:   0%|          | 0/29 [00:00<?, ?it/s]\n",
            "Validation DataLoader 0:  69%|██████▉   | 20/29 [00:01<00:00, 11.24it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:02<00:00, 13.13it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 29/29 [00:02<00:00, 12.87it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │     16.03902244567871     │\n",
            "│          val/nll          │    11.117403984069824     │\n",
            "│          val/ppl          │       67332.8828125       │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_table(rows):\n",
        "    if not rows:\n",
        "        print(\"No data to display.\")\n",
        "        return\n",
        "\n",
        "    columns = list(rows[0].keys())\n",
        "\n",
        "    str_rows = [\n",
        "        {col: str(row.get(col, \"\")) for col in columns}\n",
        "        for row in rows\n",
        "    ]\n",
        "\n",
        "    widths = {\n",
        "        col: max(len(col), max(len(row[col]) for row in str_rows))\n",
        "        for col in columns\n",
        "    }\n",
        "\n",
        "    def print_separator():\n",
        "        print(\"+\" + \"+\".join(\"-\" * (widths[col] + 2) for col in columns) + \"+\")\n",
        "\n",
        "    def print_row(row):\n",
        "        print(\n",
        "            \"| \" +\n",
        "            \" | \".join(row[col].ljust(widths[col]) for col in columns) +\n",
        "            \" |\"\n",
        "        )\n",
        "\n",
        "    # Print table\n",
        "    print_separator()\n",
        "    print_row({col: col for col in columns})\n",
        "    print_separator()\n",
        "    for row in str_rows:\n",
        "        print_row(row)\n",
        "    print_separator()\n",
        "\n",
        "print_table(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKf-xFOJTrGS",
        "outputId": "198a8b49-b2a9-49b1-cbac-27e7daa7eba5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+-------------------+---------------+\n",
            "| model                   | block_size_Lprime | val_ppl       |\n",
            "+-------------------------+-------------------+---------------+\n",
            "| Block diffusion (BD3LM) | 16                | 76213.7890625 |\n",
            "| Block diffusion (BD3LM) | 8                 | 109169.34375  |\n",
            "| Block diffusion (BD3LM) | 4                 | 67332.8828125 |\n",
            "+-------------------------+-------------------+---------------+\n"
          ]
        }
      ]
    }
  ]
}