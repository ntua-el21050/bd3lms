{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Clone YOUR fork\n",
        "!cd /content && rm -rf bd3lms\n",
        "!cd /content && git clone https://github.com/ntua-el21050/bd3lms.git\n",
        "\n",
        "!mkdir -p /content/bd3lms/data\n",
        "!mkdir -p /content/repro_runs\n",
        "!mkdir -p /content/hf_checkpoints\n",
        "!mkdir -p /content/sample_logs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOMzWIMDWVO2",
        "outputId": "8f5115d6-06be-4202-ff4b-986c2f7100c0"
      },
      "id": "cOMzWIMDWVO2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bd3lms'...\n",
            "remote: Enumerating objects: 1195, done.\u001b[K\n",
            "remote: Counting objects: 100% (459/459), done.\u001b[K\n",
            "remote: Compressing objects: 100% (221/221), done.\u001b[K\n",
            "remote: Total 1195 (delta 341), reused 315 (delta 237), pack-reused 736 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1195/1195), 8.43 MiB | 2.43 MiB/s, done.\n",
            "Resolving deltas: 100% (748/748), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchmetrics==1.6.2 datasets==3.3.2 einops==0.8.1 \\\n",
        "    hydra-core==1.3.2 lightning==2.5.0.post0 transformers==4.49.0 \\\n",
        "    huggingface_hub fsspec==2024.2.0 omegaconf==2.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZnhEGWlWYeH",
        "outputId": "685d16d7-5110-4d98-d597-bd56cfba2df9"
      },
      "id": "bZnhEGWlWYeH",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.3/857.3 kB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ef238998",
      "metadata": {
        "id": "ef238998"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.insert(0, '/content/bd3lms')\n",
        "\n",
        "def run_main(overrides, timeout=None):\n",
        "    \"\"\"Run `python -u main.py ...` and return combined stdout/stderr text.\"\"\"\n",
        "    env = dict(os.environ)\n",
        "    env.setdefault(\"HYDRA_FULL_ERROR\", \"1\")\n",
        "    cmd = [sys.executable, \"-u\", \"/content/bd3lms/main.py\", *overrides]\n",
        "    print(\"\\n$\", \" \".join(cmd))\n",
        "    proc = subprocess.run(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        timeout=timeout,\n",
        "        check=False,\n",
        "        env=env,\n",
        "    )\n",
        "    print(proc.stdout[-4000:])  # tail for quick visibility\n",
        "    if proc.returncode != 0:\n",
        "        raise RuntimeError(f\"Command failed with return code {proc.returncode}\")\n",
        "    return proc.stdout\n",
        "\n",
        "_METRIC_PATTERNS = [\n",
        "    # Key: value (some loggers print this)\n",
        "    re.compile(r\"val/ppl\\s*[:=]\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
        "    re.compile(r\"'val/ppl'\\s*:\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
        "\n",
        "    # Lightning \"rich\" table row (note the unicode box character │)\n",
        "    re.compile(r\"val/ppl\\s*[│|]\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
        "]\n",
        "\n",
        "# def extract_val_ppl(log_text: str):\n",
        "#     # First try line-based parse from the end (most reliable for tables)\n",
        "#     for line in reversed(log_text.splitlines()):\n",
        "#         if \"val/ppl\" in line.lower():\n",
        "#             m = re.search(r\"val/ppl.*?([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", line, re.IGNORECASE)\n",
        "#             if m:\n",
        "#                 return float(m.group(1))\n",
        "\n",
        "#     # Fallback: scan entire text with known patterns\n",
        "#     hits = []\n",
        "#     for pat in _METRIC_PATTERNS:\n",
        "#         hits.extend(pat.findall(log_text))\n",
        "#     return float(hits[-1]) if hits else None\n",
        "\n",
        "def strip_ansi(text):\n",
        "    \"\"\"Αφαιρεί τους κρυφούς χαρακτήρες χρώματος/μορφοποίησης (ANSI codes).\"\"\"\n",
        "    ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
        "    return ansi_escape.sub('', text)\n",
        "\n",
        "def extract_val_ppl(log_text: str):\n",
        "    # 1. Καθαρίζουμε το κείμενο από χρώματα και ειδικούς χαρακτήρες\n",
        "    clean_text = strip_ansi(log_text)\n",
        "\n",
        "    # 2. Ψάχνουμε από το τέλος προς την αρχή (για να βρούμε το τελικό αποτέλεσμα)\n",
        "    for line in reversed(clean_text.splitlines()):\n",
        "        # Εντοπισμός της γραμμής που περιέχει το metric\n",
        "        if \"val/ppl\" in line.lower():\n",
        "            # Ψάχνουμε για τον αριθμό (float) που ακολουθεί το \"val/ppl\"\n",
        "            # Το .*? επιτρέπει να υπάρχουν οσοιδήποτε χαρακτήρες (π.χ. │ ή κενά) ενδιάμεσα\n",
        "            m = re.search(r\"val/ppl.*?([0-9]+\\.[0-9]+(?:e[+-]?\\d+)?)\", line, re.IGNORECASE)\n",
        "            if m:\n",
        "                return float(m.group(1))\n",
        "\n",
        "    # Fallback σε περίπτωση που δεν βρεθεί στη γραμμική σάρωση\n",
        "    print(\"Warning: Could not parse ppl from line, trying patterns...\")\n",
        "    return 0.0\n",
        "\n",
        "def _small_loader_overrides(batch_size=8, num_workers=2):\n",
        "    \"\"\"Overrides needed to avoid huge default batch sizes on Colab.\"\"\"\n",
        "    return [\n",
        "        f\"loader.global_batch_size={batch_size}\",\n",
        "        f\"loader.eval_global_batch_size={batch_size}\",\n",
        "        f\"loader.batch_size={batch_size}\",\n",
        "        f\"loader.eval_batch_size={batch_size}\",\n",
        "        f\"loader.num_workers={num_workers}\",\n",
        "        \"trainer.accumulate_grad_batches=1\",\n",
        "    ]\n",
        "\n",
        "def train_run(run_name, algo, block_size=None, from_pretrained=None, max_steps=800, extra_overrides=None):\n",
        "    \"\"\"Train a model (optionally from a base checkpoint) and return the last.ckpt path.\"\"\"\n",
        "    save_dir = (Path(\"/content/repro_runs\") / run_name).resolve()\n",
        "    #save_dir = Path(\"/content/repro_runs\") / run_name\n",
        "    if save_dir.exists():\n",
        "        shutil.rmtree(save_dir)\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    overrides = [\n",
        "        \"mode=train\",\n",
        "        \"data=lm1b-wrap\",\n",
        "        \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\",\n",
        "        \"data.max_train_samples=5000\",\n",
        "        # For LM1B, validation uses the 'test' split in this codebase\n",
        "        \"model=tiny\",\n",
        "        \"model.length=128\",\n",
        "        \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\",\n",
        "        \"trainer.accelerator=gpu\",\n",
        "        \"trainer.devices=1\",\n",
        "        \"trainer.num_nodes=1\",\n",
        "        \"trainer.precision=16-mixed\",\n",
        "        \"trainer.num_sanity_val_steps=0\",\n",
        "        \"trainer.log_every_n_steps=10\",\n",
        "        \"trainer.val_check_interval=50\",\n",
        "        f\"trainer.max_steps={max_steps}\",\n",
        "        \"data.max_valid_samples=1000\",\n",
        "        \"data.max_test_samples=1000\",\n",
        "        f\"checkpointing.save_dir={str(save_dir)}\",\n",
        "        \"checkpointing.resume_from_ckpt=false\",\n",
        "        \"wandb=null\",\n",
        "    ]\n",
        "    overrides.extend(_small_loader_overrides(batch_size=8, num_workers=2))\n",
        "    if block_size is not None:\n",
        "        overrides.append(f\"block_size={block_size}\")\n",
        "    if from_pretrained is not None:\n",
        "        overrides.append(f\"training.from_pretrained={Path(from_pretrained).resolve()}\")\n",
        "    if extra_overrides:\n",
        "        overrides.extend(extra_overrides)\n",
        "\n",
        "    _ = run_main(overrides)\n",
        "    ckpt = save_dir / \"checkpoints\" / \"best.ckpt\"\n",
        "    if not ckpt.exists():\n",
        "        raise FileNotFoundError(f\"Expected checkpoint not found: {ckpt}\")\n",
        "    return str(ckpt)\n",
        "\n",
        "def eval_run(algo, checkpoint_path, block_size=None, extra_overrides=None):\n",
        "    \"\"\"Evaluate perplexity (val/ppl) for a given checkpoint.\"\"\"\n",
        "    overrides = [\n",
        "        \"mode=ppl_eval\",\n",
        "        \"data=lm1b-wrap\",\n",
        "        \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\",\n",
        "        # For LM1B, `get_dataloaders` maps validation to the 'test' split\n",
        "        \"data.max_test_samples=1000\",\n",
        "        \"data.max_valid_samples=1000\",\n",
        "        \"data.max_train_samples=5000\",\n",
        "        \"+data.skip_samples=0\",\n",
        "        \"model=tiny\",\n",
        "        \"model.length=128\",\n",
        "        \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\",\n",
        "        f\"eval.checkpoint_path={checkpoint_path}\",\n",
        "        \"trainer.accelerator=gpu\",\n",
        "        \"trainer.devices=1\",\n",
        "        \"trainer.num_nodes=1\",\n",
        "        \"trainer.precision=16-mixed\",\n",
        "        \"trainer.num_sanity_val_steps=0\",\n",
        "        \"wandb=null\",\n",
        "    ]\n",
        "    overrides.extend(_small_loader_overrides(batch_size=8, num_workers=2))\n",
        "    if block_size is not None:\n",
        "        overrides.append(f\"block_size={block_size}\")\n",
        "    if extra_overrides:\n",
        "        overrides.extend(extra_overrides)\n",
        "\n",
        "    log_text = run_main(overrides)\n",
        "    ppl = extract_val_ppl(log_text)\n",
        "    if ppl is None:\n",
        "        raise ValueError(\"Could not parse val/ppl from output. Try increasing the log tail or printing full logs.\")\n",
        "    return ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "55f0d35b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55f0d35b",
        "outputId": "0de48cf6-f46c-4cee-ef64-cdf4209403fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "$ /usr/bin/python3 -u /content/bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=1000 data.max_test_samples=1000 checkpointing.save_dir=/content/repro_runs/bd3lm_base_len128 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=128 training.resample=false algo.var_min=false algo.clip_search_widths=[]\n",
            "------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-15 16:49:46.422580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771174186.445112   18647 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771174186.452604   18647 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771174186.472776   18647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771174186.472806   18647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771174186.472809   18647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771174186.472811   18647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 16:49:46.477707: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type           | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | backbone | DIT            | 22.8 M | train\n",
            "1 | noise    | LogLinearNoise | 0      | train\n",
            "----------------------------------------------------\n",
            "22.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 M    Total params\n",
            "91.266    Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "Epoch 0, global step 50: 'val/nll' reached 5.10607 (best 5.10607), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n",
            "Epoch 0, global step 100: 'val/nll' reached 5.05176 (best 5.05176), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n",
            "Epoch 1, global step 196: 'val/nll' reached 4.85796 (best 4.85796), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n",
            "Epoch 1, global step 246: 'val/nll' reached 4.76169 (best 4.76169), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n",
            "Epoch 2, global step 342: 'val/nll' reached 4.05337 (best 4.05337), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n",
            "Epoch 2, global step 392: 'val/nll' reached 3.64889 (best 3.64889), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n",
            "Epoch 3, global step 488: 'val/nll' reached 3.52240 (best 3.52240), saving model to '/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt' as top 1\n",
            "Epoch 3, global step 538: 'val/nll' was not in top 1\n",
            "Epoch 4, global step 634: 'val/nll' was not in top 1\n",
            "Epoch 4, global step 684: 'val/nll' was not in top 1\n",
            "Epoch 5, global step 780: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u /content/bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=1000 data.max_test_samples=1000 checkpointing.save_dir=/content/repro_runs/bd3lm_finetune_Lp16 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=16 training.from_pretrained=/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt training.resample=true algo.var_min=false algo.clip_search_widths=[]\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-15 16:56:21.357298: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771174581.376591   20371 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771174581.382608   20371 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771174581.399717   20371 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771174581.399737   20371 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771174581.399742   20371 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771174581.399744   20371 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 16:56:21.403854: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type           | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | backbone | DIT            | 22.8 M | train\n",
            "1 | noise    | LogLinearNoise | 0      | train\n",
            "----------------------------------------------------\n",
            "22.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 M    Total params\n",
            "91.266    Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "Epoch 0, global step 50: 'val/nll' reached 14.35295 (best 14.35295), saving model to '/content/repro_runs/bd3lm_finetune_Lp16/checkpoints/best.ckpt' as top 1\n",
            "Epoch 0, global step 100: 'val/nll' reached 9.43230 (best 9.43230), saving model to '/content/repro_runs/bd3lm_finetune_Lp16/checkpoints/best.ckpt' as top 1\n",
            "Epoch 1, global step 196: 'val/nll' reached 8.07561 (best 8.07561), saving model to '/content/repro_runs/bd3lm_finetune_Lp16/checkpoints/best.ckpt' as top 1\n",
            "Epoch 2, global step 292: 'val/nll' was not in top 1\n",
            "Epoch 2, global step 342: 'val/nll' was not in top 1\n",
            "Epoch 3, global step 438: 'val/nll' was not in top 1\n",
            "Epoch 3, global step 488: 'val/nll' was not in top 1\n",
            "Epoch 4, global step 584: 'val/nll' was not in top 1\n",
            "Epoch 4, global step 634: 'val/nll' was not in top 1\n",
            "Epoch 5, global step 730: 'val/nll' was not in top 1\n",
            "Epoch 5, global step 780: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u /content/bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 data.max_valid_samples=1000 data.max_train_samples=5000 +data.skip_samples=0 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_finetune_Lp16/checkpoints/best.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=16 algo.var_min=false\n",
            " at 196\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['sampling_eps_min', 'sampling_eps_max']\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "[2026-02-15 17:00:12,664][dataloader][INFO] - Generating new data at: /content/bd3lms/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-15 17:00:12,664][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2400.56 examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2392.22 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 39523.79 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-15 17:01:57.989018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771174918.007725   22023 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771174918.013794   22023 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771174918.029608   22023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771174918.029632   22023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771174918.029634   22023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771174918.029637   22023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 17:01:58.033847: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │    15.408323287963867     │\n",
            "│          val/nll          │    10.680235862731934     │\n",
            "│          val/ppl          │       43487.8046875       │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u /content/bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=1000 data.max_test_samples=1000 checkpointing.save_dir=/content/repro_runs/bd3lm_finetune_Lp8 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=8 training.from_pretrained=/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt training.resample=true algo.var_min=false algo.clip_search_widths=[]\n",
            "PU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-15 17:05:26.941895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771175126.961554   22606 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771175126.967698   22606 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771175126.986678   22606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771175126.986701   22606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771175126.986703   22606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771175126.986705   22606 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 17:05:26.990964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type           | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | backbone | DIT            | 22.8 M | train\n",
            "1 | noise    | LogLinearNoise | 0      | train\n",
            "----------------------------------------------------\n",
            "22.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 M    Total params\n",
            "91.266    Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "Epoch 0, global step 50: 'val/nll' reached 43.73481 (best 43.73481), saving model to '/content/repro_runs/bd3lm_finetune_Lp8/checkpoints/best.ckpt' as top 1\n",
            "Epoch 0, global step 100: 'val/nll' reached 11.75743 (best 11.75743), saving model to '/content/repro_runs/bd3lm_finetune_Lp8/checkpoints/best.ckpt' as top 1\n",
            "Epoch 1, global step 196: 'val/nll' was not in top 1\n",
            "Epoch 2, global step 292: 'val/nll' was not in top 1\n",
            "Epoch 2, global step 342: 'val/nll' reached 11.00661 (best 11.00661), saving model to '/content/repro_runs/bd3lm_finetune_Lp8/checkpoints/best.ckpt' as top 1\n",
            "Epoch 3, global step 438: 'val/nll' was not in top 1\n",
            "Epoch 3, global step 488: 'val/nll' was not in top 1\n",
            "Epoch 4, global step 584: 'val/nll' was not in top 1\n",
            "Epoch 4, global step 634: 'val/nll' was not in top 1\n",
            "Epoch 5, global step 730: 'val/nll' was not in top 1\n",
            "Epoch 5, global step 780: 'val/nll' was not in top 1\n",
            "`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u /content/bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 data.max_valid_samples=1000 data.max_train_samples=5000 +data.skip_samples=0 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_finetune_Lp8/checkpoints/best.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=8 algo.var_min=false\n",
            " at 342\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['sampling_eps_min', 'sampling_eps_max']\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "[2026-02-15 17:09:25,949][dataloader][INFO] - Generating new data at: /content/bd3lms/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-15 17:09:25,949][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2188.32 examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2180.92 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 40101.96 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-15 17:12:38.427606: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771175558.446301   24501 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771175558.452228   24501 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771175558.467946   24501 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771175558.467967   24501 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771175558.467970   24501 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771175558.467972   24501 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 17:12:38.471919: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │     32.94111633300781     │\n",
            "│          val/nll          │     22.83304214477539     │\n",
            "│          val/ppl          │       8246395904.0        │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u /content/bd3lms/main.py mode=train data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=5000 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=50 trainer.max_steps=800 data.max_valid_samples=1000 data.max_test_samples=1000 checkpointing.save_dir=/content/repro_runs/bd3lm_finetune_Lp4 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 training.from_pretrained=/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt training.resample=true algo.var_min=false algo.clip_search_widths=[]\n",
            "..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-15 17:15:55.205205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771175755.237663   25461 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771175755.247720   25461 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771175755.274587   25461 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771175755.274624   25461 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771175755.274629   25461 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771175755.274631   25461 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 17:15:55.280937: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type           | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | backbone | DIT            | 22.8 M | train\n",
            "1 | noise    | LogLinearNoise | 0      | train\n",
            "----------------------------------------------------\n",
            "22.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 M    Total params\n",
            "91.266    Total estimated model params size (MB)\n",
            "110       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "Epoch 0, global step 50: 'val/nll' reached 14.00021 (best 14.00021), saving model to '/content/repro_runs/bd3lm_finetune_Lp4/checkpoints/best.ckpt' as top 1\n",
            "Epoch 0, global step 100: 'val/nll' reached 13.39778 (best 13.39778), saving model to '/content/repro_runs/bd3lm_finetune_Lp4/checkpoints/best.ckpt' as top 1\n",
            "Epoch 1, global step 196: 'val/nll' was not in top 1\n",
            "Epoch 2, global step 292: 'val/nll' reached 12.83599 (best 12.83599), saving model to '/content/repro_runs/bd3lm_finetune_Lp4/checkpoints/best.ckpt' as top 1\n",
            "Epoch 2, global step 342: 'val/nll' was not in top 1\n",
            "Epoch 3, global step 438: 'val/nll' was not in top 1\n",
            "Epoch 3, global step 488: 'val/nll' reached 12.30832 (best 12.30832), saving model to '/content/repro_runs/bd3lm_finetune_Lp4/checkpoints/best.ckpt' as top 1\n",
            "Epoch 4, global step 584: 'val/nll' was not in top 1\n",
            "Epoch 4, global step 634: 'val/nll' was not in top 1\n",
            "Epoch 5, global step 730: 'val/nll' was not in top 1\n",
            "Epoch 5, global step 780: 'val/nll' reached 11.47666 (best 11.47666), saving model to '/content/repro_runs/bd3lm_finetune_Lp4/checkpoints/best.ckpt' as top 1\n",
            "`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u /content/bd3lms/main.py mode=ppl_eval data=lm1b-wrap data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=1000 data.max_valid_samples=1000 data.max_train_samples=5000 +data.skip_samples=0 model=tiny model.length=128 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_finetune_Lp4/checkpoints/best.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=8 loader.eval_global_batch_size=8 loader.batch_size=8 loader.eval_batch_size=8 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 algo.var_min=false\n",
            " at 780\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['sampling_eps_min', 'sampling_eps_max']\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
            "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
            "Seed set to 1\n",
            "[2026-02-15 17:19:48,521][dataloader][INFO] - Generating new data at: /content/bd3lms/data/lm1b_test_bs128_wrapped.dat\n",
            "[2026-02-15 17:19:48,521][dataloader][INFO] - streaming=True\n",
            "\n",
            "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2365.02 examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2355.60 examples/s]\n",
            "\n",
            "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
            "Map: 100%|██████████| 1000/1000 [00:00<00:00, 37653.21 examples/s]\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "2026-02-15 17:21:35.906648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771176095.924900   27357 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771176095.930837   27357 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771176095.946179   27357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771176095.946201   27357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771176095.946203   27357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771176095.946205   27357 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 17:21:35.950226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
            "  warnings.warn(  # warn only once\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │    20.162639617919922     │\n",
            "│          val/nll          │    13.975676536560059     │\n",
            "│          val/ppl          │        1173705.625        │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "results = []\n",
        "\n",
        "bd3lm_base_run = \"bd3lm_base_len128\"\n",
        "bd3lm_base_ckpt = train_run(\n",
        "    bd3lm_base_run,\n",
        "    algo=\"bd3lm\",\n",
        "    block_size=128,\n",
        "    extra_overrides=[\n",
        "        \"training.resample=false\",\n",
        "        \"algo.var_min=false\",\n",
        "        \"algo.clip_search_widths=[]\",\n",
        "    ],\n",
        ")\n",
        "bd3lm_base_ckpt = \"/content/repro_runs/bd3lm_base_len128/checkpoints/best.ckpt\"\n",
        "\n",
        "for Lprime in [16, 8, 4]:\n",
        "    finetune_run = f\"bd3lm_finetune_Lp{Lprime}\"\n",
        "    finetune_ckpt = train_run(\n",
        "        finetune_run,\n",
        "        algo=\"bd3lm\",\n",
        "        block_size=Lprime,\n",
        "        from_pretrained=bd3lm_base_ckpt,\n",
        "        extra_overrides=[\n",
        "            \"training.resample=true\",\n",
        "            \"algo.var_min=false\",\n",
        "            \"algo.clip_search_widths=[]\",\n",
        "        ],\n",
        "    )\n",
        "    ppl = eval_run(\n",
        "        algo=\"bd3lm\",\n",
        "        checkpoint_path=finetune_ckpt,\n",
        "        block_size=Lprime,\n",
        "        extra_overrides=[\n",
        "            \"algo.var_min=false\",\n",
        "        ],\n",
        "    )\n",
        "    results.append({\"model\": \"Block diffusion (BD3LM)\", \"block_size_Lprime\": Lprime, \"val_ppl\": ppl})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5b803b39",
      "metadata": {
        "id": "5b803b39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d5a8fd-afa4-4abc-ba3d-2cbc2a5e798b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+-------------------+---------------+\n",
            "| model                   | block_size_Lprime | val_ppl       |\n",
            "+-------------------------+-------------------+---------------+\n",
            "| Block diffusion (BD3LM) | 16                | 43487.8046875 |\n",
            "| Block diffusion (BD3LM) | 8                 | 8246395904.0  |\n",
            "| Block diffusion (BD3LM) | 4                 | 1173705.625   |\n",
            "+-------------------------+-------------------+---------------+\n"
          ]
        }
      ],
      "source": [
        "def print_table(rows):\n",
        "    if not rows:\n",
        "        print(\"No data to display.\")\n",
        "        return\n",
        "\n",
        "    columns = list(rows[0].keys())\n",
        "\n",
        "    str_rows = [\n",
        "        {col: str(row.get(col, \"\")) for col in columns}\n",
        "        for row in rows\n",
        "    ]\n",
        "\n",
        "    widths = {\n",
        "        col: max(len(col), max(len(row[col]) for row in str_rows))\n",
        "        for col in columns\n",
        "    }\n",
        "\n",
        "    def print_separator():\n",
        "        print(\"+\" + \"+\".join(\"-\" * (widths[col] + 2) for col in columns) + \"+\")\n",
        "\n",
        "    def print_row(row):\n",
        "        print(\n",
        "            \"| \" +\n",
        "            \" | \".join(row[col].ljust(widths[col]) for col in columns) +\n",
        "            \" |\"\n",
        "        )\n",
        "\n",
        "    # Print table\n",
        "    print_separator()\n",
        "    print_row({col: col for col in columns})\n",
        "    print_separator()\n",
        "    for row in str_rows:\n",
        "        print_row(row)\n",
        "    print_separator()\n",
        "\n",
        "print_table(results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x3BkXGAnYEzn"
      },
      "id": "x3BkXGAnYEzn",
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}