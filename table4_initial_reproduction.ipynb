{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OZ08dSN50mU",
        "outputId": "6431fad8-b63f-4b57-d60c-7f0fc7ec92ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'bd3lms'...\n",
            "remote: Enumerating objects: 768, done.\u001b[K\n",
            "remote: Counting objects: 100% (229/229), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 768 (delta 198), reused 182 (delta 173), pack-reused 539 (from 2)\u001b[K\n",
            "Receiving objects: 100% (768/768), 1.78 MiB | 22.51 MiB/s, done.\n",
            "Resolving deltas: 100% (494/494), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone repo\n",
        "!cd /content && rm -rf bd3lms\n",
        "!cd /content && git clone https://github.com/ntua-el21050/bd3lms.git\n",
        "\n",
        "# Create directories\n",
        "!mkdir -p /content/bd3lms/data\n",
        "!mkdir -p /content/repro_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM40BOTY52-j",
        "outputId": "feafc7b8-aac0-472e-e893-a468621f6d8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q torchmetrics==1.6.2 datasets==3.3.2 einops==0.8.1 \\\n",
        "    hydra-core==1.3.2 lightning==2.5.0.post0 transformers==4.49.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97Oz2avj6MI_"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def run_main(overrides, timeout=None):\n",
        "    \"\"\"Run `python -u main.py ...` and return combined stdout/stderr text.\"\"\"\n",
        "    env = dict(os.environ)\n",
        "    env.setdefault(\"HYDRA_FULL_ERROR\", \"1\")\n",
        "    cmd = [sys.executable, \"-u\", \"bd3lms/main.py\", *overrides]\n",
        "    print(\"\\n$\", \" \".join(cmd))\n",
        "    proc = subprocess.run(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        timeout=timeout,\n",
        "        check=False,\n",
        "        env=env,\n",
        "    )\n",
        "    print(proc.stdout[-4000:])\n",
        "    if proc.returncode != 0:\n",
        "        raise RuntimeError(f\"Command failed with return code {proc.returncode}\")\n",
        "    return proc.stdout\n",
        "\n",
        "_METRIC_PATTERNS = [\n",
        "    re.compile(r\"val/ppl\\s*[:=]\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
        "    re.compile(r\"'val/ppl'\\s*:\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
        "    re.compile(r\"val/ppl\\s*[│|]\\s*([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", re.IGNORECASE),\n",
        "]\n",
        "\n",
        "def extract_val_ppl(log_text: str):\n",
        "    for line in reversed(log_text.splitlines()):\n",
        "        if \"val/ppl\" in line.lower():\n",
        "            m = re.search(r\"val/ppl.*?([0-9]+(?:\\.[0-9]+)?(?:e[+-]?\\d+)?)\", line, re.IGNORECASE)\n",
        "            if m:\n",
        "                return float(m.group(1))\n",
        "    hits = []\n",
        "    for pat in _METRIC_PATTERNS:\n",
        "        hits.extend(pat.findall(log_text))\n",
        "    return float(hits[-1]) if hits else None\n",
        "\n",
        "def _small_loader_overrides(batch_size=4, num_workers=2):\n",
        "    \"\"\"Smaller batch size for OWT (1024 context length).\"\"\"\n",
        "    return [\n",
        "        f\"loader.global_batch_size={batch_size}\",\n",
        "        f\"loader.eval_global_batch_size={batch_size}\",\n",
        "        f\"loader.batch_size={batch_size}\",\n",
        "        f\"loader.eval_batch_size={batch_size}\",\n",
        "        f\"loader.num_workers={num_workers}\",\n",
        "        \"trainer.accumulate_grad_batches=1\",\n",
        "    ]\n",
        "\n",
        "def train_run(run_name, algo, block_size=None, from_pretrained=None, max_steps=800, extra_overrides=None):\n",
        "    \"\"\"Train a model for Table 4 (OpenWebText).\"\"\"\n",
        "    save_dir = Path(\"/content/repro_runs\") / run_name\n",
        "    if save_dir.exists():\n",
        "        shutil.rmtree(save_dir)\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    overrides = [\n",
        "        \"mode=train\",\n",
        "\n",
        "        # TABLE 4 SPECIFIC: OpenWebText, length=1024\n",
        "  \n",
        "        \"data=openwebtext-split\",           # ← OWT dataset\n",
        "        \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\",\n",
        "        \"data.max_train_samples=1200\",      # Scaled down\n",
        "        \"model=tiny\",\n",
        "        \"model.length=1024\",                # ← 1024 context (8x larger than Table 3)\n",
        "        \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\",\n",
        "\n",
        "        # Trainer settings\n",
        "\n",
        "        \"trainer.accelerator=gpu\",\n",
        "        \"trainer.devices=1\",\n",
        "        \"trainer.num_nodes=1\",\n",
        "        \"trainer.precision=16-mixed\",# Mixed precision για ταχύτητα\n",
        "        \"trainer.num_sanity_val_steps=0\",\n",
        "        \"trainer.log_every_n_steps=10\",\n",
        "        \"trainer.val_check_interval=10\",    # ← Μικρό για λίγα samples\n",
        "        f\"trainer.max_steps={max_steps}\",\n",
        "        \"data.max_valid_samples=100\",\n",
        "        \"data.max_test_samples=100\",\n",
        "        f\"checkpointing.save_dir=/content/repro_runs/{run_name}\",\n",
        "        \"checkpointing.resume_from_ckpt=false\",\n",
        "        \"wandb=null\",\n",
        "    ]\n",
        "    overrides.extend(_small_loader_overrides(batch_size=4, num_workers=2))  # ← batch=4 για memory\n",
        "\n",
        "    if block_size is not None:\n",
        "        overrides.append(f\"block_size={block_size}\")\n",
        "    if from_pretrained is not None:\n",
        "        overrides.append(f\"training.from_pretrained={from_pretrained}\")\n",
        "    if extra_overrides:\n",
        "        overrides.extend(extra_overrides)\n",
        "\n",
        "    _ = run_main(overrides)\n",
        "    ckpt = save_dir / \"checkpoints\" / \"last.ckpt\"\n",
        "    if not ckpt.exists():\n",
        "        raise FileNotFoundError(f\"Expected checkpoint not found: {ckpt}\")\n",
        "    return str(ckpt)\n",
        "\n",
        "def eval_run(algo, checkpoint_path, block_size=None, extra_overrides=None):\n",
        "    \"\"\"Evaluate perplexity for Table 4 (OpenWebText).\"\"\"\n",
        "    overrides = [\n",
        "        \"mode=ppl_eval\",\n",
        "  \n",
        "        # TABLE 4 SPECIFIC\n",
        "   \n",
        "        \"data=openwebtext-split\",           # ← OWT dataset\n",
        "        \"data.cache_dir=/content/bd3lms/data\",\n",
        "        \"data.streaming=true\",\n",
        "        \"data.max_test_samples=500\",        # Evaluation samples\n",
        "        \"model=tiny\",\n",
        "        \"model.length=1024\",                # ← 1024 context\n",
        "        \"model.attn_backend=sdpa\",\n",
        "        f\"algo={algo}\",\n",
        "        f\"eval.checkpoint_path={checkpoint_path}\",\n",
        "        \"trainer.accelerator=gpu\",\n",
        "        \"trainer.devices=1\",\n",
        "        \"trainer.num_nodes=1\",\n",
        "        \"trainer.precision=16-mixed\",\n",
        "        \"trainer.num_sanity_val_steps=0\",\n",
        "        \"wandb=null\",\n",
        "    ]\n",
        "    overrides.extend(_small_loader_overrides(batch_size=4, num_workers=2))\n",
        "\n",
        "    if block_size is not None:\n",
        "        overrides.append(f\"block_size={block_size}\")\n",
        "    if extra_overrides:\n",
        "        overrides.extend(extra_overrides)\n",
        "\n",
        "    log_text = run_main(overrides)\n",
        "    ppl = extract_val_ppl(log_text)\n",
        "    if ppl is None:\n",
        "        raise ValueError(\"Could not parse val/ppl from output.\")\n",
        "    return ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4FUCk6N6OVf",
        "outputId": "196af09f-8ff3-4132-e03f-20a714c9544c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Training AR baseline...\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=1200 model=tiny model.length=1024 model.attn_backend=sdpa algo=ar trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=10 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/ar_tiny_owt_len1024 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=4 loader.eval_global_batch_size=4 loader.batch_size=4 loader.eval_batch_size=4 loader.num_workers=2 trainer.accumulate_grad_batches=1\n",
            "able this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 67.87it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 68.69it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:36<01:27,  2.74it/s, v_num=0]Epoch 2, global step 780: 'val/nll' reached 7.70358 (best 7.70358), saving model to '/content/repro_runs/ar_tiny_owt_len1024/checkpoints/best.ckpt' as top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 67.04it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 67.80it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:40<01:36,  2.49it/s, v_num=0]Epoch 2, global step 790: 'val/nll' reached 7.69996 (best 7.69996), saving model to '/content/repro_runs/ar_tiny_owt_len1024/checkpoints/best.ckpt' as top 1\n",
            "\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:44<01:21,  2.71it/s, v_num=0]\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:44<01:21,  2.71it/s, v_num=0]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 67.08it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 67.78it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:44<01:22,  2.68it/s, v_num=0]Epoch 2, global step 800: 'val/nll' reached 7.69617 (best 7.69617), saving model to '/content/repro_runs/ar_tiny_owt_len1024/checkpoints/best.ckpt' as top 1\n",
            "\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:47<01:27,  2.52it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:47<01:27,  2.52it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=500 model=tiny model.length=1024 model.attn_backend=sdpa algo=ar eval.checkpoint_path=/content/repro_runs/ar_tiny_owt_len1024/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=4 loader.eval_global_batch_size=4 loader.batch_size=4 loader.eval_batch_size=4 loader.num_workers=2 trainer.accumulate_grad_batches=1\n",
            "| 26740/27619 [06:13<00:12, 71.61it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26760/27619 [06:13<00:11, 71.61it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26780/27619 [06:13<00:11, 71.61it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26800/27619 [06:14<00:11, 71.61it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26820/27619 [06:14<00:11, 71.61it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26840/27619 [06:14<00:10, 71.61it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26860/27619 [06:15<00:10, 71.62it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26880/27619 [06:15<00:10, 71.62it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26900/27619 [06:15<00:10, 71.62it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26920/27619 [06:15<00:09, 71.62it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26940/27619 [06:16<00:09, 71.62it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26960/27619 [06:16<00:09, 71.62it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26980/27619 [06:16<00:08, 71.62it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27000/27619 [06:16<00:08, 71.62it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27020/27619 [06:17<00:08, 71.62it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27040/27619 [06:17<00:08, 71.62it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27060/27619 [06:17<00:07, 71.62it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27080/27619 [06:18<00:07, 71.62it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27100/27619 [06:18<00:07, 71.62it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27120/27619 [06:18<00:06, 71.62it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27140/27619 [06:18<00:06, 71.61it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27160/27619 [06:19<00:06, 71.61it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27180/27619 [06:19<00:06, 71.61it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27200/27619 [06:19<00:05, 71.61it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27220/27619 [06:20<00:05, 71.60it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27240/27619 [06:20<00:05, 71.60it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27260/27619 [06:20<00:05, 71.60it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27280/27619 [06:21<00:04, 71.60it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27300/27619 [06:21<00:04, 71.59it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27320/27619 [06:21<00:04, 71.59it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27340/27619 [06:21<00:03, 71.59it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27360/27619 [06:22<00:03, 71.59it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27380/27619 [06:22<00:03, 71.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27400/27619 [06:22<00:03, 71.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27420/27619 [06:23<00:02, 71.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27440/27619 [06:23<00:02, 71.57it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27460/27619 [06:23<00:02, 71.57it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27480/27619 [06:23<00:01, 71.57it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27500/27619 [06:24<00:01, 71.57it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27520/27619 [06:24<00:01, 71.56it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27540/27619 [06:24<00:01, 71.56it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27560/27619 [06:25<00:00, 71.56it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27580/27619 [06:25<00:00, 71.56it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27600/27619 [06:25<00:00, 71.56it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 27619/27619 [06:25<00:00, 71.56it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 27619/27619 [06:26<00:00, 71.54it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │    10.991216659545898     │\n",
            "│          val/nll          │     7.618531227111816     │\n",
            "│          val/ppl          │     2035.570068359375     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "✓ AR PPL: 2035.570068359375\n",
            "============================================================\n",
            "Training SEDD baseline...\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=1200 model=tiny model.length=1024 model.attn_backend=sdpa algo=sedd trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=10 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/sedd_tiny_owt_len1024 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=4 loader.eval_global_batch_size=4 loader.batch_size=4 loader.eval_batch_size=4 loader.num_workers=2 trainer.accumulate_grad_batches=1 training.resample=false algo.var_min=false algo.clip_search_widths=[]\n",
            "l/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:31<01:15,  3.20it/s, v_num=0]\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:31<01:15,  3.20it/s, v_num=0]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 42.50it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 42.85it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:31<01:16,  3.13it/s, v_num=0]Epoch 2, global step 780: 'val/nll' was not in top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 43.01it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 43.54it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:35<01:25,  2.82it/s, v_num=0]Epoch 2, global step 790: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:37<01:08,  3.19it/s, v_num=0]\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:37<01:08,  3.19it/s, v_num=0]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 44.06it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 44.61it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:38<01:10,  3.14it/s, v_num=0]Epoch 2, global step 800: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:39<01:12,  3.02it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:39<01:12,  3.02it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=500 model=tiny model.length=1024 model.attn_backend=sdpa algo=sedd eval.checkpoint_path=/content/repro_runs/sedd_tiny_owt_len1024/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=4 loader.eval_global_batch_size=4 loader.batch_size=4 loader.eval_batch_size=4 loader.num_workers=2 trainer.accumulate_grad_batches=1 algo.var_min=false\n",
            "| 26740/27619 [09:05<00:17, 49.05it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26760/27619 [09:05<00:17, 49.05it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26780/27619 [09:05<00:17, 49.05it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26800/27619 [09:06<00:16, 49.05it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26820/27619 [09:06<00:16, 49.05it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26840/27619 [09:07<00:15, 49.05it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26860/27619 [09:07<00:15, 49.05it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26880/27619 [09:08<00:15, 49.04it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26900/27619 [09:08<00:14, 49.04it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26920/27619 [09:08<00:14, 49.04it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26940/27619 [09:09<00:13, 49.04it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26960/27619 [09:09<00:13, 49.04it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26980/27619 [09:10<00:13, 49.04it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27000/27619 [09:10<00:12, 49.04it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27020/27619 [09:10<00:12, 49.04it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27040/27619 [09:11<00:11, 49.04it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27060/27619 [09:11<00:11, 49.04it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27080/27619 [09:12<00:10, 49.04it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27100/27619 [09:12<00:10, 49.05it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27120/27619 [09:12<00:10, 49.05it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27140/27619 [09:13<00:09, 49.05it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27160/27619 [09:13<00:09, 49.05it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27180/27619 [09:14<00:08, 49.05it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27200/27619 [09:14<00:08, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27220/27619 [09:14<00:08, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27240/27619 [09:15<00:07, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27260/27619 [09:15<00:07, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27280/27619 [09:16<00:06, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27300/27619 [09:16<00:06, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27320/27619 [09:16<00:06, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27340/27619 [09:17<00:05, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27360/27619 [09:17<00:05, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27380/27619 [09:18<00:04, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27400/27619 [09:18<00:04, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27420/27619 [09:19<00:04, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27440/27619 [09:19<00:03, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27460/27619 [09:19<00:03, 49.05it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27480/27619 [09:20<00:02, 49.05it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27500/27619 [09:20<00:02, 49.05it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27520/27619 [09:21<00:02, 49.05it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27540/27619 [09:21<00:01, 49.05it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27560/27619 [09:21<00:01, 49.05it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27580/27619 [09:22<00:00, 49.05it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27600/27619 [09:22<00:00, 49.05it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 27619/27619 [09:23<00:00, 49.04it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 27619/27619 [09:23<00:00, 49.04it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │    11.049884796142578     │\n",
            "│          val/nll          │     7.659196376800537     │\n",
            "│          val/ppl          │     2120.052978515625     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "✓ SEDD PPL: 2120.052978515625\n",
            "============================================================\n",
            "Training MDLM baseline...\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=1200 model=tiny model.length=1024 model.attn_backend=sdpa algo=mdlm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=10 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/mdlm_tiny_owt_len1024 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=4 loader.eval_global_batch_size=4 loader.batch_size=4 loader.eval_batch_size=4 loader.num_workers=2 trainer.accumulate_grad_batches=1 training.resample=false algo.var_min=false algo.clip_search_widths=[]\n",
            "l/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:28<01:08,  3.51it/s, v_num=0]\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:28<01:08,  3.51it/s, v_num=0]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 42.49it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 42.57it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:29<01:10,  3.42it/s, v_num=0]Epoch 2, global step 780: 'val/nll' was not in top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 41.68it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 42.08it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:31<01:16,  3.13it/s, v_num=0]Epoch 2, global step 790: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:34<01:04,  3.43it/s, v_num=0]\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:34<01:04,  3.43it/s, v_num=0]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 41.94it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 42.41it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:35<01:05,  3.36it/s, v_num=0]Epoch 2, global step 800: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:38<01:09,  3.15it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:38<01:09,  3.15it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=500 model=tiny model.length=1024 model.attn_backend=sdpa algo=mdlm eval.checkpoint_path=/content/repro_runs/mdlm_tiny_owt_len1024/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=4 loader.eval_global_batch_size=4 loader.batch_size=4 loader.eval_batch_size=4 loader.num_workers=2 trainer.accumulate_grad_batches=1 algo.var_min=false\n",
            "| 26740/27619 [09:46<00:19, 45.58it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26760/27619 [09:47<00:18, 45.58it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26780/27619 [09:47<00:18, 45.58it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26800/27619 [09:47<00:17, 45.58it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26820/27619 [09:48<00:17, 45.58it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26840/27619 [09:48<00:17, 45.58it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26860/27619 [09:49<00:16, 45.59it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26880/27619 [09:49<00:16, 45.59it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26900/27619 [09:50<00:15, 45.59it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26920/27619 [09:50<00:15, 45.59it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26940/27619 [09:50<00:14, 45.59it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26960/27619 [09:51<00:14, 45.59it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26980/27619 [09:51<00:14, 45.59it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27000/27619 [09:52<00:13, 45.59it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27020/27619 [09:52<00:13, 45.59it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27040/27619 [09:53<00:12, 45.59it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27060/27619 [09:53<00:12, 45.59it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27080/27619 [09:54<00:11, 45.58it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27100/27619 [09:54<00:11, 45.58it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27120/27619 [09:54<00:10, 45.58it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27140/27619 [09:55<00:10, 45.58it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27160/27619 [09:55<00:10, 45.58it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27180/27619 [09:56<00:09, 45.58it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27200/27619 [09:56<00:09, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27220/27619 [09:57<00:08, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27240/27619 [09:57<00:08, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27260/27619 [09:58<00:07, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27280/27619 [09:58<00:07, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27300/27619 [09:58<00:06, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27320/27619 [09:59<00:06, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27340/27619 [09:59<00:06, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27360/27619 [10:00<00:05, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27380/27619 [10:00<00:05, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27400/27619 [10:01<00:04, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27420/27619 [10:01<00:04, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27440/27619 [10:01<00:03, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27460/27619 [10:02<00:03, 45.58it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27480/27619 [10:02<00:03, 45.58it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27500/27619 [10:03<00:02, 45.58it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27520/27619 [10:03<00:02, 45.58it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27540/27619 [10:04<00:01, 45.58it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27560/27619 [10:04<00:01, 45.58it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27580/27619 [10:05<00:00, 45.58it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27600/27619 [10:05<00:00, 45.58it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 27619/27619 [10:05<00:00, 45.58it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 27619/27619 [10:05<00:00, 45.58it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │    11.036568641662598     │\n",
            "│          val/nll          │     7.649966239929199     │\n",
            "│          val/ppl          │     2100.57470703125      │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "✓ MDLM PPL: 2100.57470703125\n",
            "============================================================\n",
            "Training BD3-LM BASE (block_size=1024)...\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=1200 model=tiny model.length=1024 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=10 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_base_owt_len1024 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=4 loader.eval_global_batch_size=4 loader.batch_size=4 loader.eval_batch_size=4 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=1024 training.resample=false algo.var_min=false algo.clip_search_widths=[]\n",
            "l/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:35<01:26,  2.78it/s, v_num=0]\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:35<01:26,  2.78it/s, v_num=0]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 34.59it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 34.92it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:36<01:28,  2.71it/s, v_num=0]Epoch 2, global step 780: 'val/nll' was not in top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 35.07it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 35.48it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  29%|██▉       | 100/340 [00:40<01:36,  2.48it/s, v_num=0]Epoch 2, global step 790: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:42<01:18,  2.81it/s, v_num=0]\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:42<01:18,  2.81it/s, v_num=0]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 35.09it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 35.24it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:43<01:19,  2.76it/s, v_num=0]Epoch 2, global step 800: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:44<01:22,  2.67it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "Epoch 2:  35%|███▌      | 120/340 [00:44<01:22,  2.67it/s, v_num=0]\n",
            "\n",
            "✓ BD3-LM base checkpoint saved\n",
            "============================================================\n",
            "Fine-tuning BD3-LM (block_size=16)...\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=1200 model=tiny model.length=1024 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=10 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_finetune_owt_Lp16 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=4 loader.eval_global_batch_size=4 loader.batch_size=4 loader.eval_batch_size=4 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=16 training.from_pretrained=/content/repro_runs/bd3lm_base_owt_len1024/checkpoints/last.ckpt training.resample=true algo.var_min=false algo.clip_search_widths=[]\n",
            "l/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  65%|██████▍   | 220/340 [01:09<00:37,  3.18it/s, v_num=0]\n",
            "Epoch 2:  65%|██████▍   | 220/340 [01:09<00:37,  3.18it/s, v_num=0]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 34.67it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 35.05it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  65%|██████▍   | 220/340 [01:10<00:38,  3.14it/s, v_num=0]Epoch 2, global step 780: 'val/nll' was not in top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 35.38it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 35.71it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  65%|██████▍   | 220/340 [01:14<00:40,  2.97it/s, v_num=0]Epoch 2, global step 790: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  71%|███████   | 240/340 [01:16<00:31,  3.14it/s, v_num=0]\n",
            "Epoch 2:  71%|███████   | 240/340 [01:16<00:31,  3.14it/s, v_num=0]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 35.69it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 35.97it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  71%|███████   | 240/340 [01:17<00:32,  3.11it/s, v_num=0]Epoch 2, global step 800: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  71%|███████   | 240/340 [01:18<00:32,  3.05it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "Epoch 2:  71%|███████   | 240/340 [01:18<00:32,  3.05it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=500 model=tiny model.length=1024 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_finetune_owt_Lp16/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=4 loader.eval_global_batch_size=4 loader.batch_size=4 loader.eval_batch_size=4 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=16 algo.var_min=false\n",
            "| 26740/27619 [11:41<00:23, 38.11it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26760/27619 [11:42<00:22, 38.11it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26780/27619 [11:42<00:22, 38.11it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26800/27619 [11:43<00:21, 38.11it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26820/27619 [11:43<00:20, 38.11it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26840/27619 [11:44<00:20, 38.11it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26860/27619 [11:44<00:19, 38.11it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26880/27619 [11:45<00:19, 38.11it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26900/27619 [11:45<00:18, 38.11it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26920/27619 [11:46<00:18, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26940/27619 [11:46<00:17, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26960/27619 [11:47<00:17, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26980/27619 [11:47<00:16, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27000/27619 [11:48<00:16, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27020/27619 [11:48<00:15, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27040/27619 [11:49<00:15, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27060/27619 [11:50<00:14, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27080/27619 [11:50<00:14, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27100/27619 [11:51<00:13, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27120/27619 [11:51<00:13, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27140/27619 [11:52<00:12, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27160/27619 [11:52<00:12, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27180/27619 [11:53<00:11, 38.11it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27200/27619 [11:53<00:10, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27220/27619 [11:54<00:10, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27240/27619 [11:54<00:09, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27260/27619 [11:55<00:09, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27280/27619 [11:55<00:08, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27300/27619 [11:56<00:08, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27320/27619 [11:56<00:07, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27340/27619 [11:57<00:07, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27360/27619 [11:57<00:06, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27380/27619 [11:58<00:06, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27400/27619 [11:58<00:05, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27420/27619 [11:59<00:05, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27440/27619 [11:59<00:04, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27460/27619 [12:00<00:04, 38.11it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27480/27619 [12:01<00:03, 38.11it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27500/27619 [12:01<00:03, 38.11it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27520/27619 [12:02<00:02, 38.11it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27540/27619 [12:02<00:02, 38.11it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27560/27619 [12:03<00:01, 38.12it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27580/27619 [12:03<00:01, 38.12it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27600/27619 [12:04<00:00, 38.12it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 27619/27619 [12:04<00:00, 38.12it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 27619/27619 [12:04<00:00, 38.11it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │    10.921751022338867     │\n",
            "│          val/nll          │     7.570381164550781     │\n",
            "│          val/ppl          │     1939.87939453125      │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "✓ BD3-LM (L'=16) PPL: 1939.87939453125\n",
            "============================================================\n",
            "Fine-tuning BD3-LM (block_size=8)...\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=1200 model=tiny model.length=1024 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=10 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_finetune_owt_Lp8 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=4 loader.eval_global_batch_size=4 loader.batch_size=4 loader.eval_batch_size=4 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=8 training.from_pretrained=/content/repro_runs/bd3lm_base_owt_len1024/checkpoints/last.ckpt training.resample=true algo.var_min=false algo.clip_search_widths=[]\n",
            "l/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  65%|██████▍   | 220/340 [01:08<00:37,  3.19it/s, v_num=0]\n",
            "Epoch 2:  65%|██████▍   | 220/340 [01:08<00:37,  3.19it/s, v_num=0]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 34.26it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 34.47it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  65%|██████▍   | 220/340 [01:09<00:38,  3.15it/s, v_num=0]Epoch 2, global step 780: 'val/nll' was not in top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 35.06it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 35.18it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  65%|██████▍   | 220/340 [01:13<00:40,  2.98it/s, v_num=0]Epoch 2, global step 790: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  71%|███████   | 240/340 [01:16<00:31,  3.15it/s, v_num=0]\n",
            "Epoch 2:  71%|███████   | 240/340 [01:16<00:31,  3.15it/s, v_num=0]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 34.32it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 34.56it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  71%|███████   | 240/340 [01:17<00:32,  3.11it/s, v_num=0]Epoch 2, global step 800: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  71%|███████   | 240/340 [01:19<00:33,  3.02it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "Epoch 2:  71%|███████   | 240/340 [01:19<00:33,  3.02it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=500 model=tiny model.length=1024 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_finetune_owt_Lp8/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=4 loader.eval_global_batch_size=4 loader.batch_size=4 loader.eval_batch_size=4 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=8 algo.var_min=false\n",
            "| 26740/27619 [11:45<00:23, 37.93it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26760/27619 [11:45<00:22, 37.93it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26780/27619 [11:46<00:22, 37.93it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26800/27619 [11:46<00:21, 37.93it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26820/27619 [11:47<00:21, 37.93it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26840/27619 [11:47<00:20, 37.93it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26860/27619 [11:48<00:20, 37.93it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26880/27619 [11:48<00:19, 37.93it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26900/27619 [11:49<00:18, 37.93it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26920/27619 [11:49<00:18, 37.93it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26940/27619 [11:50<00:17, 37.93it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26960/27619 [11:50<00:17, 37.93it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26980/27619 [11:51<00:16, 37.93it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27000/27619 [11:51<00:16, 37.92it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27020/27619 [11:52<00:15, 37.92it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27040/27619 [11:53<00:15, 37.92it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27060/27619 [11:53<00:14, 37.92it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27080/27619 [11:54<00:14, 37.92it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27100/27619 [11:54<00:13, 37.92it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27120/27619 [11:55<00:13, 37.92it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27140/27619 [11:55<00:12, 37.92it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27160/27619 [11:56<00:12, 37.92it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27180/27619 [11:56<00:11, 37.93it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27200/27619 [11:57<00:11, 37.93it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27220/27619 [11:57<00:10, 37.93it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27240/27619 [11:58<00:09, 37.93it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27260/27619 [11:58<00:09, 37.93it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27280/27619 [11:59<00:08, 37.93it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27300/27619 [11:59<00:08, 37.93it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27320/27619 [12:00<00:07, 37.93it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27340/27619 [12:00<00:07, 37.92it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27360/27619 [12:01<00:06, 37.92it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27380/27619 [12:02<00:06, 37.92it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27400/27619 [12:02<00:05, 37.92it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27420/27619 [12:03<00:05, 37.92it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27440/27619 [12:03<00:04, 37.92it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27460/27619 [12:04<00:04, 37.92it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27480/27619 [12:04<00:03, 37.92it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27500/27619 [12:05<00:03, 37.92it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27520/27619 [12:05<00:02, 37.92it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27540/27619 [12:06<00:02, 37.92it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27560/27619 [12:06<00:01, 37.92it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27580/27619 [12:07<00:01, 37.92it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27600/27619 [12:07<00:00, 37.92it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 27619/27619 [12:08<00:00, 37.92it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 27619/27619 [12:08<00:00, 37.92it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │     10.92239761352539     │\n",
            "│          val/nll          │     7.570828914642334     │\n",
            "│          val/ppl          │     1940.748291015625     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "✓ BD3-LM (L'=8) PPL: 1940.748291015625\n",
            "============================================================\n",
            "Fine-tuning BD3-LM (block_size=4)...\n",
            "============================================================\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=train data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_train_samples=1200 model=tiny model.length=1024 model.attn_backend=sdpa algo=bd3lm trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 trainer.log_every_n_steps=10 trainer.val_check_interval=10 trainer.max_steps=800 data.max_valid_samples=100 data.max_test_samples=100 checkpointing.save_dir=/content/repro_runs/bd3lm_finetune_owt_Lp4 checkpointing.resume_from_ckpt=false wandb=null loader.global_batch_size=4 loader.eval_global_batch_size=4 loader.batch_size=4 loader.eval_batch_size=4 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 training.from_pretrained=/content/repro_runs/bd3lm_base_owt_len1024/checkpoints/last.ckpt training.resample=true algo.var_min=false algo.clip_search_widths=[]\n",
            "l/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  65%|██████▍   | 220/340 [01:10<00:38,  3.12it/s, v_num=0]\n",
            "Epoch 2:  65%|██████▍   | 220/340 [01:10<00:38,  3.12it/s, v_num=0]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 33.39it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 33.85it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  65%|██████▍   | 220/340 [01:11<00:38,  3.08it/s, v_num=0]Epoch 2, global step 780: 'val/nll' was not in top 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 35.06it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 35.33it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  65%|██████▍   | 220/340 [01:14<00:40,  2.94it/s, v_num=0]Epoch 2, global step 790: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  71%|███████   | 240/340 [01:17<00:32,  3.11it/s, v_num=0]\n",
            "Epoch 2:  71%|███████   | 240/340 [01:17<00:32,  3.11it/s, v_num=0]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0:  83%|████████▎ | 20/24 [00:00<00:00, 33.43it/s]\u001b[A\n",
            "\n",
            "Validation DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 33.80it/s]\u001b[A\n",
            "\n",
            "                                                                        \u001b[A\n",
            "Epoch 2:  71%|███████   | 240/340 [01:18<00:32,  3.07it/s, v_num=0]Epoch 2, global step 800: 'val/nll' was not in top 1\n",
            "\n",
            "Epoch 2:  71%|███████   | 240/340 [01:19<00:33,  3.01it/s, v_num=0]`Trainer.fit` stopped: `max_steps=800` reached.\n",
            "\n",
            "Epoch 2:  71%|███████   | 240/340 [01:19<00:33,  3.01it/s, v_num=0]\n",
            "\n",
            "\n",
            "$ /usr/bin/python3 -u bd3lms/main.py mode=ppl_eval data=openwebtext-split data.cache_dir=/content/bd3lms/data data.streaming=true data.max_test_samples=500 model=tiny model.length=1024 model.attn_backend=sdpa algo=bd3lm eval.checkpoint_path=/content/repro_runs/bd3lm_finetune_owt_Lp4/checkpoints/last.ckpt trainer.accelerator=gpu trainer.devices=1 trainer.num_nodes=1 trainer.precision=16-mixed trainer.num_sanity_val_steps=0 wandb=null loader.global_batch_size=4 loader.eval_global_batch_size=4 loader.batch_size=4 loader.eval_batch_size=4 loader.num_workers=2 trainer.accumulate_grad_batches=1 block_size=4 algo.var_min=false\n",
            "| 26740/27619 [11:39<00:22, 38.22it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26760/27619 [11:40<00:22, 38.22it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26780/27619 [11:40<00:21, 38.22it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26800/27619 [11:41<00:21, 38.22it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26820/27619 [11:41<00:20, 38.22it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26840/27619 [11:42<00:20, 38.22it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26860/27619 [11:42<00:19, 38.22it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26880/27619 [11:43<00:19, 38.22it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26900/27619 [11:43<00:18, 38.22it/s]\n",
            "Validation DataLoader 0:  97%|█████████▋| 26920/27619 [11:44<00:18, 38.22it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26940/27619 [11:44<00:17, 38.22it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26960/27619 [11:45<00:17, 38.22it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 26980/27619 [11:45<00:16, 38.22it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27000/27619 [11:46<00:16, 38.22it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27020/27619 [11:46<00:15, 38.23it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27040/27619 [11:47<00:15, 38.23it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27060/27619 [11:47<00:14, 38.23it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27080/27619 [11:48<00:14, 38.23it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27100/27619 [11:48<00:13, 38.23it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27120/27619 [11:49<00:13, 38.23it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27140/27619 [11:49<00:12, 38.23it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27160/27619 [11:50<00:12, 38.23it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27180/27619 [11:51<00:11, 38.23it/s]\n",
            "Validation DataLoader 0:  98%|█████████▊| 27200/27619 [11:51<00:10, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27220/27619 [11:52<00:10, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27240/27619 [11:52<00:09, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▊| 27260/27619 [11:53<00:09, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27280/27619 [11:53<00:08, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27300/27619 [11:54<00:08, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27320/27619 [11:54<00:07, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27340/27619 [11:55<00:07, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27360/27619 [11:55<00:06, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27380/27619 [11:56<00:06, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27400/27619 [11:56<00:05, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27420/27619 [11:57<00:05, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27440/27619 [11:57<00:04, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27460/27619 [11:58<00:04, 38.23it/s]\n",
            "Validation DataLoader 0:  99%|█████████▉| 27480/27619 [11:58<00:03, 38.23it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27500/27619 [11:59<00:03, 38.23it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27520/27619 [11:59<00:02, 38.23it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27540/27619 [12:00<00:02, 38.23it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27560/27619 [12:00<00:01, 38.23it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27580/27619 [12:01<00:01, 38.23it/s]\n",
            "Validation DataLoader 0: 100%|█████████▉| 27600/27619 [12:01<00:00, 38.23it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 27619/27619 [12:02<00:00, 38.23it/s]\n",
            "Validation DataLoader 0: 100%|██████████| 27619/27619 [12:02<00:00, 38.23it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃      Validate metric      ┃       DataLoader 0        ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│          val/bpd          │     10.91845989227295     │\n",
            "│          val/nll          │     7.568099498748779     │\n",
            "│          val/ppl          │    1935.4583740234375     │\n",
            "└───────────────────────────┴───────────────────────────┘\n",
            "\n",
            "✓ BD3-LM (L'=4) PPL: 1935.4583740234375\n",
            "\n",
            "============================================================\n",
            "ALL EXPERIMENTS COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "# 1) AUTOREGRESSIVE BASELINE\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training AR baseline...\")\n",
        "print(\"=\" * 60)\n",
        "ar_run = \"ar_tiny_owt_len1024\"\n",
        "ar_ckpt = train_run(ar_run, algo=\"ar\")\n",
        "ar_ppl = eval_run(algo=\"ar\", checkpoint_path=ar_ckpt)\n",
        "results.append({\"model\": \"Autoregressive\", \"block_size_Lprime\": \"-\", \"val_ppl\": ar_ppl})\n",
        "print(f\"✓ AR PPL: {ar_ppl}\")\n",
        "\n",
        "\n",
        "# 2) DIFFUSION BASELINES: SEDD + MDLM\n",
        "\n",
        "for algo_name, display_name in [(\"sedd\", \"SEDD\"), (\"mdlm\", \"MDLM\")]:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Training {display_name} baseline...\")\n",
        "    print(\"=\" * 60)\n",
        "    run_name = f\"{algo_name}_tiny_owt_len1024\"\n",
        "    ckpt = train_run(\n",
        "        run_name,\n",
        "        algo=algo_name,\n",
        "        extra_overrides=[\n",
        "            \"training.resample=false\",\n",
        "            \"algo.var_min=false\",\n",
        "            \"algo.clip_search_widths=[]\",\n",
        "        ],\n",
        "    )\n",
        "    ppl = eval_run(\n",
        "        algo=algo_name,\n",
        "        checkpoint_path=ckpt,\n",
        "        extra_overrides=[\n",
        "            \"algo.var_min=false\",\n",
        "        ],\n",
        "    )\n",
        "    results.append({\"model\": display_name, \"block_size_Lprime\": \"-\", \"val_ppl\": ppl})\n",
        "    print(f\"✓ {display_name} PPL: {ppl}\")\n",
        "\n",
        "\n",
        "# 3) BD3-LM BASE TRAINING (block_size = 1024 = L)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training BD3-LM BASE (block_size=1024)...\")\n",
        "print(\"=\" * 60)\n",
        "bd3lm_base_run = \"bd3lm_base_owt_len1024\"\n",
        "bd3lm_base_ckpt = train_run(\n",
        "    bd3lm_base_run,\n",
        "    algo=\"bd3lm\",\n",
        "    block_size=1024,  # ← L' = L (full context for base)\n",
        "    extra_overrides=[\n",
        "        \"training.resample=false\",  # No resampling for base\n",
        "        \"algo.var_min=false\",\n",
        "        \"algo.clip_search_widths=[]\",\n",
        "    ],\n",
        ")\n",
        "print(f\"✓ BD3-LM base checkpoint saved\")\n",
        "\n",
        "# 4) BD3-LM FINE-TUNING (block_size = 16, 8, 4)\n",
        "\n",
        "for Lprime in [16, 8, 4]:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Fine-tuning BD3-LM (block_size={Lprime})...\")\n",
        "    print(\"=\" * 60)\n",
        "    finetune_run = f\"bd3lm_finetune_owt_Lp{Lprime}\"\n",
        "    finetune_ckpt = train_run(\n",
        "        finetune_run,\n",
        "        algo=\"bd3lm\",\n",
        "        block_size=Lprime,\n",
        "        from_pretrained=bd3lm_base_ckpt,  # ← Start from base!\n",
        "        extra_overrides=[\n",
        "            \"training.resample=true\",  # ← Enable resampling for fine-tune!\n",
        "            \"algo.var_min=false\",\n",
        "            \"algo.clip_search_widths=[]\",\n",
        "        ],\n",
        "    )\n",
        "    ppl = eval_run(\n",
        "        algo=\"bd3lm\",\n",
        "        checkpoint_path=finetune_ckpt,\n",
        "        block_size=Lprime,\n",
        "        extra_overrides=[\n",
        "            \"algo.var_min=false\",\n",
        "        ],\n",
        "    )\n",
        "    results.append({\"model\": \"BD3-LM\", \"block_size_Lprime\": Lprime, \"val_ppl\": ppl})\n",
        "    print(f\"✓ BD3-LM (L'={Lprime}) PPL: {ppl}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ALL EXPERIMENTS COMPLETE!\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
